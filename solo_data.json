{
  "extraction_info": {
    "timestamp": "2025-07-10T22:37:46.859121",
    "source_url": "https://github.com/GetSoloTech/solo-server",
    "parameters": {
      "max_file_size": null,
      "file_categories": {
        "README.md": {
          "category": "Documentation",
          "type": "include"
        },
        "LICENSE": {
          "category": "Project Files",
          "type": "include"
        },
        "install.sh": {
          "category": "Scripts",
          "type": "include"
        },
        "setup.py": {
          "category": "Source Code",
          "type": "include"
        },
        "next.config.js": {
          "category": "Source Code",
          "type": "include"
        },
        "package-lock.json": {
          "category": "Configuration",
          "type": "include"
        },
        "package.json": {
          "category": "Configuration",
          "type": "include"
        },
        "postcss.config.js": {
          "category": "Source Code",
          "type": "include"
        },
        "tailwind.config.js": {
          "category": "Source Code",
          "type": "include"
        },
        "tsconfig.json": {
          "category": "Configuration",
          "type": "include"
        },
        ".gitignore": {
          "category": "Git Files",
          "type": "include"
        },
        "globals.css": {
          "category": "Web Files",
          "type": "include"
        },
        "layout.tsx": {
          "category": "Source Code",
          "type": "include"
        },
        "page.module.css": {
          "category": "Web Files",
          "type": "include"
        },
        "page.tsx": {
          "category": "Source Code",
          "type": "include"
        },
        "Dockerfile": {
          "category": "Build Files",
          "type": "include"
        },
        "client.py": {
          "category": "Source Code",
          "type": "include"
        },
        "model.py": {
          "category": "Source Code",
          "type": "include"
        },
        "server.py": {
          "category": "Source Code",
          "type": "include"
        },
        "__init__.py": {
          "category": "Source Code",
          "type": "include"
        },
        "cli.py": {
          "category": "Source Code",
          "type": "include"
        },
        "finetune_script.py": {
          "category": "Source Code",
          "type": "include"
        },
        "main.py": {
          "category": "Source Code",
          "type": "include"
        },
        "utils.py": {
          "category": "Source Code",
          "type": "include"
        },
        "benchmark.py": {
          "category": "Source Code",
          "type": "include"
        },
        "download_hf.py": {
          "category": "Source Code",
          "type": "include"
        },
        "finetune.py": {
          "category": "Source Code",
          "type": "include"
        },
        "models_list.py": {
          "category": "Source Code",
          "type": "include"
        },
        "serve.py": {
          "category": "Source Code",
          "type": "include"
        },
        "status.py": {
          "category": "Source Code",
          "type": "include"
        },
        "stop.py": {
          "category": "Source Code",
          "type": "include"
        },
        "test.py": {
          "category": "Source Code",
          "type": "include"
        },
        "config.yaml": {
          "category": "Configuration",
          "type": "include"
        },
        "config_loader.py": {
          "category": "Source Code",
          "type": "include"
        },
        "docker_utils.py": {
          "category": "Source Code",
          "type": "include"
        },
        "hardware.py": {
          "category": "Source Code",
          "type": "include"
        },
        "hf_utils.py": {
          "category": "Source Code",
          "type": "include"
        },
        "llama_cpp_utils.py": {
          "category": "Source Code",
          "type": "include"
        },
        "nvidia.py": {
          "category": "Source Code",
          "type": "include"
        },
        "server_utils.py": {
          "category": "Source Code",
          "type": "include"
        }
      },
      "directories": [
        "getsolotech-solo-server",
        "assets",
        "logo",
        "example_apps",
        "ai-chat",
        "public",
        "images",
        "src",
        "app",
        "examples",
        "containers",
        "Browser Use",
        "Computer Use",
        "Cosmos",
        "JAX",
        "LITA",
        "LeRobot",
        "NanoLLM",
        "NanoOWL",
        "NanoSAM",
        "OpenEMMA",
        "Policy",
        "Pytorch",
        "ROS",
        "Tensorflow",
        "Transformers",
        "VILA",
        "homeassistant-core",
        "langchain",
        "llama-index",
        "llama.cpp",
        "llava",
        "ollama",
        "onnx",
        "piper",
        "rag",
        "txtai",
        "vLLM",
        "whisper",
        "xtts",
        "endpoints",
        "CLiP",
        "GLiNer",
        "Paligemma-2",
        "agents-llm",
        "deepseek-r1",
        "huggingface",
        "llama3.2-rag-vllm",
        "llama3.2-vision",
        "miniCPM",
        "ocr",
        "qwen2.5",
        "sentence-transformers",
        "tensorTorch",
        "whisper",
        "xgboost",
        "xtts",
        "solo_server",
        "commands",
        "config",
        "utils"
      ]
    }
  },
  "raw_response": {
    "summary": "Repository: getsolotech/solo-server\nFiles analyzed: 119\n\nEstimated tokens: 99.4k",
    "tree": "Directory structure:\nâ””â”€â”€ getsolotech-solo-server/\n    â”œâ”€â”€ README.md\n    â”œâ”€â”€ LICENSE\n    â”œâ”€â”€ MANIFEST.in\n    â”œâ”€â”€ install.sh\n    â”œâ”€â”€ setup.py\n    â”œâ”€â”€ assets/\n    â”‚   â””â”€â”€ logo/\n    â”œâ”€â”€ example_apps/\n    â”‚   â””â”€â”€ ai-chat/\n    â”‚       â”œâ”€â”€ README.md\n    â”‚       â”œâ”€â”€ next.config.js\n    â”‚       â”œâ”€â”€ package-lock.json\n    â”‚       â”œâ”€â”€ package.json\n    â”‚       â”œâ”€â”€ postcss.config.js\n    â”‚       â”œâ”€â”€ tailwind.config.js\n    â”‚       â”œâ”€â”€ tsconfig.json\n    â”‚       â”œâ”€â”€ .gitignore\n    â”‚       â”œâ”€â”€ public/\n    â”‚       â”‚   â””â”€â”€ images/\n    â”‚       â””â”€â”€ src/\n    â”‚           â””â”€â”€ app/\n    â”‚               â”œâ”€â”€ globals.css\n    â”‚               â”œâ”€â”€ layout.tsx\n    â”‚               â”œâ”€â”€ page.module.css\n    â”‚               â””â”€â”€ page.tsx\n    â”œâ”€â”€ examples/\n    â”‚   â”œâ”€â”€ containers/\n    â”‚   â”‚   â”œâ”€â”€ Dockerfile.finetune\n    â”‚   â”‚   â”œâ”€â”€ Browser Use/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ Computer Use/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ Cosmos/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ JAX/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ LITA/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ LeRobot/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ NanoLLM/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ NanoOWL/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ NanoSAM/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ OpenEMMA/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ Policy/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ Pytorch/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ ROS/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ Tensorflow/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ Transformers/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ VILA/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ homeassistant-core/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ langchain/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ llama-index/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ llama.cpp/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ llava/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ ollama/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ onnx/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ piper/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ rag/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ txtai/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ vLLM/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â”œâ”€â”€ whisper/\n    â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile\n    â”‚   â”‚   â””â”€â”€ xtts/\n    â”‚   â”‚       â””â”€â”€ Dockerfile\n    â”‚   â””â”€â”€ endpoints/\n    â”‚       â”œâ”€â”€ CLiP/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ GLiNer/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ Paligemma-2/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ agents-llm/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ deepseek-r1/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ huggingface/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ llama3.2-rag-vllm/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ llama3.2-vision/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ miniCPM/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ ocr/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ qwen2.5/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ sentence-transformers/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ tensorTorch/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ whisper/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â”œâ”€â”€ xgboost/\n    â”‚       â”‚   â”œâ”€â”€ client.py\n    â”‚       â”‚   â”œâ”€â”€ model.py\n    â”‚       â”‚   â””â”€â”€ server.py\n    â”‚       â””â”€â”€ xtts/\n    â”‚           â”œâ”€â”€ client.py\n    â”‚           â”œâ”€â”€ model.py\n    â”‚           â””â”€â”€ server.py\n    â””â”€â”€ solo_server/\n        â”œâ”€â”€ __init__.py\n        â”œâ”€â”€ cli.py\n        â”œâ”€â”€ finetune_script.py\n        â”œâ”€â”€ main.py\n        â”œâ”€â”€ utils.py\n        â”œâ”€â”€ commands/\n        â”‚   â”œâ”€â”€ __init__.py\n        â”‚   â”œâ”€â”€ benchmark.py\n        â”‚   â”œâ”€â”€ download_hf.py\n        â”‚   â”œâ”€â”€ finetune.py\n        â”‚   â”œâ”€â”€ models_list.py\n        â”‚   â”œâ”€â”€ serve.py\n        â”‚   â”œâ”€â”€ status.py\n        â”‚   â”œâ”€â”€ stop.py\n        â”‚   â””â”€â”€ test.py\n        â”œâ”€â”€ config/\n        â”‚   â”œâ”€â”€ __init__.py\n        â”‚   â”œâ”€â”€ config.yaml\n        â”‚   â””â”€â”€ config_loader.py\n        â””â”€â”€ utils/\n            â”œâ”€â”€ __init__.py\n            â”œâ”€â”€ docker_utils.py\n            â”œâ”€â”€ hardware.py\n            â”œâ”€â”€ hf_utils.py\n            â”œâ”€â”€ llama_cpp_utils.py\n            â”œâ”€â”€ nvidia.py\n            â””â”€â”€ server_utils.py\n",
    "content": "================================================\nFile: README.md\n================================================\n# Solo Server\n\n<div align=\"center\">\n\n<img src=\"assets/logo/logo.png\" alt=\"Solovision Logo\" width=\"200\"/>\n\n[![Python 3.9+](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/pypi/l/solo-server)](https://opensource.org/licenses/MIT)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/solo-server)](https://pypi.org/project/solo-server/)\n[![PyPI - Version](https://img.shields.io/pypi/v/solo-server)](https://pypi.org/project/solo-server/)\n\nSolo Server is a lightweight and performant server for Physical AI inference.\n\n</div>\n\n\n```bash\n# Install the solo-server package using pip\npip install solo-server\n\n# Run the solo server setup in simple mode\nsolo setup\n```\n\nhttps://github.com/user-attachments/assets/22b424da-6693-497f-9d9f-c45894c520e0\n\n\n## Features\n\n- **Seamless Setup:** Manage your on device AI with a simple CLI and HTTP servers\n- **Open Model Registry:** Pull models from registries like  Ollama & Hugging Face\n- **Cross-Platform Compatibility:** Deploy AI models effortlessly on your hardware\n- **Configurable Framework:** Auto-detect hardware (CPU, GPU, RAM) and sets configs\n\n## Table of Contents\n\n- [Features](#-features)\n- [Installation](#installation)\n- [Commands](#commands)\n- [Contribution](#contribution)\n- [ Inspiration](#inspiration)\n\n## Installation\n\n### **ðŸ”¹Prerequisites** \n\n- **ðŸ‹ Docker:** Required for containerization \n  - [Install Docker](https://docs.docker.com/get-docker/)\n\n### **ðŸ”¹ Install Solo Server**\n```sh\n# Install Solo-Server\npip install solo-server\n```\n\nRun the **interactive setup** to configure Solo Server:\n```sh\n# Setup Solo-Server\nsolo setup\n```\n### **ðŸ”¹ Setup Features**\nâœ”ï¸ **Detects CPU, GPU, RAM** for **hardware-optimized execution**  \nâœ”ï¸ **Auto-configures `solo.conf` with optimal settings**  \nâœ”ï¸ **Recommends the compute backend OCI (CUDA, HIP, SYCL, Vulkan, CPU, Metal)**  \n\n---\n\n```sh\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ System Information â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ Operating System: Windows                              â”‚\nâ”‚ CPU: AMD64 Family 23 Model 96 Stepping 1, AuthenticAMD â”‚\nâ”‚ CPU Cores: 8                                           â”‚\nâ”‚ Memory: 15.42GB                                        â”‚\nâ”‚ GPU: NVIDIA                                            â”‚\nâ”‚ GPU Model: NVIDIA GeForce GTX 1660 Ti                  â”‚\nâ”‚ GPU Memory: 6144.0GB                                   â”‚\nâ”‚ Compute Backend: CUDA                                  â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n\nðŸ–¥ï¸  Detected GPU: NVIDIA GeForce GTX 1660 Ti (NVIDIA)\nâœ… NVIDIA GPU drivers and toolkit are correctly installed.\nWould you like to use GPU for inference? [y/n] (y): y\n\nðŸ¢ Choose the domain that best describes your field:\n  1. Personal\n  2. Education\n  3. Agriculture\n  4. Software\n  5. Healthcare\n  6. Forensics\n  7. Robotics\n  8. Enterprise\n  9. Custom\nEnter the number of your domain (1):\n```\n## **Commands**\n```\nâ•­â”€ Commands â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ setup      Set up Solo Server environment with interactive prompts and saves configuration to config.json.                     â”‚\nâ”‚ serve      Start a model server with the specified model.                                                                      â”‚\nâ”‚ status     Check running models, system status, and configuration.                                                             â”‚\nâ”‚ list       List all downloaded models available in HuggingFace cache and Ollama.                                               â”‚\nâ”‚ test       Test if the Solo server is running correctly. Performs an inference test to verify server functionality.            â”‚\nâ”‚ stop       Stops Solo Server services. If a server type is specified (e.g., 'ollama', 'vllm', 'llama.cpp'), only that specific â”‚\nâ”‚            service will be stopped. Otherwise, all Solo services will be stopped.                                              â”‚\nâ”‚ download   Downloads a Hugging Face model using the huggingface repo id.                                                       â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n\n```\n### **Serve a Model**\n```sh\nsolo serve -s ollama -m llama3.2\n```\n```\nâ•­â”€ Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ --model   -m      TEXT     Model name or path. Can be: - HuggingFace repo ID (e.g., 'meta-llama/Llama-3.2-1B-Instruct') -      â”‚\nâ”‚                            Ollama model Registry (e.g., 'llama3.2') - Local path to a model file (e.g., '/path/to/model.gguf') â”‚\nâ”‚                            If not specified, the default model from configuration will be used.                                â”‚\nâ”‚                            [default: None]                                                                                     â”‚\nâ”‚ --server  -s      TEXT     Server type (ollama, vllm, llama.cpp) [default: None]                                               â”‚\nâ”‚ --port    -p      INTEGER  Port to run the server on [default: None]                                                           â”‚\nâ”‚ --ui                       Start the UI for the server [default: True]                                                         â”‚\nâ”‚ --help                     Show this message and exit.                                                                         â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n```\n\n### **List Available Models**\nView all downloaded models in your HuggingFace cache and Ollama:\n\n```sh\nsolo list\n```\n### **Stop Solo Server**\n```sh\nsolo stop \n```\n## REST API\n\nSolo Server provides consistent REST API endpoints across different server types (Ollama, vLLM, llama.cpp). The exact API endpoint and format differs slightly depending on which server type you're using.\n\n### API Endpoints by Server Type\n\n#### Ollama API \n\n```shell\n# Generate a response\ncurl http://localhost:5070/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n\n# Chat with a model\ncurl http://localhost:5070/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ]\n}'\n```\n\n#### vLLM and llama.cpp API \nBoth use OpenAI-compatible endpoints:\n\n```shell\n# Chat completion\ncurl http://localhost:5070/v1/chat/completions -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"Why is the sky blue?\" }\n  ],\n  \"max_tokens\": 50,\n  \"temperature\": 0.7\n}'\n\n# Text completion\ncurl http://localhost:5070/v1/completions -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"max_tokens\": 50,\n  \"temperature\": 0.7\n}'\n```\n\n## ðŸ“ Contributions \nRefer example_apps for sample applications.\n1. [ai-chat](https://github.com/GetSoloTech/solo-server/tree/main/example_apps/ai-chat)\n\n\n### **ðŸ”¹ To Contribute, Setup in Dev Mode**\n\n```sh\n# Clone the repository\ngit clone https://github.com/GetSoloTech/solo-server.git\n\n# Navigate to the directory\ncd solo-server\n\n# Create and activate virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Unix/MacOS\n# OR\n.venv\\Scripts\\activate     # On Windows\n\n# Install in editable mode\npip install -e .\n```\n\n\n\n## ðŸ“ Project Inspiration \n\nThis project wouldn't be possible without the help of other projects like:\n\n* uv\n* llama.cpp\n* ramalama\n* ollama\n* whisper.cpp\n* vllm\n* podman\n* huggingface\n* aiaio\n* llamafile\n* cog\n\nLike using Solo, consider leaving us a â­ on GitHub\n\n\n\n\n================================================\nFile: LICENSE\n================================================\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\n\n================================================\nFile: MANIFEST.in\n================================================\ninclude solo_server/config/*.yaml \n\n\n================================================\nFile: install.sh\n================================================\n#!/usr/bin/env bash\n# install python packages required for running build.sh/autotag\n# and link these scripts under /usr/local so they're in the path\nset -ex\n\nROOT=\"$(dirname \"$(readlink -f \"$0\")\")\"\nINSTALL_PREFIX=\"/usr/local/bin\"\n\n# install pip if needed\npip3 --version || sudo apt-get install python3-pip\n\n# install package requirements\npip3 install -r $ROOT/requirements.txt\n\n\n================================================\nFile: setup.py\n================================================\nfrom setuptools import setup, find_packages\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"solo-server\",\n    version=\"0.4.4\",\n    author=\"Dhruv Diddi\",\n    author_email=\"dhruv.diddi@gmail.com\",\n    description=\"Platform for Hardware Aware Inference.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/GetSoloTech/solo-server\",\n    packages=find_packages(include=[\"solo_server\", \"solo_server.*\"]),\n    include_package_data=True,\n    package_data={\n        \"solo_server.config\": [\"*.yaml\"],\n    },\n    install_requires=[\n        \"typer\",\n        \"GPUtil\",\n        \"psutil\",\n        \"requests\", \n        \"rich\",\n        \"huggingface_hub\",\n        \"pydantic\",\n    ],\n    extras_require={\n        \"dev\": [\"pytest\", \"black\", \"isort\"],\n    },\n    python_requires=\">=3.8\",\n    entry_points={\n        \"console_scripts\": [\n            \"solo=solo_server.cli:app\",\n        ],\n    },\n)\n\n\n\n================================================\nFile: example_apps/ai-chat/README.md\n================================================\n\nA Next.js application that provides AI-assisted frontend question-and-answer functionality using a local LLM server.\n\n![AI Frontend Assistant Demo]([/public/images/demo.png](https://raw.githubusercontent.com/GetSoloTech/solo-server/refs/heads/main/example_apps/ai-chat/public/images/demo.png))\n\n## Features\n\n- Real-time streaming responses\n- Clean, modern UI\n- Typing indicator animation\n- Mobile-responsive design\n- Message history display\n\n## Getting Started\n\n1. Install dependencies:\n```bash\nnpm install\n```\n\n2. Start the development server:\n```bash\nnpm run dev\n```\n\n3. Ensure the local LLM server is running on port 11434\n\n4. Open [http://localhost:3000](http://localhost:3000) in your browser\n\n## Usage\n\nSimply type your frontend-related questions in the input field and press Enter or click Send. The AI will respond in real-time with helpful answers.\n\n## Demo\n\nThe application provides a modern chat interface where you can ask questions about frontend development topics. The AI assistant will provide helpful responses in real-time, with a typing animation to indicate when it's generating a response.\n\n![AI Frontend Assistant Demo](/public/images/demo.png)\n\n\n\n================================================\nFile: example_apps/ai-chat/next.config.js\n================================================\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  reactStrictMode: true,\n  swcMinify: true,\n}\n\nmodule.exports = nextConfig;\n\n\n\n================================================\nFile: example_apps/ai-chat/package-lock.json\n================================================\n{\n  \"name\": \"ai-chat\",\n  \"version\": \"0.1.0\",\n  \"lockfileVersion\": 3,\n  \"requires\": true,\n  \"packages\": {\n    \"\": {\n      \"name\": \"ai-chat\",\n      \"version\": \"0.1.0\",\n      \"dependencies\": {\n        \"next\": \"^14.0.0\",\n        \"react\": \"^18.2.0\",\n        \"react-dom\": \"^18.2.0\"\n      },\n      \"devDependencies\": {\n        \"@types/node\": \"22.13.9\",\n        \"@types/react\": \"19.0.10\",\n        \"autoprefixer\": \"^10.4.16\",\n        \"postcss\": \"^8.4.31\",\n        \"tailwindcss\": \"^3.3.5\",\n        \"typescript\": \"5.8.2\"\n      }\n    },\n    \"node_modules/@alloc/quick-lru\": {\n      \"version\": \"5.2.0\",\n      \"resolved\": \"https://registry.npmjs.org/@alloc/quick-lru/-/quick-lru-5.2.0.tgz\",\n      \"integrity\": \"sha512-UrcABB+4bUrFABwbluTIBErXwvbsU/V7TZWfmbgJfbkwiBuziS9gxdODUyuiecfdGQ85jglMW6juS3+z5TsKLw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=10\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/sindresorhus\"\n      }\n    },\n    \"node_modules/@isaacs/cliui\": {\n      \"version\": \"8.0.2\",\n      \"resolved\": \"https://registry.npmjs.org/@isaacs/cliui/-/cliui-8.0.2.tgz\",\n      \"integrity\": \"sha512-O8jcjabXaleOG9DQ0+ARXWZBTfnP4WNAqzuiJK7ll44AmxGKv/J2M4TPjxjY3znBCfvBXFzucm1twdyFybFqEA==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"string-width\": \"^5.1.2\",\n        \"string-width-cjs\": \"npm:string-width@^4.2.0\",\n        \"strip-ansi\": \"^7.0.1\",\n        \"strip-ansi-cjs\": \"npm:strip-ansi@^6.0.1\",\n        \"wrap-ansi\": \"^8.1.0\",\n        \"wrap-ansi-cjs\": \"npm:wrap-ansi@^7.0.0\"\n      },\n      \"engines\": {\n        \"node\": \">=12\"\n      }\n    },\n    \"node_modules/@jridgewell/gen-mapping\": {\n      \"version\": \"0.3.8\",\n      \"resolved\": \"https://registry.npmjs.org/@jridgewell/gen-mapping/-/gen-mapping-0.3.8.tgz\",\n      \"integrity\": \"sha512-imAbBGkb+ebQyxKgzv5Hu2nmROxoDOXHh80evxdoXNOrvAnVx7zimzc1Oo5h9RlfV4vPXaE2iM5pOFbvOCClWA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"@jridgewell/set-array\": \"^1.2.1\",\n        \"@jridgewell/sourcemap-codec\": \"^1.4.10\",\n        \"@jridgewell/trace-mapping\": \"^0.3.24\"\n      },\n      \"engines\": {\n        \"node\": \">=6.0.0\"\n      }\n    },\n    \"node_modules/@jridgewell/resolve-uri\": {\n      \"version\": \"3.1.2\",\n      \"resolved\": \"https://registry.npmjs.org/@jridgewell/resolve-uri/-/resolve-uri-3.1.2.tgz\",\n      \"integrity\": \"sha512-bRISgCIjP20/tbWSPWMEi54QVPRZExkuD9lJL+UIxUKtwVJA8wW1Trb1jMs1RFXo1CBTNZ/5hpC9QvmKWdopKw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=6.0.0\"\n      }\n    },\n    \"node_modules/@jridgewell/set-array\": {\n      \"version\": \"1.2.1\",\n      \"resolved\": \"https://registry.npmjs.org/@jridgewell/set-array/-/set-array-1.2.1.tgz\",\n      \"integrity\": \"sha512-R8gLRTZeyp03ymzP/6Lil/28tGeGEzhx1q2k703KGWRAI1VdvPIXdG70VJc2pAMw3NA6JKL5hhFu1sJX0Mnn/A==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=6.0.0\"\n      }\n    },\n    \"node_modules/@jridgewell/sourcemap-codec\": {\n      \"version\": \"1.5.0\",\n      \"resolved\": \"https://registry.npmjs.org/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.0.tgz\",\n      \"integrity\": \"sha512-gv3ZRaISU3fjPAgNsriBRqGWQL6quFx04YMPW/zD8XMLsU32mhCCbfbO6KZFLjvYpCZ8zyDEgqsgf+PwPaM7GQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/@jridgewell/trace-mapping\": {\n      \"version\": \"0.3.25\",\n      \"resolved\": \"https://registry.npmjs.org/@jridgewell/trace-mapping/-/trace-mapping-0.3.25.tgz\",\n      \"integrity\": \"sha512-vNk6aEwybGtawWmy/PzwnGDOjCkLWSD2wqvjGGAgOAwCGWySYXfYoxt00IJkTF+8Lb57DwOb3Aa0o9CApepiYQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"@jridgewell/resolve-uri\": \"^3.1.0\",\n        \"@jridgewell/sourcemap-codec\": \"^1.4.14\"\n      }\n    },\n    \"node_modules/@next/env\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/env/-/env-14.2.24.tgz\",\n      \"integrity\": \"sha512-LAm0Is2KHTNT6IT16lxT+suD0u+VVfYNQqM+EJTKuFRRuY2z+zj01kueWXPCxbMBDt0B5vONYzabHGUNbZYAhA==\",\n      \"license\": \"MIT\"\n    },\n    \"node_modules/@next/swc-darwin-arm64\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/swc-darwin-arm64/-/swc-darwin-arm64-14.2.24.tgz\",\n      \"integrity\": \"sha512-7Tdi13aojnAZGpapVU6meVSpNzgrFwZ8joDcNS8cJVNuP3zqqrLqeory9Xec5TJZR/stsGJdfwo8KeyloT3+rQ==\",\n      \"cpu\": [\n        \"arm64\"\n      ],\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"darwin\"\n      ],\n      \"engines\": {\n        \"node\": \">= 10\"\n      }\n    },\n    \"node_modules/@next/swc-darwin-x64\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/swc-darwin-x64/-/swc-darwin-x64-14.2.24.tgz\",\n      \"integrity\": \"sha512-lXR2WQqUtu69l5JMdTwSvQUkdqAhEWOqJEYUQ21QczQsAlNOW2kWZCucA6b3EXmPbcvmHB1kSZDua/713d52xg==\",\n      \"cpu\": [\n        \"x64\"\n      ],\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"darwin\"\n      ],\n      \"engines\": {\n        \"node\": \">= 10\"\n      }\n    },\n    \"node_modules/@next/swc-linux-arm64-gnu\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/swc-linux-arm64-gnu/-/swc-linux-arm64-gnu-14.2.24.tgz\",\n      \"integrity\": \"sha512-nxvJgWOpSNmzidYvvGDfXwxkijb6hL9+cjZx1PVG6urr2h2jUqBALkKjT7kpfurRWicK6hFOvarmaWsINT1hnA==\",\n      \"cpu\": [\n        \"arm64\"\n      ],\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"linux\"\n      ],\n      \"engines\": {\n        \"node\": \">= 10\"\n      }\n    },\n    \"node_modules/@next/swc-linux-arm64-musl\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/swc-linux-arm64-musl/-/swc-linux-arm64-musl-14.2.24.tgz\",\n      \"integrity\": \"sha512-PaBgOPhqa4Abxa3y/P92F3kklNPsiFjcjldQGT7kFmiY5nuFn8ClBEoX8GIpqU1ODP2y8P6hio6vTomx2Vy0UQ==\",\n      \"cpu\": [\n        \"arm64\"\n      ],\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"linux\"\n      ],\n      \"engines\": {\n        \"node\": \">= 10\"\n      }\n    },\n    \"node_modules/@next/swc-linux-x64-gnu\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/swc-linux-x64-gnu/-/swc-linux-x64-gnu-14.2.24.tgz\",\n      \"integrity\": \"sha512-vEbyadiRI7GOr94hd2AB15LFVgcJZQWu7Cdi9cWjCMeCiUsHWA0U5BkGPuoYRnTxTn0HacuMb9NeAmStfBCLoQ==\",\n      \"cpu\": [\n        \"x64\"\n      ],\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"linux\"\n      ],\n      \"engines\": {\n        \"node\": \">= 10\"\n      }\n    },\n    \"node_modules/@next/swc-linux-x64-musl\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/swc-linux-x64-musl/-/swc-linux-x64-musl-14.2.24.tgz\",\n      \"integrity\": \"sha512-df0FC9ptaYsd8nQCINCzFtDWtko8PNRTAU0/+d7hy47E0oC17tI54U/0NdGk7l/76jz1J377dvRjmt6IUdkpzQ==\",\n      \"cpu\": [\n        \"x64\"\n      ],\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"linux\"\n      ],\n      \"engines\": {\n        \"node\": \">= 10\"\n      }\n    },\n    \"node_modules/@next/swc-win32-arm64-msvc\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/swc-win32-arm64-msvc/-/swc-win32-arm64-msvc-14.2.24.tgz\",\n      \"integrity\": \"sha512-ZEntbLjeYAJ286eAqbxpZHhDFYpYjArotQ+/TW9j7UROh0DUmX7wYDGtsTPpfCV8V+UoqHBPU7q9D4nDNH014Q==\",\n      \"cpu\": [\n        \"arm64\"\n      ],\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"win32\"\n      ],\n      \"engines\": {\n        \"node\": \">= 10\"\n      }\n    },\n    \"node_modules/@next/swc-win32-ia32-msvc\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/swc-win32-ia32-msvc/-/swc-win32-ia32-msvc-14.2.24.tgz\",\n      \"integrity\": \"sha512-9KuS+XUXM3T6v7leeWU0erpJ6NsFIwiTFD5nzNg8J5uo/DMIPvCp3L1Ao5HjbHX0gkWPB1VrKoo/Il4F0cGK2Q==\",\n      \"cpu\": [\n        \"ia32\"\n      ],\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"win32\"\n      ],\n      \"engines\": {\n        \"node\": \">= 10\"\n      }\n    },\n    \"node_modules/@next/swc-win32-x64-msvc\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/@next/swc-win32-x64-msvc/-/swc-win32-x64-msvc-14.2.24.tgz\",\n      \"integrity\": \"sha512-cXcJ2+x0fXQ2CntaE00d7uUH+u1Bfp/E0HsNQH79YiLaZE5Rbm7dZzyAYccn3uICM7mw+DxoMqEfGXZtF4Fgaw==\",\n      \"cpu\": [\n        \"x64\"\n      ],\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"win32\"\n      ],\n      \"engines\": {\n        \"node\": \">= 10\"\n      }\n    },\n    \"node_modules/@nodelib/fs.scandir\": {\n      \"version\": \"2.1.5\",\n      \"resolved\": \"https://registry.npmjs.org/@nodelib/fs.scandir/-/fs.scandir-2.1.5.tgz\",\n      \"integrity\": \"sha512-vq24Bq3ym5HEQm2NKCr3yXDwjc7vTsEThRDnkp2DK9p1uqLR+DHurm/NOTo0KG7HYHU7eppKZj3MyqYuMBf62g==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"@nodelib/fs.stat\": \"2.0.5\",\n        \"run-parallel\": \"^1.1.9\"\n      },\n      \"engines\": {\n        \"node\": \">= 8\"\n      }\n    },\n    \"node_modules/@nodelib/fs.stat\": {\n      \"version\": \"2.0.5\",\n      \"resolved\": \"https://registry.npmjs.org/@nodelib/fs.stat/-/fs.stat-2.0.5.tgz\",\n      \"integrity\": \"sha512-RkhPPp2zrqDAQA/2jNhnztcPAlv64XdhIp7a7454A5ovI7Bukxgt7MX7udwAu3zg1DcpPU0rz3VV1SeaqvY4+A==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">= 8\"\n      }\n    },\n    \"node_modules/@nodelib/fs.walk\": {\n      \"version\": \"1.2.8\",\n      \"resolved\": \"https://registry.npmjs.org/@nodelib/fs.walk/-/fs.walk-1.2.8.tgz\",\n      \"integrity\": \"sha512-oGB+UxlgWcgQkgwo8GcEGwemoTFt3FIO9ababBmaGwXIoBKZ+GTy0pP185beGg7Llih/NSHSV2XAs1lnznocSg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"@nodelib/fs.scandir\": \"2.1.5\",\n        \"fastq\": \"^1.6.0\"\n      },\n      \"engines\": {\n        \"node\": \">= 8\"\n      }\n    },\n    \"node_modules/@pkgjs/parseargs\": {\n      \"version\": \"0.11.0\",\n      \"resolved\": \"https://registry.npmjs.org/@pkgjs/parseargs/-/parseargs-0.11.0.tgz\",\n      \"integrity\": \"sha512-+1VkjdD0QBLPodGrJUeqarH8VAIvQODIbwh9XpP5Syisf7YoQgsJKPNFoqqLQlu+VQ/tVSshMR6loPMn8U+dPg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"engines\": {\n        \"node\": \">=14\"\n      }\n    },\n    \"node_modules/@swc/counter\": {\n      \"version\": \"0.1.3\",\n      \"resolved\": \"https://registry.npmjs.org/@swc/counter/-/counter-0.1.3.tgz\",\n      \"integrity\": \"sha512-e2BR4lsJkkRlKZ/qCHPw9ZaSxc0MVUd7gtbtaB7aMvHeJVYe8sOB8DBZkP2DtISHGSku9sCK6T6cnY0CtXrOCQ==\",\n      \"license\": \"Apache-2.0\"\n    },\n    \"node_modules/@swc/helpers\": {\n      \"version\": \"0.5.5\",\n      \"resolved\": \"https://registry.npmjs.org/@swc/helpers/-/helpers-0.5.5.tgz\",\n      \"integrity\": \"sha512-KGYxvIOXcceOAbEk4bi/dVLEK9z8sZ0uBB3Il5b1rhfClSpcX0yfRO0KmTkqR2cnQDymwLB+25ZyMzICg/cm/A==\",\n      \"license\": \"Apache-2.0\",\n      \"dependencies\": {\n        \"@swc/counter\": \"^0.1.3\",\n        \"tslib\": \"^2.4.0\"\n      }\n    },\n    \"node_modules/@types/node\": {\n      \"version\": \"22.13.9\",\n      \"resolved\": \"https://registry.npmjs.org/@types/node/-/node-22.13.9.tgz\",\n      \"integrity\": \"sha512-acBjXdRJ3A6Pb3tqnw9HZmyR3Fiol3aGxRCK1x3d+6CDAMjl7I649wpSd+yNURCjbOUGu9tqtLKnTGxmK6CyGw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"undici-types\": \"~6.20.0\"\n      }\n    },\n    \"node_modules/@types/react\": {\n      \"version\": \"19.0.10\",\n      \"resolved\": \"https://registry.npmjs.org/@types/react/-/react-19.0.10.tgz\",\n      \"integrity\": \"sha512-JuRQ9KXLEjaUNjTWpzuR231Z2WpIwczOkBEIvbHNCzQefFIT0L8IqE6NV6ULLyC1SI/i234JnDoMkfg+RjQj2g==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"csstype\": \"^3.0.2\"\n      }\n    },\n    \"node_modules/ansi-regex\": {\n      \"version\": \"6.1.0\",\n      \"resolved\": \"https://registry.npmjs.org/ansi-regex/-/ansi-regex-6.1.0.tgz\",\n      \"integrity\": \"sha512-7HSX4QQb4CspciLpVFwyRe79O3xsIZDDLER21kERQ71oaPodF8jL725AgJMFAYbooIqolJoRLuM81SpeUkpkvA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=12\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/chalk/ansi-regex?sponsor=1\"\n      }\n    },\n    \"node_modules/ansi-styles\": {\n      \"version\": \"6.2.1\",\n      \"resolved\": \"https://registry.npmjs.org/ansi-styles/-/ansi-styles-6.2.1.tgz\",\n      \"integrity\": \"sha512-bN798gFfQX+viw3R7yrGWRqnrN2oRkEkUjjl4JNn4E8GxxbjtG3FbrEIIY3l8/hrwUwIeCZvi4QuOTP4MErVug==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=12\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/chalk/ansi-styles?sponsor=1\"\n      }\n    },\n    \"node_modules/any-promise\": {\n      \"version\": \"1.3.0\",\n      \"resolved\": \"https://registry.npmjs.org/any-promise/-/any-promise-1.3.0.tgz\",\n      \"integrity\": \"sha512-7UvmKalWRt1wgjL1RrGxoSJW/0QZFIegpeGvZG9kjp8vrRu55XTHbwnqq2GpXm9uLbcuhxm3IqX9OB4MZR1b2A==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/anymatch\": {\n      \"version\": \"3.1.3\",\n      \"resolved\": \"https://registry.npmjs.org/anymatch/-/anymatch-3.1.3.tgz\",\n      \"integrity\": \"sha512-KMReFUr0B4t+D+OBkjR3KYqvocp2XaSzO55UcB6mgQMd3KbcE+mWTyvVV7D/zsdEbNnV6acZUutkiHQXvTr1Rw==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"normalize-path\": \"^3.0.0\",\n        \"picomatch\": \"^2.0.4\"\n      },\n      \"engines\": {\n        \"node\": \">= 8\"\n      }\n    },\n    \"node_modules/arg\": {\n      \"version\": \"5.0.2\",\n      \"resolved\": \"https://registry.npmjs.org/arg/-/arg-5.0.2.tgz\",\n      \"integrity\": \"sha512-PYjyFOLKQ9y57JvQ6QLo8dAgNqswh8M1RMJYdQduT6xbWSgK36P/Z/v+p888pM69jMMfS8Xd8F6I1kQ/I9HUGg==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/autoprefixer\": {\n      \"version\": \"10.4.20\",\n      \"resolved\": \"https://registry.npmjs.org/autoprefixer/-/autoprefixer-10.4.20.tgz\",\n      \"integrity\": \"sha512-XY25y5xSv/wEoqzDyXXME4AFfkZI0P23z6Fs3YgymDnKJkCGOnkL0iTxCa85UTqaSgfcqyf3UA6+c7wUvx/16g==\",\n      \"dev\": true,\n      \"funding\": [\n        {\n          \"type\": \"opencollective\",\n          \"url\": \"https://opencollective.com/postcss/\"\n        },\n        {\n          \"type\": \"tidelift\",\n          \"url\": \"https://tidelift.com/funding/github/npm/autoprefixer\"\n        },\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/ai\"\n        }\n      ],\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"browserslist\": \"^4.23.3\",\n        \"caniuse-lite\": \"^1.0.30001646\",\n        \"fraction.js\": \"^4.3.7\",\n        \"normalize-range\": \"^0.1.2\",\n        \"picocolors\": \"^1.0.1\",\n        \"postcss-value-parser\": \"^4.2.0\"\n      },\n      \"bin\": {\n        \"autoprefixer\": \"bin/autoprefixer\"\n      },\n      \"engines\": {\n        \"node\": \"^10 || ^12 || >=14\"\n      },\n      \"peerDependencies\": {\n        \"postcss\": \"^8.1.0\"\n      }\n    },\n    \"node_modules/balanced-match\": {\n      \"version\": \"1.0.2\",\n      \"resolved\": \"https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz\",\n      \"integrity\": \"sha512-3oSeUO0TMV67hN1AmbXsK4yaqU7tjiHlbxRDZOpH0KW9+CeX4bRAaX0Anxt0tx2MrpRpWwQaPwIlISEJhYU5Pw==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/binary-extensions\": {\n      \"version\": \"2.3.0\",\n      \"resolved\": \"https://registry.npmjs.org/binary-extensions/-/binary-extensions-2.3.0.tgz\",\n      \"integrity\": \"sha512-Ceh+7ox5qe7LJuLHoY0feh3pHuUDHAcRUeyL2VYghZwfpkNIy/+8Ocg0a3UuSoYzavmylwuLWQOf3hl0jjMMIw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=8\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/sindresorhus\"\n      }\n    },\n    \"node_modules/brace-expansion\": {\n      \"version\": \"2.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/brace-expansion/-/brace-expansion-2.0.1.tgz\",\n      \"integrity\": \"sha512-XnAIvQ8eM+kC6aULx6wuQiwVsnzsi9d3WxzV3FpWTGA19F621kwdbsAcFKXgKUHZWsy+mY6iL1sHTxWEFCytDA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"balanced-match\": \"^1.0.0\"\n      }\n    },\n    \"node_modules/braces\": {\n      \"version\": \"3.0.3\",\n      \"resolved\": \"https://registry.npmjs.org/braces/-/braces-3.0.3.tgz\",\n      \"integrity\": \"sha512-yQbXgO/OSZVD2IsiLlro+7Hf6Q18EJrKSEsdoMzKePKXct3gvD8oLcOQdIzGupr5Fj+EDe8gO/lxc1BzfMpxvA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"fill-range\": \"^7.1.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/browserslist\": {\n      \"version\": \"4.24.4\",\n      \"resolved\": \"https://registry.npmjs.org/browserslist/-/browserslist-4.24.4.tgz\",\n      \"integrity\": \"sha512-KDi1Ny1gSePi1vm0q4oxSF8b4DR44GF4BbmS2YdhPLOEqd8pDviZOGH/GsmRwoWJ2+5Lr085X7naowMwKHDG1A==\",\n      \"dev\": true,\n      \"funding\": [\n        {\n          \"type\": \"opencollective\",\n          \"url\": \"https://opencollective.com/browserslist\"\n        },\n        {\n          \"type\": \"tidelift\",\n          \"url\": \"https://tidelift.com/funding/github/npm/browserslist\"\n        },\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/ai\"\n        }\n      ],\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"caniuse-lite\": \"^1.0.30001688\",\n        \"electron-to-chromium\": \"^1.5.73\",\n        \"node-releases\": \"^2.0.19\",\n        \"update-browserslist-db\": \"^1.1.1\"\n      },\n      \"bin\": {\n        \"browserslist\": \"cli.js\"\n      },\n      \"engines\": {\n        \"node\": \"^6 || ^7 || ^8 || ^9 || ^10 || ^11 || ^12 || >=13.7\"\n      }\n    },\n    \"node_modules/busboy\": {\n      \"version\": \"1.6.0\",\n      \"resolved\": \"https://registry.npmjs.org/busboy/-/busboy-1.6.0.tgz\",\n      \"integrity\": \"sha512-8SFQbg/0hQ9xy3UNTB0YEnsNBbWfhf7RtnzpL7TkBiTBRfrQ9Fxcnz7VJsleJpyp6rVLvXiuORqjlHi5q+PYuA==\",\n      \"dependencies\": {\n        \"streamsearch\": \"^1.1.0\"\n      },\n      \"engines\": {\n        \"node\": \">=10.16.0\"\n      }\n    },\n    \"node_modules/camelcase-css\": {\n      \"version\": \"2.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/camelcase-css/-/camelcase-css-2.0.1.tgz\",\n      \"integrity\": \"sha512-QOSvevhslijgYwRx6Rv7zKdMF8lbRmx+uQGx2+vDc+KI/eBnsy9kit5aj23AgGu3pa4t9AgwbnXWqS+iOY+2aA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">= 6\"\n      }\n    },\n    \"node_modules/caniuse-lite\": {\n      \"version\": \"1.0.30001702\",\n      \"resolved\": \"https://registry.npmjs.org/caniuse-lite/-/caniuse-lite-1.0.30001702.tgz\",\n      \"integrity\": \"sha512-LoPe/D7zioC0REI5W73PeR1e1MLCipRGq/VkovJnd6Df+QVqT+vT33OXCp8QUd7kA7RZrHWxb1B36OQKI/0gOA==\",\n      \"funding\": [\n        {\n          \"type\": \"opencollective\",\n          \"url\": \"https://opencollective.com/browserslist\"\n        },\n        {\n          \"type\": \"tidelift\",\n          \"url\": \"https://tidelift.com/funding/github/npm/caniuse-lite\"\n        },\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/ai\"\n        }\n      ],\n      \"license\": \"CC-BY-4.0\"\n    },\n    \"node_modules/chokidar\": {\n      \"version\": \"3.6.0\",\n      \"resolved\": \"https://registry.npmjs.org/chokidar/-/chokidar-3.6.0.tgz\",\n      \"integrity\": \"sha512-7VT13fmjotKpGipCW9JEQAusEPE+Ei8nl6/g4FBAmIm0GOOLMua9NDDo/DWp0ZAxCr3cPq5ZpBqmPAQgDda2Pw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"anymatch\": \"~3.1.2\",\n        \"braces\": \"~3.0.2\",\n        \"glob-parent\": \"~5.1.2\",\n        \"is-binary-path\": \"~2.1.0\",\n        \"is-glob\": \"~4.0.1\",\n        \"normalize-path\": \"~3.0.0\",\n        \"readdirp\": \"~3.6.0\"\n      },\n      \"engines\": {\n        \"node\": \">= 8.10.0\"\n      },\n      \"funding\": {\n        \"url\": \"https://paulmillr.com/funding/\"\n      },\n      \"optionalDependencies\": {\n        \"fsevents\": \"~2.3.2\"\n      }\n    },\n    \"node_modules/chokidar/node_modules/glob-parent\": {\n      \"version\": \"5.1.2\",\n      \"resolved\": \"https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz\",\n      \"integrity\": \"sha512-AOIgSQCepiJYwP3ARnGx+5VnTu2HBYdzbGP45eLw1vr3zB3vZLeyed1sC9hnbcOc9/SrMyM5RPQrkGz4aS9Zow==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"is-glob\": \"^4.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">= 6\"\n      }\n    },\n    \"node_modules/client-only\": {\n      \"version\": \"0.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/client-only/-/client-only-0.0.1.tgz\",\n      \"integrity\": \"sha512-IV3Ou0jSMzZrd3pZ48nLkT9DA7Ag1pnPzaiQhpW7c3RbcqqzvzzVu+L8gfqMp/8IM2MQtSiqaCxrrcfu8I8rMA==\",\n      \"license\": \"MIT\"\n    },\n    \"node_modules/color-convert\": {\n      \"version\": \"2.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz\",\n      \"integrity\": \"sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"color-name\": \"~1.1.4\"\n      },\n      \"engines\": {\n        \"node\": \">=7.0.0\"\n      }\n    },\n    \"node_modules/color-name\": {\n      \"version\": \"1.1.4\",\n      \"resolved\": \"https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz\",\n      \"integrity\": \"sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/commander\": {\n      \"version\": \"4.1.1\",\n      \"resolved\": \"https://registry.npmjs.org/commander/-/commander-4.1.1.tgz\",\n      \"integrity\": \"sha512-NOKm8xhkzAjzFx8B2v5OAHT+u5pRQc2UCa2Vq9jYL/31o2wi9mxBA7LIFs3sV5VSC49z6pEhfbMULvShKj26WA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">= 6\"\n      }\n    },\n    \"node_modules/cross-spawn\": {\n      \"version\": \"7.0.6\",\n      \"resolved\": \"https://registry.npmjs.org/cross-spawn/-/cross-spawn-7.0.6.tgz\",\n      \"integrity\": \"sha512-uV2QOWP2nWzsy2aMp8aRibhi9dlzF5Hgh5SHaB9OiTGEyDTiJJyx0uy51QXdyWbtAHNua4XJzUKca3OzKUd3vA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"path-key\": \"^3.1.0\",\n        \"shebang-command\": \"^2.0.0\",\n        \"which\": \"^2.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">= 8\"\n      }\n    },\n    \"node_modules/cssesc\": {\n      \"version\": \"3.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/cssesc/-/cssesc-3.0.0.tgz\",\n      \"integrity\": \"sha512-/Tb/JcjK111nNScGob5MNtsntNM1aCNUDipB/TkwZFhyDrrE47SOx/18wF2bbjgc3ZzCSKW1T5nt5EbFoAz/Vg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"bin\": {\n        \"cssesc\": \"bin/cssesc\"\n      },\n      \"engines\": {\n        \"node\": \">=4\"\n      }\n    },\n    \"node_modules/csstype\": {\n      \"version\": \"3.1.3\",\n      \"resolved\": \"https://registry.npmjs.org/csstype/-/csstype-3.1.3.tgz\",\n      \"integrity\": \"sha512-M1uQkMl8rQK/szD0LNhtqxIPLpimGm8sOBwU7lLnCpSbTyY3yeU1Vc7l4KT5zT4s/yOxHH5O7tIuuLOCnLADRw==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/didyoumean\": {\n      \"version\": \"1.2.2\",\n      \"resolved\": \"https://registry.npmjs.org/didyoumean/-/didyoumean-1.2.2.tgz\",\n      \"integrity\": \"sha512-gxtyfqMg7GKyhQmb056K7M3xszy/myH8w+B4RT+QXBQsvAOdc3XymqDDPHx1BgPgsdAA5SIifona89YtRATDzw==\",\n      \"dev\": true,\n      \"license\": \"Apache-2.0\"\n    },\n    \"node_modules/dlv\": {\n      \"version\": \"1.1.3\",\n      \"resolved\": \"https://registry.npmjs.org/dlv/-/dlv-1.1.3.tgz\",\n      \"integrity\": \"sha512-+HlytyjlPKnIG8XuRG8WvmBP8xs8P71y+SKKS6ZXWoEgLuePxtDoUEiH7WkdePWrQ5JBpE6aoVqfZfJUQkjXwA==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/eastasianwidth\": {\n      \"version\": \"0.2.0\",\n      \"resolved\": \"https://registry.npmjs.org/eastasianwidth/-/eastasianwidth-0.2.0.tgz\",\n      \"integrity\": \"sha512-I88TYZWc9XiYHRQ4/3c5rjjfgkjhLyW2luGIheGERbNQ6OY7yTybanSpDXZa8y7VUP9YmDcYa+eyq4ca7iLqWA==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/electron-to-chromium\": {\n      \"version\": \"1.5.112\",\n      \"resolved\": \"https://registry.npmjs.org/electron-to-chromium/-/electron-to-chromium-1.5.112.tgz\",\n      \"integrity\": \"sha512-oen93kVyqSb3l+ziUgzIOlWt/oOuy4zRmpwestMn4rhFWAoFJeFuCVte9F2fASjeZZo7l/Cif9TiyrdW4CwEMA==\",\n      \"dev\": true,\n      \"license\": \"ISC\"\n    },\n    \"node_modules/emoji-regex\": {\n      \"version\": \"9.2.2\",\n      \"resolved\": \"https://registry.npmjs.org/emoji-regex/-/emoji-regex-9.2.2.tgz\",\n      \"integrity\": \"sha512-L18DaJsXSUk2+42pv8mLs5jJT2hqFkFE4j21wOmgbUqsZ2hL72NsUU785g9RXgo3s0ZNgVl42TiHp3ZtOv/Vyg==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/escalade\": {\n      \"version\": \"3.2.0\",\n      \"resolved\": \"https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz\",\n      \"integrity\": \"sha512-WUj2qlxaQtO4g6Pq5c29GTcWGDyd8itL8zTlipgECz3JesAiiOKotd8JU6otB3PACgG6xkJUyVhboMS+bje/jA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=6\"\n      }\n    },\n    \"node_modules/fast-glob\": {\n      \"version\": \"3.3.3\",\n      \"resolved\": \"https://registry.npmjs.org/fast-glob/-/fast-glob-3.3.3.tgz\",\n      \"integrity\": \"sha512-7MptL8U0cqcFdzIzwOTHoilX9x5BrNqye7Z/LuC7kCMRio1EMSyqRK3BEAUD7sXRq4iT4AzTVuZdhgQ2TCvYLg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"@nodelib/fs.stat\": \"^2.0.2\",\n        \"@nodelib/fs.walk\": \"^1.2.3\",\n        \"glob-parent\": \"^5.1.2\",\n        \"merge2\": \"^1.3.0\",\n        \"micromatch\": \"^4.0.8\"\n      },\n      \"engines\": {\n        \"node\": \">=8.6.0\"\n      }\n    },\n    \"node_modules/fast-glob/node_modules/glob-parent\": {\n      \"version\": \"5.1.2\",\n      \"resolved\": \"https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz\",\n      \"integrity\": \"sha512-AOIgSQCepiJYwP3ARnGx+5VnTu2HBYdzbGP45eLw1vr3zB3vZLeyed1sC9hnbcOc9/SrMyM5RPQrkGz4aS9Zow==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"is-glob\": \"^4.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">= 6\"\n      }\n    },\n    \"node_modules/fastq\": {\n      \"version\": \"1.19.1\",\n      \"resolved\": \"https://registry.npmjs.org/fastq/-/fastq-1.19.1.tgz\",\n      \"integrity\": \"sha512-GwLTyxkCXjXbxqIhTsMI2Nui8huMPtnxg7krajPJAjnEG/iiOS7i+zCtWGZR9G0NBKbXKh6X9m9UIsYX/N6vvQ==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"reusify\": \"^1.0.4\"\n      }\n    },\n    \"node_modules/fill-range\": {\n      \"version\": \"7.1.1\",\n      \"resolved\": \"https://registry.npmjs.org/fill-range/-/fill-range-7.1.1.tgz\",\n      \"integrity\": \"sha512-YsGpe3WHLK8ZYi4tWDg2Jy3ebRz2rXowDxnld4bkQB00cc/1Zw9AWnC0i9ztDJitivtQvaI9KaLyKrc+hBW0yg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"to-regex-range\": \"^5.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/foreground-child\": {\n      \"version\": \"3.3.1\",\n      \"resolved\": \"https://registry.npmjs.org/foreground-child/-/foreground-child-3.3.1.tgz\",\n      \"integrity\": \"sha512-gIXjKqtFuWEgzFRJA9WCQeSJLZDjgJUOMCMzxtvFq/37KojM1BFGufqsCy0r4qSQmYLsZYMeyRqzIWOMup03sw==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"cross-spawn\": \"^7.0.6\",\n        \"signal-exit\": \"^4.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=14\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/isaacs\"\n      }\n    },\n    \"node_modules/fraction.js\": {\n      \"version\": \"4.3.7\",\n      \"resolved\": \"https://registry.npmjs.org/fraction.js/-/fraction.js-4.3.7.tgz\",\n      \"integrity\": \"sha512-ZsDfxO51wGAXREY55a7la9LScWpwv9RxIrYABrlvOFBlH/ShPnrtsXeuUIfXKKOVicNxQ+o8JTbJvjS4M89yew==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \"*\"\n      },\n      \"funding\": {\n        \"type\": \"patreon\",\n        \"url\": \"https://github.com/sponsors/rawify\"\n      }\n    },\n    \"node_modules/fsevents\": {\n      \"version\": \"2.3.3\",\n      \"resolved\": \"https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz\",\n      \"integrity\": \"sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==\",\n      \"dev\": true,\n      \"hasInstallScript\": true,\n      \"license\": \"MIT\",\n      \"optional\": true,\n      \"os\": [\n        \"darwin\"\n      ],\n      \"engines\": {\n        \"node\": \"^8.16.0 || ^10.6.0 || >=11.0.0\"\n      }\n    },\n    \"node_modules/function-bind\": {\n      \"version\": \"1.1.2\",\n      \"resolved\": \"https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz\",\n      \"integrity\": \"sha512-7XHNxH7qX9xG5mIwxkhumTox/MIRNcOgDrxWsMt2pAr23WHp6MrRlN7FBSFpCpr+oVO0F744iUgR82nJMfG2SA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/ljharb\"\n      }\n    },\n    \"node_modules/glob\": {\n      \"version\": \"10.4.5\",\n      \"resolved\": \"https://registry.npmjs.org/glob/-/glob-10.4.5.tgz\",\n      \"integrity\": \"sha512-7Bv8RF0k6xjo7d4A/PxYLbUCfb6c+Vpd2/mB2yRDlew7Jb5hEXiCD9ibfO7wpk8i4sevK6DFny9h7EYbM3/sHg==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"foreground-child\": \"^3.1.0\",\n        \"jackspeak\": \"^3.1.2\",\n        \"minimatch\": \"^9.0.4\",\n        \"minipass\": \"^7.1.2\",\n        \"package-json-from-dist\": \"^1.0.0\",\n        \"path-scurry\": \"^1.11.1\"\n      },\n      \"bin\": {\n        \"glob\": \"dist/esm/bin.mjs\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/isaacs\"\n      }\n    },\n    \"node_modules/glob-parent\": {\n      \"version\": \"6.0.2\",\n      \"resolved\": \"https://registry.npmjs.org/glob-parent/-/glob-parent-6.0.2.tgz\",\n      \"integrity\": \"sha512-XxwI8EOhVQgWp6iDL+3b0r86f4d6AX6zSU55HfB4ydCEuXLXc5FcYeOu+nnGftS4TEju/11rt4KJPTMgbfmv4A==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"is-glob\": \"^4.0.3\"\n      },\n      \"engines\": {\n        \"node\": \">=10.13.0\"\n      }\n    },\n    \"node_modules/graceful-fs\": {\n      \"version\": \"4.2.11\",\n      \"resolved\": \"https://registry.npmjs.org/graceful-fs/-/graceful-fs-4.2.11.tgz\",\n      \"integrity\": \"sha512-RbJ5/jmFcNNCcDV5o9eTnBLJ/HszWV0P73bc+Ff4nS/rJj+YaS6IGyiOL0VoBYX+l1Wrl3k63h/KrH+nhJ0XvQ==\",\n      \"license\": \"ISC\"\n    },\n    \"node_modules/hasown\": {\n      \"version\": \"2.0.2\",\n      \"resolved\": \"https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz\",\n      \"integrity\": \"sha512-0hJU9SCPvmMzIBdZFqNPXWa6dqh7WdH0cII9y+CyS8rG3nL48Bclra9HmKhVVUHyPWNH5Y7xDwAB7bfgSjkUMQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"function-bind\": \"^1.1.2\"\n      },\n      \"engines\": {\n        \"node\": \">= 0.4\"\n      }\n    },\n    \"node_modules/is-binary-path\": {\n      \"version\": \"2.1.0\",\n      \"resolved\": \"https://registry.npmjs.org/is-binary-path/-/is-binary-path-2.1.0.tgz\",\n      \"integrity\": \"sha512-ZMERYes6pDydyuGidse7OsHxtbI7WVeUEozgR/g7rd0xUimYNlvZRE/K2MgZTjWy725IfelLeVcEM97mmtRGXw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"binary-extensions\": \"^2.0.0\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/is-core-module\": {\n      \"version\": \"2.16.1\",\n      \"resolved\": \"https://registry.npmjs.org/is-core-module/-/is-core-module-2.16.1.tgz\",\n      \"integrity\": \"sha512-UfoeMA6fIJ8wTYFEUjelnaGI67v6+N7qXJEvQuIGa99l4xsCruSYOVSQ0uPANn4dAzm8lkYPaKLrrijLq7x23w==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"hasown\": \"^2.0.2\"\n      },\n      \"engines\": {\n        \"node\": \">= 0.4\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/ljharb\"\n      }\n    },\n    \"node_modules/is-extglob\": {\n      \"version\": \"2.1.1\",\n      \"resolved\": \"https://registry.npmjs.org/is-extglob/-/is-extglob-2.1.1.tgz\",\n      \"integrity\": \"sha512-SbKbANkN603Vi4jEZv49LeVJMn4yGwsbzZworEoyEiutsN3nJYdbO36zfhGJ6QEDpOZIFkDtnq5JRxmvl3jsoQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=0.10.0\"\n      }\n    },\n    \"node_modules/is-fullwidth-code-point\": {\n      \"version\": \"3.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-3.0.0.tgz\",\n      \"integrity\": \"sha512-zymm5+u+sCsSWyD9qNaejV3DFvhCKclKdizYaJUuHA83RLjb7nSuGnddCHGv0hk+KY7BMAlsWeK4Ueg6EV6XQg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/is-glob\": {\n      \"version\": \"4.0.3\",\n      \"resolved\": \"https://registry.npmjs.org/is-glob/-/is-glob-4.0.3.tgz\",\n      \"integrity\": \"sha512-xelSayHH36ZgE7ZWhli7pW34hNbNl8Ojv5KVmkJD4hBdD3th8Tfk9vYasLM+mXWOZhFkgZfxhLSnrwRr4elSSg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"is-extglob\": \"^2.1.1\"\n      },\n      \"engines\": {\n        \"node\": \">=0.10.0\"\n      }\n    },\n    \"node_modules/is-number\": {\n      \"version\": \"7.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/is-number/-/is-number-7.0.0.tgz\",\n      \"integrity\": \"sha512-41Cifkg6e8TylSpdtTpeLVMqvSBEVzTttHvERD741+pnZ8ANv0004MRL43QKPDlK9cGvNp6NZWZUBlbGXYxxng==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=0.12.0\"\n      }\n    },\n    \"node_modules/isexe\": {\n      \"version\": \"2.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz\",\n      \"integrity\": \"sha512-RHxMLp9lnKHGHRng9QFhRCMbYAcVpn69smSGcq3f36xjgVVWThj4qqLbTLlq7Ssj8B+fIQ1EuCEGI2lKsyQeIw==\",\n      \"dev\": true,\n      \"license\": \"ISC\"\n    },\n    \"node_modules/jackspeak\": {\n      \"version\": \"3.4.3\",\n      \"resolved\": \"https://registry.npmjs.org/jackspeak/-/jackspeak-3.4.3.tgz\",\n      \"integrity\": \"sha512-OGlZQpz2yfahA/Rd1Y8Cd9SIEsqvXkLVoSw/cgwhnhFMDbsQFeZYoJJ7bIZBS9BcamUW96asq/npPWugM+RQBw==\",\n      \"dev\": true,\n      \"license\": \"BlueOak-1.0.0\",\n      \"dependencies\": {\n        \"@isaacs/cliui\": \"^8.0.2\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/isaacs\"\n      },\n      \"optionalDependencies\": {\n        \"@pkgjs/parseargs\": \"^0.11.0\"\n      }\n    },\n    \"node_modules/js-tokens\": {\n      \"version\": \"4.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz\",\n      \"integrity\": \"sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==\",\n      \"license\": \"MIT\"\n    },\n    \"node_modules/lilconfig\": {\n      \"version\": \"3.1.3\",\n      \"resolved\": \"https://registry.npmjs.org/lilconfig/-/lilconfig-3.1.3.tgz\",\n      \"integrity\": \"sha512-/vlFKAoH5Cgt3Ie+JLhRbwOsCQePABiU3tJ1egGvyQ+33R/vcwM2Zl2QR/LzjsBeItPt3oSVXapn+m4nQDvpzw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=14\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/antonk52\"\n      }\n    },\n    \"node_modules/lines-and-columns\": {\n      \"version\": \"1.2.4\",\n      \"resolved\": \"https://registry.npmjs.org/lines-and-columns/-/lines-and-columns-1.2.4.tgz\",\n      \"integrity\": \"sha512-7ylylesZQ/PV29jhEDl3Ufjo6ZX7gCqJr5F7PKrqc93v7fzSymt1BpwEU8nAUXs8qzzvqhbjhK5QZg6Mt/HkBg==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/loose-envify\": {\n      \"version\": \"1.4.0\",\n      \"resolved\": \"https://registry.npmjs.org/loose-envify/-/loose-envify-1.4.0.tgz\",\n      \"integrity\": \"sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==\",\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"js-tokens\": \"^3.0.0 || ^4.0.0\"\n      },\n      \"bin\": {\n        \"loose-envify\": \"cli.js\"\n      }\n    },\n    \"node_modules/lru-cache\": {\n      \"version\": \"10.4.3\",\n      \"resolved\": \"https://registry.npmjs.org/lru-cache/-/lru-cache-10.4.3.tgz\",\n      \"integrity\": \"sha512-JNAzZcXrCt42VGLuYz0zfAzDfAvJWW6AfYlDBQyDV5DClI2m5sAmK+OIO7s59XfsRsWHp02jAJrRadPRGTt6SQ==\",\n      \"dev\": true,\n      \"license\": \"ISC\"\n    },\n    \"node_modules/merge2\": {\n      \"version\": \"1.4.1\",\n      \"resolved\": \"https://registry.npmjs.org/merge2/-/merge2-1.4.1.tgz\",\n      \"integrity\": \"sha512-8q7VEgMJW4J8tcfVPy8g09NcQwZdbwFEqhe/WZkoIzjn/3TGDwtOCYtXGxA3O8tPzpczCCDgv+P2P5y00ZJOOg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">= 8\"\n      }\n    },\n    \"node_modules/micromatch\": {\n      \"version\": \"4.0.8\",\n      \"resolved\": \"https://registry.npmjs.org/micromatch/-/micromatch-4.0.8.tgz\",\n      \"integrity\": \"sha512-PXwfBhYu0hBCPw8Dn0E+WDYb7af3dSLVWKi3HGv84IdF4TyFoC0ysxFd0Goxw7nSv4T/PzEJQxsYsEiFCKo2BA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"braces\": \"^3.0.3\",\n        \"picomatch\": \"^2.3.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8.6\"\n      }\n    },\n    \"node_modules/minimatch\": {\n      \"version\": \"9.0.5\",\n      \"resolved\": \"https://registry.npmjs.org/minimatch/-/minimatch-9.0.5.tgz\",\n      \"integrity\": \"sha512-G6T0ZX48xgozx7587koeX9Ys2NYy6Gmv//P89sEte9V9whIapMNF4idKxnW2QtCcLiTWlb/wfCabAtAFWhhBow==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"brace-expansion\": \"^2.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=16 || 14 >=14.17\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/isaacs\"\n      }\n    },\n    \"node_modules/minipass\": {\n      \"version\": \"7.1.2\",\n      \"resolved\": \"https://registry.npmjs.org/minipass/-/minipass-7.1.2.tgz\",\n      \"integrity\": \"sha512-qOOzS1cBTWYF4BH8fVePDBOO9iptMnGUEZwNc/cMWnTV2nVLZ7VoNWEPHkYczZA0pdoA7dl6e7FL659nX9S2aw==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"engines\": {\n        \"node\": \">=16 || 14 >=14.17\"\n      }\n    },\n    \"node_modules/mz\": {\n      \"version\": \"2.7.0\",\n      \"resolved\": \"https://registry.npmjs.org/mz/-/mz-2.7.0.tgz\",\n      \"integrity\": \"sha512-z81GNO7nnYMEhrGh9LeymoE4+Yr0Wn5McHIZMK5cfQCl+NDX08sCZgUc9/6MHni9IWuFLm1Z3HTCXu2z9fN62Q==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"any-promise\": \"^1.0.0\",\n        \"object-assign\": \"^4.0.1\",\n        \"thenify-all\": \"^1.0.0\"\n      }\n    },\n    \"node_modules/nanoid\": {\n      \"version\": \"3.3.8\",\n      \"resolved\": \"https://registry.npmjs.org/nanoid/-/nanoid-3.3.8.tgz\",\n      \"integrity\": \"sha512-WNLf5Sd8oZxOm+TzppcYk8gVOgP+l58xNy58D0nbUnOxOWRWvlcCV4kUF7ltmI6PsrLl/BgKEyS4mqsGChFN0w==\",\n      \"funding\": [\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/ai\"\n        }\n      ],\n      \"license\": \"MIT\",\n      \"bin\": {\n        \"nanoid\": \"bin/nanoid.cjs\"\n      },\n      \"engines\": {\n        \"node\": \"^10 || ^12 || ^13.7 || ^14 || >=15.0.1\"\n      }\n    },\n    \"node_modules/next\": {\n      \"version\": \"14.2.24\",\n      \"resolved\": \"https://registry.npmjs.org/next/-/next-14.2.24.tgz\",\n      \"integrity\": \"sha512-En8VEexSJ0Py2FfVnRRh8gtERwDRaJGNvsvad47ShkC2Yi8AXQPXEA2vKoDJlGFSj5WE5SyF21zNi4M5gyi+SQ==\",\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"@next/env\": \"14.2.24\",\n        \"@swc/helpers\": \"0.5.5\",\n        \"busboy\": \"1.6.0\",\n        \"caniuse-lite\": \"^1.0.30001579\",\n        \"graceful-fs\": \"^4.2.11\",\n        \"postcss\": \"8.4.31\",\n        \"styled-jsx\": \"5.1.1\"\n      },\n      \"bin\": {\n        \"next\": \"dist/bin/next\"\n      },\n      \"engines\": {\n        \"node\": \">=18.17.0\"\n      },\n      \"optionalDependencies\": {\n        \"@next/swc-darwin-arm64\": \"14.2.24\",\n        \"@next/swc-darwin-x64\": \"14.2.24\",\n        \"@next/swc-linux-arm64-gnu\": \"14.2.24\",\n        \"@next/swc-linux-arm64-musl\": \"14.2.24\",\n        \"@next/swc-linux-x64-gnu\": \"14.2.24\",\n        \"@next/swc-linux-x64-musl\": \"14.2.24\",\n        \"@next/swc-win32-arm64-msvc\": \"14.2.24\",\n        \"@next/swc-win32-ia32-msvc\": \"14.2.24\",\n        \"@next/swc-win32-x64-msvc\": \"14.2.24\"\n      },\n      \"peerDependencies\": {\n        \"@opentelemetry/api\": \"^1.1.0\",\n        \"@playwright/test\": \"^1.41.2\",\n        \"react\": \"^18.2.0\",\n        \"react-dom\": \"^18.2.0\",\n        \"sass\": \"^1.3.0\"\n      },\n      \"peerDependenciesMeta\": {\n        \"@opentelemetry/api\": {\n          \"optional\": true\n        },\n        \"@playwright/test\": {\n          \"optional\": true\n        },\n        \"sass\": {\n          \"optional\": true\n        }\n      }\n    },\n    \"node_modules/next/node_modules/postcss\": {\n      \"version\": \"8.4.31\",\n      \"resolved\": \"https://registry.npmjs.org/postcss/-/postcss-8.4.31.tgz\",\n      \"integrity\": \"sha512-PS08Iboia9mts/2ygV3eLpY5ghnUcfLV/EXTOW1E2qYxJKGGBUtNjN76FYHnMs36RmARn41bC0AZmn+rR0OVpQ==\",\n      \"funding\": [\n        {\n          \"type\": \"opencollective\",\n          \"url\": \"https://opencollective.com/postcss/\"\n        },\n        {\n          \"type\": \"tidelift\",\n          \"url\": \"https://tidelift.com/funding/github/npm/postcss\"\n        },\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/ai\"\n        }\n      ],\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"nanoid\": \"^3.3.6\",\n        \"picocolors\": \"^1.0.0\",\n        \"source-map-js\": \"^1.0.2\"\n      },\n      \"engines\": {\n        \"node\": \"^10 || ^12 || >=14\"\n      }\n    },\n    \"node_modules/node-releases\": {\n      \"version\": \"2.0.19\",\n      \"resolved\": \"https://registry.npmjs.org/node-releases/-/node-releases-2.0.19.tgz\",\n      \"integrity\": \"sha512-xxOWJsBKtzAq7DY0J+DTzuz58K8e7sJbdgwkbMWQe8UYB6ekmsQ45q0M/tJDsGaZmbC+l7n57UV8Hl5tHxO9uw==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/normalize-path\": {\n      \"version\": \"3.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/normalize-path/-/normalize-path-3.0.0.tgz\",\n      \"integrity\": \"sha512-6eZs5Ls3WtCisHWp9S2GUy8dqkpGi4BVSz3GaqiE6ezub0512ESztXUwUB6C6IKbQkY2Pnb/mD4WYojCRwcwLA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=0.10.0\"\n      }\n    },\n    \"node_modules/normalize-range\": {\n      \"version\": \"0.1.2\",\n      \"resolved\": \"https://registry.npmjs.org/normalize-range/-/normalize-range-0.1.2.tgz\",\n      \"integrity\": \"sha512-bdok/XvKII3nUpklnV6P2hxtMNrCboOjAcyBuQnWEhO665FwrSNRxU+AqpsyvO6LgGYPspN+lu5CLtw4jPRKNA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=0.10.0\"\n      }\n    },\n    \"node_modules/object-assign\": {\n      \"version\": \"4.1.1\",\n      \"resolved\": \"https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz\",\n      \"integrity\": \"sha512-rJgTQnkUnH1sFw8yT6VSU3zD3sWmu6sZhIseY8VX+GRu3P6F7Fu+JNDoXfklElbLJSnc3FUQHVe4cU5hj+BcUg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=0.10.0\"\n      }\n    },\n    \"node_modules/object-hash\": {\n      \"version\": \"3.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/object-hash/-/object-hash-3.0.0.tgz\",\n      \"integrity\": \"sha512-RSn9F68PjH9HqtltsSnqYC1XXoWe9Bju5+213R98cNGttag9q9yAOTzdbsqvIa7aNm5WffBZFpWYr2aWrklWAw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">= 6\"\n      }\n    },\n    \"node_modules/package-json-from-dist\": {\n      \"version\": \"1.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/package-json-from-dist/-/package-json-from-dist-1.0.1.tgz\",\n      \"integrity\": \"sha512-UEZIS3/by4OC8vL3P2dTXRETpebLI2NiI5vIrjaD/5UtrkFX/tNbwjTSRAGC/+7CAo2pIcBaRgWmcBBHcsaCIw==\",\n      \"dev\": true,\n      \"license\": \"BlueOak-1.0.0\"\n    },\n    \"node_modules/path-key\": {\n      \"version\": \"3.1.1\",\n      \"resolved\": \"https://registry.npmjs.org/path-key/-/path-key-3.1.1.tgz\",\n      \"integrity\": \"sha512-ojmeN0qd+y0jszEtoY48r0Peq5dwMEkIlCOu6Q5f41lfkswXuKtYrhgoTpLnyIcHm24Uhqx+5Tqm2InSwLhE6Q==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/path-parse\": {\n      \"version\": \"1.0.7\",\n      \"resolved\": \"https://registry.npmjs.org/path-parse/-/path-parse-1.0.7.tgz\",\n      \"integrity\": \"sha512-LDJzPVEEEPR+y48z93A0Ed0yXb8pAByGWo/k5YYdYgpY2/2EsOsksJrq7lOHxryrVOn1ejG6oAp8ahvOIQD8sw==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/path-scurry\": {\n      \"version\": \"1.11.1\",\n      \"resolved\": \"https://registry.npmjs.org/path-scurry/-/path-scurry-1.11.1.tgz\",\n      \"integrity\": \"sha512-Xa4Nw17FS9ApQFJ9umLiJS4orGjm7ZzwUrwamcGQuHSzDyth9boKDaycYdDcZDuqYATXw4HFXgaqWTctW/v1HA==\",\n      \"dev\": true,\n      \"license\": \"BlueOak-1.0.0\",\n      \"dependencies\": {\n        \"lru-cache\": \"^10.2.0\",\n        \"minipass\": \"^5.0.0 || ^6.0.2 || ^7.0.0\"\n      },\n      \"engines\": {\n        \"node\": \">=16 || 14 >=14.18\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/isaacs\"\n      }\n    },\n    \"node_modules/picocolors\": {\n      \"version\": \"1.1.1\",\n      \"resolved\": \"https://registry.npmjs.org/picocolors/-/picocolors-1.1.1.tgz\",\n      \"integrity\": \"sha512-xceH2snhtb5M9liqDsmEw56le376mTZkEX/jEb/RxNFyegNul7eNslCXP9FDj/Lcu0X8KEyMceP2ntpaHrDEVA==\",\n      \"license\": \"ISC\"\n    },\n    \"node_modules/picomatch\": {\n      \"version\": \"2.3.1\",\n      \"resolved\": \"https://registry.npmjs.org/picomatch/-/picomatch-2.3.1.tgz\",\n      \"integrity\": \"sha512-JU3teHTNjmE2VCGFzuY8EXzCDVwEqB2a8fsIvwaStHhAWJEeVd1o1QD80CU6+ZdEXXSLbSsuLwJjkCBWqRQUVA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=8.6\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/jonschlinkert\"\n      }\n    },\n    \"node_modules/pify\": {\n      \"version\": \"2.3.0\",\n      \"resolved\": \"https://registry.npmjs.org/pify/-/pify-2.3.0.tgz\",\n      \"integrity\": \"sha512-udgsAY+fTnvv7kI7aaxbqwWNb0AHiB0qBO89PZKPkoTmGOgdbrHDKD+0B2X4uTfJ/FT1R09r9gTsjUjNJotuog==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=0.10.0\"\n      }\n    },\n    \"node_modules/pirates\": {\n      \"version\": \"4.0.6\",\n      \"resolved\": \"https://registry.npmjs.org/pirates/-/pirates-4.0.6.tgz\",\n      \"integrity\": \"sha512-saLsH7WeYYPiD25LDuLRRY/i+6HaPYr6G1OUlN39otzkSTxKnubR9RTxS3/Kk50s1g2JTgFwWQDQyplC5/SHZg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">= 6\"\n      }\n    },\n    \"node_modules/postcss\": {\n      \"version\": \"8.5.3\",\n      \"resolved\": \"https://registry.npmjs.org/postcss/-/postcss-8.5.3.tgz\",\n      \"integrity\": \"sha512-dle9A3yYxlBSrt8Fu+IpjGT8SY8hN0mlaA6GY8t0P5PjIOZemULz/E2Bnm/2dcUOena75OTNkHI76uZBNUUq3A==\",\n      \"dev\": true,\n      \"funding\": [\n        {\n          \"type\": \"opencollective\",\n          \"url\": \"https://opencollective.com/postcss/\"\n        },\n        {\n          \"type\": \"tidelift\",\n          \"url\": \"https://tidelift.com/funding/github/npm/postcss\"\n        },\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/ai\"\n        }\n      ],\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"nanoid\": \"^3.3.8\",\n        \"picocolors\": \"^1.1.1\",\n        \"source-map-js\": \"^1.2.1\"\n      },\n      \"engines\": {\n        \"node\": \"^10 || ^12 || >=14\"\n      }\n    },\n    \"node_modules/postcss-import\": {\n      \"version\": \"15.1.0\",\n      \"resolved\": \"https://registry.npmjs.org/postcss-import/-/postcss-import-15.1.0.tgz\",\n      \"integrity\": \"sha512-hpr+J05B2FVYUAXHeK1YyI267J/dDDhMU6B6civm8hSY1jYJnBXxzKDKDswzJmtLHryrjhnDjqqp/49t8FALew==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"postcss-value-parser\": \"^4.0.0\",\n        \"read-cache\": \"^1.0.0\",\n        \"resolve\": \"^1.1.7\"\n      },\n      \"engines\": {\n        \"node\": \">=14.0.0\"\n      },\n      \"peerDependencies\": {\n        \"postcss\": \"^8.0.0\"\n      }\n    },\n    \"node_modules/postcss-js\": {\n      \"version\": \"4.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/postcss-js/-/postcss-js-4.0.1.tgz\",\n      \"integrity\": \"sha512-dDLF8pEO191hJMtlHFPRa8xsizHaM82MLfNkUHdUtVEV3tgTp5oj+8qbEqYM57SLfc74KSbw//4SeJma2LRVIw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"camelcase-css\": \"^2.0.1\"\n      },\n      \"engines\": {\n        \"node\": \"^12 || ^14 || >= 16\"\n      },\n      \"funding\": {\n        \"type\": \"opencollective\",\n        \"url\": \"https://opencollective.com/postcss/\"\n      },\n      \"peerDependencies\": {\n        \"postcss\": \"^8.4.21\"\n      }\n    },\n    \"node_modules/postcss-load-config\": {\n      \"version\": \"4.0.2\",\n      \"resolved\": \"https://registry.npmjs.org/postcss-load-config/-/postcss-load-config-4.0.2.tgz\",\n      \"integrity\": \"sha512-bSVhyJGL00wMVoPUzAVAnbEoWyqRxkjv64tUl427SKnPrENtq6hJwUojroMz2VB+Q1edmi4IfrAPpami5VVgMQ==\",\n      \"dev\": true,\n      \"funding\": [\n        {\n          \"type\": \"opencollective\",\n          \"url\": \"https://opencollective.com/postcss/\"\n        },\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/ai\"\n        }\n      ],\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"lilconfig\": \"^3.0.0\",\n        \"yaml\": \"^2.3.4\"\n      },\n      \"engines\": {\n        \"node\": \">= 14\"\n      },\n      \"peerDependencies\": {\n        \"postcss\": \">=8.0.9\",\n        \"ts-node\": \">=9.0.0\"\n      },\n      \"peerDependenciesMeta\": {\n        \"postcss\": {\n          \"optional\": true\n        },\n        \"ts-node\": {\n          \"optional\": true\n        }\n      }\n    },\n    \"node_modules/postcss-nested\": {\n      \"version\": \"6.2.0\",\n      \"resolved\": \"https://registry.npmjs.org/postcss-nested/-/postcss-nested-6.2.0.tgz\",\n      \"integrity\": \"sha512-HQbt28KulC5AJzG+cZtj9kvKB93CFCdLvog1WFLf1D+xmMvPGlBstkpTEZfK5+AN9hfJocyBFCNiqyS48bpgzQ==\",\n      \"dev\": true,\n      \"funding\": [\n        {\n          \"type\": \"opencollective\",\n          \"url\": \"https://opencollective.com/postcss/\"\n        },\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/ai\"\n        }\n      ],\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"postcss-selector-parser\": \"^6.1.1\"\n      },\n      \"engines\": {\n        \"node\": \">=12.0\"\n      },\n      \"peerDependencies\": {\n        \"postcss\": \"^8.2.14\"\n      }\n    },\n    \"node_modules/postcss-selector-parser\": {\n      \"version\": \"6.1.2\",\n      \"resolved\": \"https://registry.npmjs.org/postcss-selector-parser/-/postcss-selector-parser-6.1.2.tgz\",\n      \"integrity\": \"sha512-Q8qQfPiZ+THO/3ZrOrO0cJJKfpYCagtMUkXbnEfmgUjwXg6z/WBeOyS9APBBPCTSiDV+s4SwQGu8yFsiMRIudg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"cssesc\": \"^3.0.0\",\n        \"util-deprecate\": \"^1.0.2\"\n      },\n      \"engines\": {\n        \"node\": \">=4\"\n      }\n    },\n    \"node_modules/postcss-value-parser\": {\n      \"version\": \"4.2.0\",\n      \"resolved\": \"https://registry.npmjs.org/postcss-value-parser/-/postcss-value-parser-4.2.0.tgz\",\n      \"integrity\": \"sha512-1NNCs6uurfkVbeXG4S8JFT9t19m45ICnif8zWLd5oPSZ50QnwMfK+H3jv408d4jw/7Bttv5axS5IiHoLaVNHeQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/queue-microtask\": {\n      \"version\": \"1.2.3\",\n      \"resolved\": \"https://registry.npmjs.org/queue-microtask/-/queue-microtask-1.2.3.tgz\",\n      \"integrity\": \"sha512-NuaNSa6flKT5JaSYQzJok04JzTL1CA6aGhv5rfLW3PgqA+M2ChpZQnAC8h8i4ZFkBS8X5RqkDBHA7r4hej3K9A==\",\n      \"dev\": true,\n      \"funding\": [\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/feross\"\n        },\n        {\n          \"type\": \"patreon\",\n          \"url\": \"https://www.patreon.com/feross\"\n        },\n        {\n          \"type\": \"consulting\",\n          \"url\": \"https://feross.org/support\"\n        }\n      ],\n      \"license\": \"MIT\"\n    },\n    \"node_modules/react\": {\n      \"version\": \"18.3.1\",\n      \"resolved\": \"https://registry.npmjs.org/react/-/react-18.3.1.tgz\",\n      \"integrity\": \"sha512-wS+hAgJShR0KhEvPJArfuPVN1+Hz1t0Y6n5jLrGQbkb4urgPE/0Rve+1kMB1v/oWgHgm4WIcV+i7F2pTVj+2iQ==\",\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"loose-envify\": \"^1.1.0\"\n      },\n      \"engines\": {\n        \"node\": \">=0.10.0\"\n      }\n    },\n    \"node_modules/react-dom\": {\n      \"version\": \"18.3.1\",\n      \"resolved\": \"https://registry.npmjs.org/react-dom/-/react-dom-18.3.1.tgz\",\n      \"integrity\": \"sha512-5m4nQKp+rZRb09LNH59GM4BxTh9251/ylbKIbpe7TpGxfJ+9kv6BLkLBXIjjspbgbnIBNqlI23tRnTWT0snUIw==\",\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"loose-envify\": \"^1.1.0\",\n        \"scheduler\": \"^0.23.2\"\n      },\n      \"peerDependencies\": {\n        \"react\": \"^18.3.1\"\n      }\n    },\n    \"node_modules/read-cache\": {\n      \"version\": \"1.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/read-cache/-/read-cache-1.0.0.tgz\",\n      \"integrity\": \"sha512-Owdv/Ft7IjOgm/i0xvNDZ1LrRANRfew4b2prF3OWMQLxLfu3bS8FVhCsrSCMK4lR56Y9ya+AThoTpDCTxCmpRA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"pify\": \"^2.3.0\"\n      }\n    },\n    \"node_modules/readdirp\": {\n      \"version\": \"3.6.0\",\n      \"resolved\": \"https://registry.npmjs.org/readdirp/-/readdirp-3.6.0.tgz\",\n      \"integrity\": \"sha512-hOS089on8RduqdbhvQ5Z37A0ESjsqz6qnRcffsMU3495FuTdqSm+7bhJ29JvIOsBDEEnan5DPu9t3To9VRlMzA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"picomatch\": \"^2.2.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8.10.0\"\n      }\n    },\n    \"node_modules/resolve\": {\n      \"version\": \"1.22.10\",\n      \"resolved\": \"https://registry.npmjs.org/resolve/-/resolve-1.22.10.tgz\",\n      \"integrity\": \"sha512-NPRy+/ncIMeDlTAsuqwKIiferiawhefFJtkNSW0qZJEqMEb+qBt/77B/jGeeek+F0uOeN05CDa6HXbbIgtVX4w==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"is-core-module\": \"^2.16.0\",\n        \"path-parse\": \"^1.0.7\",\n        \"supports-preserve-symlinks-flag\": \"^1.0.0\"\n      },\n      \"bin\": {\n        \"resolve\": \"bin/resolve\"\n      },\n      \"engines\": {\n        \"node\": \">= 0.4\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/ljharb\"\n      }\n    },\n    \"node_modules/reusify\": {\n      \"version\": \"1.1.0\",\n      \"resolved\": \"https://registry.npmjs.org/reusify/-/reusify-1.1.0.tgz\",\n      \"integrity\": \"sha512-g6QUff04oZpHs0eG5p83rFLhHeV00ug/Yf9nZM6fLeUrPguBTkTQOdpAWWspMh55TZfVQDPaN3NQJfbVRAxdIw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"iojs\": \">=1.0.0\",\n        \"node\": \">=0.10.0\"\n      }\n    },\n    \"node_modules/run-parallel\": {\n      \"version\": \"1.2.0\",\n      \"resolved\": \"https://registry.npmjs.org/run-parallel/-/run-parallel-1.2.0.tgz\",\n      \"integrity\": \"sha512-5l4VyZR86LZ/lDxZTR6jqL8AFE2S0IFLMP26AbjsLVADxHdhB/c0GUsH+y39UfCi3dzz8OlQuPmnaJOMoDHQBA==\",\n      \"dev\": true,\n      \"funding\": [\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/feross\"\n        },\n        {\n          \"type\": \"patreon\",\n          \"url\": \"https://www.patreon.com/feross\"\n        },\n        {\n          \"type\": \"consulting\",\n          \"url\": \"https://feross.org/support\"\n        }\n      ],\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"queue-microtask\": \"^1.2.2\"\n      }\n    },\n    \"node_modules/scheduler\": {\n      \"version\": \"0.23.2\",\n      \"resolved\": \"https://registry.npmjs.org/scheduler/-/scheduler-0.23.2.tgz\",\n      \"integrity\": \"sha512-UOShsPwz7NrMUqhR6t0hWjFduvOzbtv7toDH1/hIrfRNIDBnnBWd0CwJTGvTpngVlmwGCdP9/Zl/tVrDqcuYzQ==\",\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"loose-envify\": \"^1.1.0\"\n      }\n    },\n    \"node_modules/shebang-command\": {\n      \"version\": \"2.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/shebang-command/-/shebang-command-2.0.0.tgz\",\n      \"integrity\": \"sha512-kHxr2zZpYtdmrN1qDjrrX/Z1rR1kG8Dx+gkpK1G4eXmvXswmcE1hTWBWYUzlraYw1/yZp6YuDY77YtvbN0dmDA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"shebang-regex\": \"^3.0.0\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/shebang-regex\": {\n      \"version\": \"3.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/shebang-regex/-/shebang-regex-3.0.0.tgz\",\n      \"integrity\": \"sha512-7++dFhtcx3353uBaq8DDR4NuxBetBzC7ZQOhmTQInHEd6bSrXdiEyzCvG07Z44UYdLShWUyXt5M/yhz8ekcb1A==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/signal-exit\": {\n      \"version\": \"4.1.0\",\n      \"resolved\": \"https://registry.npmjs.org/signal-exit/-/signal-exit-4.1.0.tgz\",\n      \"integrity\": \"sha512-bzyZ1e88w9O1iNJbKnOlvYTrWPDl46O1bG0D3XInv+9tkPrxrN8jUUTiFlDkkmKWgn1M6CfIA13SuGqOa9Korw==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"engines\": {\n        \"node\": \">=14\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/isaacs\"\n      }\n    },\n    \"node_modules/source-map-js\": {\n      \"version\": \"1.2.1\",\n      \"resolved\": \"https://registry.npmjs.org/source-map-js/-/source-map-js-1.2.1.tgz\",\n      \"integrity\": \"sha512-UXWMKhLOwVKb728IUtQPXxfYU+usdybtUrK/8uGE8CQMvrhOpwvzDBwj0QhSL7MQc7vIsISBG8VQ8+IDQxpfQA==\",\n      \"license\": \"BSD-3-Clause\",\n      \"engines\": {\n        \"node\": \">=0.10.0\"\n      }\n    },\n    \"node_modules/streamsearch\": {\n      \"version\": \"1.1.0\",\n      \"resolved\": \"https://registry.npmjs.org/streamsearch/-/streamsearch-1.1.0.tgz\",\n      \"integrity\": \"sha512-Mcc5wHehp9aXz1ax6bZUyY5afg9u2rv5cqQI3mRrYkGC8rW2hM02jWuwjtL++LS5qinSyhj2QfLyNsuc+VsExg==\",\n      \"engines\": {\n        \"node\": \">=10.0.0\"\n      }\n    },\n    \"node_modules/string-width\": {\n      \"version\": \"5.1.2\",\n      \"resolved\": \"https://registry.npmjs.org/string-width/-/string-width-5.1.2.tgz\",\n      \"integrity\": \"sha512-HnLOCR3vjcY8beoNLtcjZ5/nxn2afmME6lhrDrebokqMap+XbeW8n9TXpPDOqdGK5qcI3oT0GKTW6wC7EMiVqA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"eastasianwidth\": \"^0.2.0\",\n        \"emoji-regex\": \"^9.2.2\",\n        \"strip-ansi\": \"^7.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=12\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/sindresorhus\"\n      }\n    },\n    \"node_modules/string-width-cjs\": {\n      \"name\": \"string-width\",\n      \"version\": \"4.2.3\",\n      \"resolved\": \"https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz\",\n      \"integrity\": \"sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"emoji-regex\": \"^8.0.0\",\n        \"is-fullwidth-code-point\": \"^3.0.0\",\n        \"strip-ansi\": \"^6.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/string-width-cjs/node_modules/ansi-regex\": {\n      \"version\": \"5.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz\",\n      \"integrity\": \"sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/string-width-cjs/node_modules/emoji-regex\": {\n      \"version\": \"8.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz\",\n      \"integrity\": \"sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/string-width-cjs/node_modules/strip-ansi\": {\n      \"version\": \"6.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz\",\n      \"integrity\": \"sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"ansi-regex\": \"^5.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/strip-ansi\": {\n      \"version\": \"7.1.0\",\n      \"resolved\": \"https://registry.npmjs.org/strip-ansi/-/strip-ansi-7.1.0.tgz\",\n      \"integrity\": \"sha512-iq6eVVI64nQQTRYq2KtEg2d2uU7LElhTJwsH4YzIHZshxlgZms/wIc4VoDQTlG/IvVIrBKG06CrZnp0qv7hkcQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"ansi-regex\": \"^6.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=12\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/chalk/strip-ansi?sponsor=1\"\n      }\n    },\n    \"node_modules/strip-ansi-cjs\": {\n      \"name\": \"strip-ansi\",\n      \"version\": \"6.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz\",\n      \"integrity\": \"sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"ansi-regex\": \"^5.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/strip-ansi-cjs/node_modules/ansi-regex\": {\n      \"version\": \"5.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz\",\n      \"integrity\": \"sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/styled-jsx\": {\n      \"version\": \"5.1.1\",\n      \"resolved\": \"https://registry.npmjs.org/styled-jsx/-/styled-jsx-5.1.1.tgz\",\n      \"integrity\": \"sha512-pW7uC1l4mBZ8ugbiZrcIsiIvVx1UmTfw7UkC3Um2tmfUq9Bhk8IiyEIPl6F8agHgjzku6j0xQEZbfA5uSgSaCw==\",\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"client-only\": \"0.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">= 12.0.0\"\n      },\n      \"peerDependencies\": {\n        \"react\": \">= 16.8.0 || 17.x.x || ^18.0.0-0\"\n      },\n      \"peerDependenciesMeta\": {\n        \"@babel/core\": {\n          \"optional\": true\n        },\n        \"babel-plugin-macros\": {\n          \"optional\": true\n        }\n      }\n    },\n    \"node_modules/sucrase\": {\n      \"version\": \"3.35.0\",\n      \"resolved\": \"https://registry.npmjs.org/sucrase/-/sucrase-3.35.0.tgz\",\n      \"integrity\": \"sha512-8EbVDiu9iN/nESwxeSxDKe0dunta1GOlHufmSSXxMD2z2/tMZpDMpvXQGsc+ajGo8y2uYUmixaSRUc/QPoQ0GA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"@jridgewell/gen-mapping\": \"^0.3.2\",\n        \"commander\": \"^4.0.0\",\n        \"glob\": \"^10.3.10\",\n        \"lines-and-columns\": \"^1.1.6\",\n        \"mz\": \"^2.7.0\",\n        \"pirates\": \"^4.0.1\",\n        \"ts-interface-checker\": \"^0.1.9\"\n      },\n      \"bin\": {\n        \"sucrase\": \"bin/sucrase\",\n        \"sucrase-node\": \"bin/sucrase-node\"\n      },\n      \"engines\": {\n        \"node\": \">=16 || 14 >=14.17\"\n      }\n    },\n    \"node_modules/supports-preserve-symlinks-flag\": {\n      \"version\": \"1.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/supports-preserve-symlinks-flag/-/supports-preserve-symlinks-flag-1.0.0.tgz\",\n      \"integrity\": \"sha512-ot0WnXS9fgdkgIcePe6RHNk1WA8+muPa6cSjeR3V8K27q9BB1rTE3R1p7Hv0z1ZyAc8s6Vvv8DIyWf681MAt0w==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">= 0.4\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/sponsors/ljharb\"\n      }\n    },\n    \"node_modules/tailwindcss\": {\n      \"version\": \"3.4.17\",\n      \"resolved\": \"https://registry.npmjs.org/tailwindcss/-/tailwindcss-3.4.17.tgz\",\n      \"integrity\": \"sha512-w33E2aCvSDP0tW9RZuNXadXlkHXqFzSkQew/aIa2i/Sj8fThxwovwlXHSPXTbAHwEIhBFXAedUhP2tueAKP8Og==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"@alloc/quick-lru\": \"^5.2.0\",\n        \"arg\": \"^5.0.2\",\n        \"chokidar\": \"^3.6.0\",\n        \"didyoumean\": \"^1.2.2\",\n        \"dlv\": \"^1.1.3\",\n        \"fast-glob\": \"^3.3.2\",\n        \"glob-parent\": \"^6.0.2\",\n        \"is-glob\": \"^4.0.3\",\n        \"jiti\": \"^1.21.6\",\n        \"lilconfig\": \"^3.1.3\",\n        \"micromatch\": \"^4.0.8\",\n        \"normalize-path\": \"^3.0.0\",\n        \"object-hash\": \"^3.0.0\",\n        \"picocolors\": \"^1.1.1\",\n        \"postcss\": \"^8.4.47\",\n        \"postcss-import\": \"^15.1.0\",\n        \"postcss-js\": \"^4.0.1\",\n        \"postcss-load-config\": \"^4.0.2\",\n        \"postcss-nested\": \"^6.2.0\",\n        \"postcss-selector-parser\": \"^6.1.2\",\n        \"resolve\": \"^1.22.8\",\n        \"sucrase\": \"^3.35.0\"\n      },\n      \"bin\": {\n        \"tailwind\": \"lib/cli.js\",\n        \"tailwindcss\": \"lib/cli.js\"\n      },\n      \"engines\": {\n        \"node\": \">=14.0.0\"\n      }\n    },\n    \"node_modules/tailwindcss/node_modules/jiti\": {\n      \"version\": \"1.21.7\",\n      \"resolved\": \"https://registry.npmjs.org/jiti/-/jiti-1.21.7.tgz\",\n      \"integrity\": \"sha512-/imKNG4EbWNrVjoNC/1H5/9GFy+tqjGBHCaSsN+P2RnPqjsLmv6UD3Ej+Kj8nBWaRAwyk7kK5ZUc+OEatnTR3A==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"bin\": {\n        \"jiti\": \"bin/jiti.js\"\n      }\n    },\n    \"node_modules/thenify\": {\n      \"version\": \"3.3.1\",\n      \"resolved\": \"https://registry.npmjs.org/thenify/-/thenify-3.3.1.tgz\",\n      \"integrity\": \"sha512-RVZSIV5IG10Hk3enotrhvz0T9em6cyHBLkH/YAZuKqd8hRkKhSfCGIcP2KUY0EPxndzANBmNllzWPwak+bheSw==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"any-promise\": \"^1.0.0\"\n      }\n    },\n    \"node_modules/thenify-all\": {\n      \"version\": \"1.6.0\",\n      \"resolved\": \"https://registry.npmjs.org/thenify-all/-/thenify-all-1.6.0.tgz\",\n      \"integrity\": \"sha512-RNxQH/qI8/t3thXJDwcstUO4zeqo64+Uy/+sNVRBx4Xn2OX+OZ9oP+iJnNFqplFra2ZUVeKCSa2oVWi3T4uVmA==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"thenify\": \">= 3.1.0 < 4\"\n      },\n      \"engines\": {\n        \"node\": \">=0.8\"\n      }\n    },\n    \"node_modules/to-regex-range\": {\n      \"version\": \"5.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/to-regex-range/-/to-regex-range-5.0.1.tgz\",\n      \"integrity\": \"sha512-65P7iz6X5yEr1cwcgvQxbbIw7Uk3gOy5dIdtZ4rDveLqhrdJP+Li/Hx6tyK0NEb+2GCyneCMJiGqrADCSNk8sQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"is-number\": \"^7.0.0\"\n      },\n      \"engines\": {\n        \"node\": \">=8.0\"\n      }\n    },\n    \"node_modules/ts-interface-checker\": {\n      \"version\": \"0.1.13\",\n      \"resolved\": \"https://registry.npmjs.org/ts-interface-checker/-/ts-interface-checker-0.1.13.tgz\",\n      \"integrity\": \"sha512-Y/arvbn+rrz3JCKl9C4kVNfTfSm2/mEp5FSz5EsZSANGPSlQrpRI5M4PKF+mJnE52jOO90PnPSc3Ur3bTQw0gA==\",\n      \"dev\": true,\n      \"license\": \"Apache-2.0\"\n    },\n    \"node_modules/tslib\": {\n      \"version\": \"2.8.1\",\n      \"resolved\": \"https://registry.npmjs.org/tslib/-/tslib-2.8.1.tgz\",\n      \"integrity\": \"sha512-oJFu94HQb+KVduSUQL7wnpmqnfmLsOA/nAh6b6EH0wCEoK0/mPeXU6c3wKDV83MkOuHPRHtSXKKU99IBazS/2w==\",\n      \"license\": \"0BSD\"\n    },\n    \"node_modules/typescript\": {\n      \"version\": \"5.8.2\",\n      \"resolved\": \"https://registry.npmjs.org/typescript/-/typescript-5.8.2.tgz\",\n      \"integrity\": \"sha512-aJn6wq13/afZp/jT9QZmwEjDqqvSGp1VT5GVg+f/t6/oVyrgXM6BY1h9BRh/O5p3PlUPAe+WuiEZOmb/49RqoQ==\",\n      \"dev\": true,\n      \"license\": \"Apache-2.0\",\n      \"bin\": {\n        \"tsc\": \"bin/tsc\",\n        \"tsserver\": \"bin/tsserver\"\n      },\n      \"engines\": {\n        \"node\": \">=14.17\"\n      }\n    },\n    \"node_modules/undici-types\": {\n      \"version\": \"6.20.0\",\n      \"resolved\": \"https://registry.npmjs.org/undici-types/-/undici-types-6.20.0.tgz\",\n      \"integrity\": \"sha512-Ny6QZ2Nju20vw1SRHe3d9jVu6gJ+4e3+MMpqu7pqE5HT6WsTSlce++GQmK5UXS8mzV8DSYHrQH+Xrf2jVcuKNg==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/update-browserslist-db\": {\n      \"version\": \"1.1.3\",\n      \"resolved\": \"https://registry.npmjs.org/update-browserslist-db/-/update-browserslist-db-1.1.3.tgz\",\n      \"integrity\": \"sha512-UxhIZQ+QInVdunkDAaiazvvT/+fXL5Osr0JZlJulepYu6Jd7qJtDZjlur0emRlT71EN3ScPoE7gvsuIKKNavKw==\",\n      \"dev\": true,\n      \"funding\": [\n        {\n          \"type\": \"opencollective\",\n          \"url\": \"https://opencollective.com/browserslist\"\n        },\n        {\n          \"type\": \"tidelift\",\n          \"url\": \"https://tidelift.com/funding/github/npm/browserslist\"\n        },\n        {\n          \"type\": \"github\",\n          \"url\": \"https://github.com/sponsors/ai\"\n        }\n      ],\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"escalade\": \"^3.2.0\",\n        \"picocolors\": \"^1.1.1\"\n      },\n      \"bin\": {\n        \"update-browserslist-db\": \"cli.js\"\n      },\n      \"peerDependencies\": {\n        \"browserslist\": \">= 4.21.0\"\n      }\n    },\n    \"node_modules/util-deprecate\": {\n      \"version\": \"1.0.2\",\n      \"resolved\": \"https://registry.npmjs.org/util-deprecate/-/util-deprecate-1.0.2.tgz\",\n      \"integrity\": \"sha512-EPD5q1uXyFxJpCrLnCc1nHnq3gOa6DZBocAIiI2TaSCA7VCJ1UJDMagCzIkXNsUYfD1daK//LTEQ8xiIbrHtcw==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/which\": {\n      \"version\": \"2.0.2\",\n      \"resolved\": \"https://registry.npmjs.org/which/-/which-2.0.2.tgz\",\n      \"integrity\": \"sha512-BLI3Tl1TW3Pvl70l3yq3Y64i+awpwXqsGBYWkkqMtnbXgrMD+yj7rhW0kuEDxzJaYXGjEW5ogapKNMEKNMjibA==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"dependencies\": {\n        \"isexe\": \"^2.0.0\"\n      },\n      \"bin\": {\n        \"node-which\": \"bin/node-which\"\n      },\n      \"engines\": {\n        \"node\": \">= 8\"\n      }\n    },\n    \"node_modules/wrap-ansi\": {\n      \"version\": \"8.1.0\",\n      \"resolved\": \"https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-8.1.0.tgz\",\n      \"integrity\": \"sha512-si7QWI6zUMq56bESFvagtmzMdGOtoxfR+Sez11Mobfc7tm+VkUckk9bW2UeffTGVUbOksxmSw0AA2gs8g71NCQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"ansi-styles\": \"^6.1.0\",\n        \"string-width\": \"^5.0.1\",\n        \"strip-ansi\": \"^7.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=12\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/chalk/wrap-ansi?sponsor=1\"\n      }\n    },\n    \"node_modules/wrap-ansi-cjs\": {\n      \"name\": \"wrap-ansi\",\n      \"version\": \"7.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-7.0.0.tgz\",\n      \"integrity\": \"sha512-YVGIj2kamLSTxw6NsZjoBxfSwsn0ycdesmc4p+Q21c5zPuZ1pl+NfxVdxPtdHvmNVOQ6XSYG4AUtyt/Fi7D16Q==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"ansi-styles\": \"^4.0.0\",\n        \"string-width\": \"^4.1.0\",\n        \"strip-ansi\": \"^6.0.0\"\n      },\n      \"engines\": {\n        \"node\": \">=10\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/chalk/wrap-ansi?sponsor=1\"\n      }\n    },\n    \"node_modules/wrap-ansi-cjs/node_modules/ansi-regex\": {\n      \"version\": \"5.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz\",\n      \"integrity\": \"sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/wrap-ansi-cjs/node_modules/ansi-styles\": {\n      \"version\": \"4.3.0\",\n      \"resolved\": \"https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz\",\n      \"integrity\": \"sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"color-convert\": \"^2.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      },\n      \"funding\": {\n        \"url\": \"https://github.com/chalk/ansi-styles?sponsor=1\"\n      }\n    },\n    \"node_modules/wrap-ansi-cjs/node_modules/emoji-regex\": {\n      \"version\": \"8.0.0\",\n      \"resolved\": \"https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz\",\n      \"integrity\": \"sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==\",\n      \"dev\": true,\n      \"license\": \"MIT\"\n    },\n    \"node_modules/wrap-ansi-cjs/node_modules/string-width\": {\n      \"version\": \"4.2.3\",\n      \"resolved\": \"https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz\",\n      \"integrity\": \"sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"emoji-regex\": \"^8.0.0\",\n        \"is-fullwidth-code-point\": \"^3.0.0\",\n        \"strip-ansi\": \"^6.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/wrap-ansi-cjs/node_modules/strip-ansi\": {\n      \"version\": \"6.0.1\",\n      \"resolved\": \"https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz\",\n      \"integrity\": \"sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==\",\n      \"dev\": true,\n      \"license\": \"MIT\",\n      \"dependencies\": {\n        \"ansi-regex\": \"^5.0.1\"\n      },\n      \"engines\": {\n        \"node\": \">=8\"\n      }\n    },\n    \"node_modules/yaml\": {\n      \"version\": \"2.7.0\",\n      \"resolved\": \"https://registry.npmjs.org/yaml/-/yaml-2.7.0.tgz\",\n      \"integrity\": \"sha512-+hSoy/QHluxmC9kCIJyL/uyFmLmc+e5CFR5Wa+bpIhIj85LVb9ZH2nVnqrHoSvKogwODv0ClqZkmiSSaIH5LTA==\",\n      \"dev\": true,\n      \"license\": \"ISC\",\n      \"bin\": {\n        \"yaml\": \"bin.mjs\"\n      },\n      \"engines\": {\n        \"node\": \">= 14\"\n      }\n    }\n  }\n}\n\n\n\n================================================\nFile: example_apps/ai-chat/package.json\n================================================\n{\n  \"name\": \"ai-chat\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  },\n  \"dependencies\": {\n    \"next\": \"^14.0.0\",\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"22.13.9\",\n    \"@types/react\": \"19.0.10\",\n    \"autoprefixer\": \"^10.4.16\",\n    \"postcss\": \"^8.4.31\",\n    \"tailwindcss\": \"^3.3.5\",\n    \"typescript\": \"5.8.2\"\n  }\n}\n\n\n\n================================================\nFile: example_apps/ai-chat/postcss.config.js\n================================================\nmodule.exports = {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n};\n\n\n\n================================================\nFile: example_apps/ai-chat/tailwind.config.js\n================================================\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {\n  content: [\n    \"./src/pages/**/*.{js,ts,jsx,tsx,mdx}\",\n    \"./src/components/**/*.{js,ts,jsx,tsx,mdx}\",\n    \"./src/app/**/*.{js,ts,jsx,tsx,mdx}\",\n  ],\n  theme: {\n    extend: {\n      colors: {\n        primary: {\n          light: '#4da6ff',\n          DEFAULT: '#0084ff',\n          dark: '#0066cc',\n        },\n      },\n      animation: {\n        'bounce-slow': 'bounce 1.4s infinite',\n      },\n    },\n  },\n  plugins: [],\n};\n\n\n\n================================================\nFile: example_apps/ai-chat/tsconfig.json\n================================================\n{\n  \"compilerOptions\": {\n    \"lib\": [\n      \"dom\",\n      \"dom.iterable\",\n      \"esnext\"\n    ],\n    \"allowJs\": true,\n    \"skipLibCheck\": true,\n    \"strict\": false,\n    \"noEmit\": true,\n    \"incremental\": true,\n    \"module\": \"esnext\",\n    \"esModuleInterop\": true,\n    \"moduleResolution\": \"node\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"jsx\": \"preserve\",\n    \"plugins\": [\n      {\n        \"name\": \"next\"\n      }\n    ]\n  },\n  \"include\": [\n    \"next-env.d.ts\",\n    \".next/types/**/*.ts\",\n    \"**/*.ts\",\n    \"**/*.tsx\"\n  ],\n  \"exclude\": [\n    \"node_modules\"\n  ]\n}\n\n\n\n================================================\nFile: example_apps/ai-chat/.gitignore\n================================================\n# dependencies\n/node_modules\n/.pnp\n.pnp.js\n\n# testing\n/coverage\n\n# next.js\n/.next/\n/out/\n\n# production\n/build\n\n# misc\n.DS_Store\n*.pem\n\n# debug\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\n\n# local env files\n.env*.local\n\n# typescript\n*.tsbuildinfo\nnext-env.d.ts\n\n# IDE\n.idea/\n.vscode/\n\n\n\n================================================\nFile: example_apps/ai-chat/src/app/globals.css\n================================================\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\n@layer components {\n  .chat-bubble {\n    @apply px-4 py-2 rounded-lg max-w-[80%] break-words;\n  }\n  \n  .user-bubble {\n    @apply chat-bubble bg-primary text-white self-end rounded-br-none;\n  }\n  \n  .ai-bubble {\n    @apply chat-bubble bg-gray-100 text-gray-800 self-start rounded-bl-none;\n  }\n  \n  .typing-dot {\n    @apply w-2 h-2 bg-gray-500 rounded-full animate-bounce-slow;\n  }\n}\n\n/* Custom animation for typing indicator */\n@keyframes bounce {\n  0%, 80%, 100% { transform: translateY(0); }\n  40% { transform: translateY(-5px); }\n}\n\n\n\n================================================\nFile: example_apps/ai-chat/src/app/layout.tsx\n================================================\nimport './globals.css';\n\nexport default function RootLayout({\n  children,\n}: {\n  children: React.ReactNode\n}) {\n  return (\n    <html lang=\"en\">\n      <head>\n        <title>AI Frontend Assistant</title>\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n      </head>\n      <body className=\"bg-gray-50 min-h-screen\">{children}</body>\n    </html>\n  )\n}\n\n\n\n================================================\nFile: example_apps/ai-chat/src/app/page.module.css\n================================================\n.main {\n  min-height: 100vh;\n  padding: 2rem;\n  background-color: #f5f5f5;\n  display: flex;\n  justify-content: center;\n}\n\n.chatContainer {\n  width: 100%;\n  max-width: 800px;\n  background: white;\n  border-radius: 12px;\n  box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n  display: flex;\n  flex-direction: column;\n  overflow: hidden;\n}\n\n.messages {\n  flex-grow: 1;\n  padding: 1.5rem;\n  overflow-y: auto;\n  max-height: 80vh;\n  display: flex;\n  flex-direction: column;\n  gap: 1rem;\n}\n\n.message {\n  padding: 1rem;\n  border-radius: 8px;\n  max-width: 80%;\n  word-wrap: break-word;\n}\n\n.userMessage {\n  background-color: #007AFF;\n  color: white;\n  align-self: flex-end;\n}\n\n.aiMessage {\n  background-color: #f0f0f0;\n  color: #333;\n  align-self: flex-start;\n}\n\n.inputForm {\n  display: flex;\n  padding: 1rem;\n  gap: 0.5rem;\n  border-top: 1px solid #eee;\n}\n\n.input {\n  flex-grow: 1;\n  padding: 0.75rem;\n  border: 1px solid #ddd;\n  border-radius: 8px;\n  font-size: 1rem;\n}\n\n.button {\n  padding: 0.75rem 1.5rem;\n  background-color: #007AFF;\n  color: white;\n  border: none;\n  border-radius: 8px;\n  cursor: pointer;\n  font-size: 1rem;\n  transition: background-color 0.2s;\n}\n\n.button:hover {\n  background-color: #0056b3;\n}\n\n.typingIndicator {\n  display: flex;\n  gap: 0.3rem;\n  padding: 0.5rem;\n}\n\n.typingIndicator span {\n  width: 8px;\n  height: 8px;\n  background-color: #888;\n  border-radius: 50%;\n  animation: bounce 1.4s infinite ease-in-out;\n}\n\n.typingIndicator span:nth-child(1) { animation-delay: -0.32s; }\n.typingIndicator span:nth-child(2) { animation-delay: -0.16s; }\n\n@keyframes bounce {\n  0%, 80%, 100% { transform: scale(0); }\n  40% { transform: scale(1); }\n}\n\n\n\n================================================\nFile: example_apps/ai-chat/src/app/page.tsx\n================================================\n'use client';\n\nimport { useState, useRef, useEffect } from 'react';\n\nexport default function Home() {\n  const [messages, setMessages] = useState<Array<{ role: string; content: string }>>([]);\n  const [input, setInput] = useState('');\n  const [isTyping, setIsTyping] = useState(false);\n  const messagesEndRef = useRef<HTMLDivElement>(null);\n\n  const scrollToBottom = () => {\n    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n  };\n\n  useEffect(() => {\n    scrollToBottom();\n  }, [messages]);\n\n  const handleSubmit = async (e: React.FormEvent) => {\n    e.preventDefault();\n    if (!input.trim()) return;\n\n    const userMessage = { role: 'user', content: input };\n    setMessages(prev => [...prev, userMessage]);\n    setInput('');\n    setIsTyping(true);\n\n    try {\n      const response = await fetch('http://localhost:11434/api/chat', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({\n          model: 'llama3.2',\n          messages: [...messages, userMessage],\n        }),\n      });\n\n      if (!response.body) throw new Error('No response body');\n\n      const reader = response.body.getReader();\n      let aiResponse = '';\n      \n      // Initialize assistant message immediately\n      setMessages(prev => [...prev, { role: 'assistant', content: '' }]);\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const text = new TextDecoder().decode(value);\n        const lines = text.split('\\n').filter(line => line.trim());\n        \n        for (const line of lines) {\n          try {\n            const data = JSON.parse(line);\n            if (data.message?.content) {\n              aiResponse += data.message.content;\n              \n              // Update the last message with new content\n              setMessages(prev => {\n                const newMessages = [...prev];\n                const lastMessage = newMessages[newMessages.length - 1];\n                if (lastMessage && lastMessage.role === 'assistant') {\n                  lastMessage.content = aiResponse;\n                  return [...newMessages];\n                }\n                return prev;\n              });\n            }\n          } catch (e) {\n            console.error('Error parsing JSON:', e);\n          }\n        }\n      }\n    } catch (error) {\n      console.error('Error:', error);\n      setMessages(prev => [...prev, { role: 'assistant', content: 'Sorry, there was an error processing your request.' }]);\n    } finally {\n      setIsTyping(false);\n    }\n  };\n\n  return (\n    <main className=\"flex justify-center items-center min-h-screen p-4 bg-gray-50\">\n      <div className=\"w-full max-w-3xl bg-white rounded-xl shadow-lg overflow-hidden flex flex-col h-[80vh]\">\n        {/* Header */}\n        <div className=\"bg-primary p-4 text-white\">\n          <h1 className=\"text-xl font-semibold\">AI Frontend Assistant</h1>\n          <p className=\"text-sm opacity-80\">Ask any frontend development questions</p>\n        </div>\n        \n        {/* Messages Area */}\n        <div className=\"flex-1 p-4 overflow-y-auto space-y-4\">\n          {messages.length === 0 && (\n            <div className=\"text-center text-gray-500 my-8\">\n              <p className=\"text-lg font-medium\">Welcome to the AI Frontend Assistant!</p>\n              <p className=\"mt-2\">Ask any questions about frontend development, React, CSS, JavaScript, and more.</p>\n            </div>\n          )}\n          \n          {messages.map((message, index) => (\n            <div key={index} className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}>\n              <div className={message.role === 'user' ? 'user-bubble' : 'ai-bubble'}>\n                {message.content || (\n                  message.role === 'assistant' && <span className=\"text-gray-400\">Loading...</span>\n                )}\n              </div>\n            </div>\n          ))}\n          \n          {isTyping && (\n            <div className=\"flex justify-start\">\n              <div className=\"ai-bubble flex space-x-1 py-3 px-4\">\n                <div className=\"typing-dot\" style={{ animationDelay: '0s' }}></div>\n                <div className=\"typing-dot\" style={{ animationDelay: '0.2s' }}></div>\n                <div className=\"typing-dot\" style={{ animationDelay: '0.4s' }}></div>\n              </div>\n            </div>\n          )}\n          <div ref={messagesEndRef} />\n        </div>\n        \n        {/* Input Area */}\n        <form onSubmit={handleSubmit} className=\"p-4 border-t border-gray-200 flex gap-2\">\n          <input\n            type=\"text\"\n            value={input}\n            onChange={(e) => setInput(e.target.value)}\n            placeholder=\"Ask a frontend question...\"\n            className=\"flex-1 py-2 px-4 border border-gray-300 rounded-full focus:outline-none focus:ring-2 focus:ring-primary focus:border-transparent\"\n          />\n          <button \n            type=\"submit\" \n            className=\"bg-primary hover:bg-primary-dark text-white font-medium py-2 px-6 rounded-full transition-colors\"\n            disabled={isTyping}\n          >\n            Send\n          </button>\n        </form>\n      </div>\n    </main>\n  );\n}\n\n\n\n================================================\nFile: examples/containers/Dockerfile.finetune\n================================================\nFROM pytorch/pytorch:2.1.1-cuda12.1-cudnn8-runtime\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    gcc \\\n    g++ \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python packages\nRUN pip install --no-cache-dir \\\n    datasets \\\n    peft \\\n    typer \\\n    requests \\\n    transformers \\\n    triton \\\n    bitsandbytes \\\n    trl \\\n    accelerate \\\n    unsloth_zoo \\\n    \"unsloth[cu121-torch221] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Set working directory\nWORKDIR /app\n\n# Copy the application code\nCOPY . .\n\n\n\n================================================\nFile: examples/containers/Browser Use/Dockerfile\n================================================\n#---\n# name: transformers\n# config: config.py\n# group: llm\n# depends: [pytorch, torchvision, huggingface_hub, rust]\n# test: [test_version.py, huggingface-benchmark.py]\n# docs: docs.md\n# notes: for quantization support in Transformers, use the bitsandbytes, AutoGPTQ, or AutoAWQ containers.\n#---\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG TRANSFORMERS_PACKAGE=transformers \\\n    TRANSFORMERS_VERSION\n\n# if you want optimum[exporters,onnxruntime] see the optimum package\n\nRUN pip3 install --no-cache-dir --verbose accelerate && \\\n    pip3 install --no-cache-dir --verbose sentencepiece && \\\n    pip3 install --no-cache-dir --verbose optimum && \\\n    \\\n    # install from pypi, git, ect (sometimes other version got installed)\n    pip3 uninstall -y transformers && \\\n    \\\n    echo \"Installing tranformers $TRANSFORMERS_VERSION (from $TRANSFORMERS_PACKAGE)\" && \\\n    pip3 install --no-cache-dir --verbose ${TRANSFORMERS_PACKAGE} && \\\n    \\\n    # \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 118\n    # AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\n    PYTHON_ROOT=`pip3 show transformers | grep Location: | cut -d' ' -f2` && \\\n    sed -i \\\n        -e 's|torch.distributed.is_initialized|torch.distributed.is_available|g' \\\n        ${PYTHON_ROOT}/transformers/modeling_utils.py\n    \n# add benchmark script\nCOPY huggingface-benchmark.py /usr/local/bin\n    \n# make sure it loads\nRUN pip3 show transformers \\\n    && python3 -c 'import transformers; print(transformers.__version__)'\n\n\n================================================\nFile: examples/containers/Computer Use/Dockerfile\n================================================\n#---\n# name: transformers\n# config: config.py\n# group: llm\n# depends: [pytorch, torchvision, huggingface_hub, rust]\n# test: [test_version.py, huggingface-benchmark.py]\n# docs: docs.md\n# notes: for quantization support in Transformers, use the bitsandbytes, AutoGPTQ, or AutoAWQ containers.\n#---\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG TRANSFORMERS_PACKAGE=transformers \\\n    TRANSFORMERS_VERSION\n\n# if you want optimum[exporters,onnxruntime] see the optimum package\n\nRUN pip3 install --no-cache-dir --verbose accelerate && \\\n    pip3 install --no-cache-dir --verbose sentencepiece && \\\n    pip3 install --no-cache-dir --verbose optimum && \\\n    \\\n    # install from pypi, git, ect (sometimes other version got installed)\n    pip3 uninstall -y transformers && \\\n    \\\n    echo \"Installing tranformers $TRANSFORMERS_VERSION (from $TRANSFORMERS_PACKAGE)\" && \\\n    pip3 install --no-cache-dir --verbose ${TRANSFORMERS_PACKAGE} && \\\n    \\\n    # \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 118\n    # AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\n    PYTHON_ROOT=`pip3 show transformers | grep Location: | cut -d' ' -f2` && \\\n    sed -i \\\n        -e 's|torch.distributed.is_initialized|torch.distributed.is_available|g' \\\n        ${PYTHON_ROOT}/transformers/modeling_utils.py\n    \n# add benchmark script\nCOPY huggingface-benchmark.py /usr/local/bin\n    \n# make sure it loads\nRUN pip3 show transformers \\\n    && python3 -c 'import transformers; print(transformers.__version__)'\n\n\n================================================\nFile: examples/containers/Cosmos/Dockerfile\n================================================\n#--- \n# name: cosmos\n# group: diffusion\n# depends: [pytorch, torchvision, torchaudio, transformer-engine, transformers, opencv:4.11.0, bitsandbytes, huggingface_hub, ffmpeg]\n# requires: '>=34.1.0'\n# docs: docs.md\n#---\n\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    WORKDIR /opt\n    \n    RUN apt-get update && apt-get install -y \\\n        ffmpeg \\\n        && rm -rf /var/lib/apt/lists/*\n    \n    RUN git clone --recursive https://github.com/NVIDIA/Cosmos && \\\n        cd Cosmos && \\\n        pip3 install --no-cache-dir einops attrs pynvml opencv-contrib-python protobuf && \\\n        pip3 install --no-cache-dir -r requirements.txt\n    \n    WORKDIR /opt/Cosmos/\n    CMD [\"/bin/bash\"]\n\n\n================================================\nFile: examples/containers/JAX/Dockerfile\n================================================\n#---\n# name: jax\n# group: jax\n# config: config.py\n# depends: [cuda, cudnn, numpy]\n# test: test.py\n# docs: Containers for JAX with CUDA support.\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    # set the CUDA architectures that JAX extensions get built for\n    # set the JAX cache directory to mounted /data volume\n    ARG JAX_CUDA_ARCH_ARGS \\\n        JAX_VERSION \\\n        JAX_BUILD_VERSION \\\n        ENABLE_NCCL \\\n        FORCE_BUILD=off\n    \n    ENV JAX_CUDA_ARCH_LIST=${JAX_CUDA_ARCH_ARGS} \\\n        JAX_CACHE_DIR=/data/models/jax\n    \n    # copy installation and build scripts for JAX\n    COPY build.sh install.sh link_cuda.sh /tmp/JAX/\n    \n    # attempt to install JAX from pip, and fall back to building it if the installation fails\n    RUN /tmp/JAX/install.sh || /tmp/JAX/build.sh\n        \n\n\n================================================\nFile: examples/containers/LITA/Dockerfile\n================================================\n#---\n# name: lita\n# group: vlm\n# docs: docs.md\n# depends: [decord, pytorch, transformers, bitsandbytes, flash-attention]\n# requires: '>=34.1.0'\n# test: test.py\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ARG LITA_ROOT=/opt/LITA \\\n        LITA_REPO=NVlabs/LITA \\\n        LITA_BRANCH=main \\\n        \n    ADD https://api.github.com/repos/${LITA_REPO}/git/refs/heads/${LITA_BRANCH} /tmp/lita_version.json\n    \n    RUN git clone --branch=${LITA_BRANCH} --depth=1 https://github.com/${LITA_REPO} ${LITA_ROOT} && \\\n        cd ${LITA_ROOT} && \\\n        #sed 's|\"transformers.*\"|\"transformers<=4.35.2\"|' -i pyproject.toml && \\\n        #sed 's|\"accelerate.*\",||' -i pyproject.toml && \\\n        #sed 's|\"deepspeed.*\",||' -i pyproject.toml && \\\n        sed 's|\"bitsandbytes.*\",||' -i pyproject.toml && \\\n        #sed 's|\"peft.*\",||' -i pyproject.toml && \\\n        #sed 's|\"timm.*\",||' -i pyproject.toml && \\\n        cat pyproject.toml\n    \n    COPY benchmark.py ${LITA_ROOT}/lita/serve/\n    \n    RUN pip3 install --no-cache-dir --verbose -e ${LITA_ROOT} \n    \n\n\n================================================\nFile: examples/containers/LeRobot/Dockerfile\n================================================\n#---\n# name: lerobot\n# group: robots\n# docs: docs.md\n# depends: [transformers, opencv:4.11.0, pyav, h5py, jupyterlab:myst]\n# requires: '>=36'\n# test: [test.sh, test.py]\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ARG LEROBOT_REPO=huggingface/lerobot/\n    ARG LEROBOT_BRANCH=main\n    \n    RUN git clone --branch=${LEROBOT_BRANCH} --depth=1 https://github.com/${LEROBOT_REPO} /opt/lerobot && \\\n        cd /opt/lerobot && \\\n        sed 's|^python.*||' -i pyproject.toml && \\\n        sed 's|^torch.*||' -i pyproject.toml && \\\n        sed 's|^opencv-python.*||' -i pyproject.toml && \\\n        sed 's|^torchvision.*||' -i pyproject.toml && \\\n        sed 's|^h5py.*||' -i pyproject.toml && \\\n        sed 's|^pyav.*||' -i pyproject.toml && \\\n        sed 's|^huggingface-hub.*||' -i pyproject.toml && \\\n        echo \"######### pyproject.toml ##########\" && \\\n        cat -n pyproject.toml\n    \n    RUN cd /opt/lerobot && \\\n        pip3 install --ignore-installed -e \".[aloha, pusht]\" --verbose\n        \n    RUN cd /opt/lerobot && \\\n        pip install -e \".[dynamixel]\" --verbose\n    \n    RUN apt-get update && \\\n        apt-get install -y --no-install-recommends \\\n                speech-dispatcher speech-dispatcher-espeak-ng pulseaudio-utils alsa-utils vim \\\n        && rm -rf /var/lib/apt/lists/* \\\n        && apt-get clean\n    \n    ENV JUPYTER_ROOT=/opt/lerobot\n    \n    # Create a symbolic link for python3 to python\n    RUN ln -s /usr/bin/python3 /usr/bin/python\n    \n    # Verify the symlink is created and the version of Python\n    RUN python --version\n    \n    WORKDIR /opt/lerobot\n    \n    ## Prevent the Gdk-ERROR by disabling MIT-SHM\n    RUN export GDK_BACKEND=x11\n    RUN export GDK_USE_X11SHM=0\n    RUN export GDK_SYNCHRONIZE=1\n    \n    RUN echo \"pactl info\" >> /root/.bash_history\n    RUN echo \"pactl list short sinks\" >> /root/.bash_history\n    RUN echo \"pactl set-default-sink 0\" >> /root/.bash_history\n    RUN echo \"wandb login\" >> /root/.bash_history\n    RUN echo \"export HF_USER=\" >> /root/.bash_history\n    RUN echo \"python lerobot/scripts/control_robot.py record \\\n      --robot-path lerobot/configs/robot/koch.yaml \\\n      --fps 30 \\\n      --root data \\\n      --repo-id \\${HF_USER}/koch_test_\\$(date +%Y%m%d_%H%M%S) \\\n      --tags tutorial \\\n      --warmup-time-s 5 \\\n      --episode-time-s 30 \\\n      --reset-time-s 30 \\\n      --num-episodes 10\" >> /root/.bash_history\n    \n    RUN echo -e \"* soft core 0\\n* hard core 0\" >> /etc/security/limits.conf\n    \n    CMD /start_jupyter && speech-dispatcher --spawn  && /bin/bash\n\n\n================================================\nFile: examples/containers/NanoLLM/Dockerfile\n================================================\n#---\n# name: nano_llm\n# group: llm\n# config: config.py\n# depends: [mlc, riva-client:python, jetson-utils, torch2trt, torchaudio, piper-tts, nanodb, mimicgen]\n# requires: '>=35'\n# docs: docs.md\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ARG NANO_LLM_BRANCH=main \\\n        NANO_LLM_PATH=/opt/NanoLLM\n    \n    ENV PYTHONPATH=${PYTHONPATH}:${NANO_LLM_PATH} \\\n        SSL_KEY=/etc/ssl/private/localhost.key.pem \\\n        SSL_CERT=/etc/ssl/private/localhost.cert.pem\n        \n    ADD https://api.github.com/repos/dusty-nv/NanoLLM/git/refs/heads/${NANO_LLM_BRANCH} /tmp/nano_llm_version.json\n    \n    COPY install.sh /tmp/nano_llm/install.sh\n    \n    RUN /tmp/nano_llm/install.sh\n\n\n================================================\nFile: examples/containers/NanoOWL/Dockerfile\n================================================\n#---\n# name: nanoowl\n# group: vit\n# depends: [pytorch, torch2trt, transformers, opencv, gstreamer]\n# requires: '>=34.1.0'\n# test: test.sh\n# docs: docs.md\n#---\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG NANOOWL_REPO=NVIDIA-AI-IOT/nanoowl \\\n    NANOOWL_BRANCH=main\n\nRUN pip3 install --verbose --no-cache-dir 'matplotlib<3.9' && \\\n    pip3 install --verbose --no-cache-dir git+https://github.com/openai/CLIP.git && \\\n    mkdir -p /root/.cache && \\\n    ln -sf /data/models/clip /root/.cache/clip\n\nADD https://api.github.com/repos/${NANOOWL_REPO}/git/refs/heads/${NANOOWL_BRANCH} /tmp/nanoowl_version.json\n\nRUN git clone --branch=${NANOOWL_BRANCH} https://github.com/${NANOOWL_REPO} /opt/nanoowl && \\\n    cd /opt/nanoowl && \\\n    git checkout main && \\\n    python3 setup.py develop\n\nRUN cd /opt/nanoowl && \\\n    mkdir data && \\\n    python3 -m nanoowl.build_image_encoder_engine \\\n        data/owl_image_encoder_patch32.engine \\\n        --onnx_opset=16\n\n    \n\n\n\n\n================================================\nFile: examples/containers/NanoSAM/Dockerfile\n================================================\n#---\n# name: nanosam\n# group: vit\n# depends: [pytorch, torch2trt, transformers]\n# requires: '>=34.1.0'\n# docs: docs.md\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    WORKDIR /opt\n    \n    # 1. Install the dependencies\n    # \n    # PyTorch and torch2trt are specified in the header yaml part (under \"depends:\")\n    #\n    RUN git clone https://github.com/NVIDIA-AI-IOT/trt_pose && \\\n        cd trt_pose && \\\n        python3 setup.py develop\n    \n    # 2. Install the NanoSAM Python package\n    RUN git clone https://github.com/NVIDIA-AI-IOT/nanosam && \\\n        cd nanosam && \\\n        python3 setup.py develop\n    \n    # 3. Build the TensorRT engine for the mask decoder\n    RUN pip3 install timm\n    \n    #RUN cd /opt/nanosam && \\\n    #    mkdir data && \\\n    #    python3 -m nanosam.tools.export_sam_mask_decoder_onnx \\\n    #        --model-type=vit_t \\\n    #        --checkpoint=assets/mobile_sam.pt \\\n    #        --output=data/mobile_sam_mask_decoder.onnx\n    \n    RUN mkdir /opt/nanosam/data && \\\n        wget --quiet --show-progress --progress=bar:force:noscroll --no-check-certificate \\\n         https://nvidia.box.com/shared/static/ho09o7ohgp7lsqe0tcxqu5gs2ddojbis.onnx \\\n         -O /opt/nanosam/data/mobile_sam_mask_decoder.onnx\n    \n    RUN cd /opt/nanosam && \\\n        /usr/src/tensorrt/bin/trtexec \\\n            --onnx=data/mobile_sam_mask_decoder.onnx \\\n            --saveEngine=data/mobile_sam_mask_decoder.engine \\\n            --minShapes=point_coords:1x1x2,point_labels:1x1 \\\n            --optShapes=point_coords:1x1x2,point_labels:1x1 \\\n            --maxShapes=point_coords:1x10x2,point_labels:1x10\n    \n    # 4. Build the TensorRT engine for the NanoSAM image encoder\n    RUN pip3 install gdown && \\\n        cd /opt/nanosam/data/ && \\\n        gdown https://drive.google.com/uc?id=14-SsvoaTl-esC3JOzomHDnI9OGgdO2OR && \\\n        ls -lh && \\\n        cd /opt/nanosam/ && \\\n        /usr/src/tensorrt/bin/trtexec \\\n            --onnx=data/resnet18_image_encoder.onnx \\\n            --saveEngine=data/resnet18_image_encoder.engine \\\n            --fp16\n    \n    # 5. Run the basic usage example\n    RUN pip3 install matplotlib\n    RUN cd /opt/nanosam/ && \\\n        python3 examples/basic_usage.py \\\n            --image_encoder=data/resnet18_image_encoder.engine \\\n            --mask_decoder=data/mobile_sam_mask_decoder.engine\n    \n    COPY benchmark.py /opt/nanosam/\n\n\n================================================\nFile: examples/containers/OpenEMMA/Dockerfile\n================================================\n#---\n# name: transformers\n# config: config.py\n# group: llm\n# depends: [pytorch, torchvision, huggingface_hub, rust]\n# test: [test_version.py, huggingface-benchmark.py]\n# docs: docs.md\n# notes: for quantization support in Transformers, use the bitsandbytes, AutoGPTQ, or AutoAWQ containers.\n#---\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG TRANSFORMERS_PACKAGE=transformers \\\n    TRANSFORMERS_VERSION\n\n# if you want optimum[exporters,onnxruntime] see the optimum package\n\nRUN pip3 install --no-cache-dir --verbose accelerate && \\\n    pip3 install --no-cache-dir --verbose sentencepiece && \\\n    pip3 install --no-cache-dir --verbose optimum && \\\n    \\\n    # install from pypi, git, ect (sometimes other version got installed)\n    pip3 uninstall -y transformers && \\\n    \\\n    echo \"Installing tranformers $TRANSFORMERS_VERSION (from $TRANSFORMERS_PACKAGE)\" && \\\n    pip3 install --no-cache-dir --verbose ${TRANSFORMERS_PACKAGE} && \\\n    \\\n    # \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 118\n    # AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\n    PYTHON_ROOT=`pip3 show transformers | grep Location: | cut -d' ' -f2` && \\\n    sed -i \\\n        -e 's|torch.distributed.is_initialized|torch.distributed.is_available|g' \\\n        ${PYTHON_ROOT}/transformers/modeling_utils.py\n    \n# add benchmark script\nCOPY huggingface-benchmark.py /usr/local/bin\n    \n# make sure it loads\nRUN pip3 show transformers \\\n    && python3 -c 'import transformers; print(transformers.__version__)'\n\n\n================================================\nFile: examples/containers/Policy/Dockerfile\n================================================\n#---\n# name: transformers\n# config: config.py\n# group: llm\n# depends: [pytorch, torchvision, huggingface_hub, rust]\n# test: [test_version.py, huggingface-benchmark.py]\n# docs: docs.md\n# notes: for quantization support in Transformers, use the bitsandbytes, AutoGPTQ, or AutoAWQ containers.\n#---\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG TRANSFORMERS_PACKAGE=transformers \\\n    TRANSFORMERS_VERSION\n\n# if you want optimum[exporters,onnxruntime] see the optimum package\n\nRUN pip3 install --no-cache-dir --verbose accelerate && \\\n    pip3 install --no-cache-dir --verbose sentencepiece && \\\n    pip3 install --no-cache-dir --verbose optimum && \\\n    \\\n    # install from pypi, git, ect (sometimes other version got installed)\n    pip3 uninstall -y transformers && \\\n    \\\n    echo \"Installing tranformers $TRANSFORMERS_VERSION (from $TRANSFORMERS_PACKAGE)\" && \\\n    pip3 install --no-cache-dir --verbose ${TRANSFORMERS_PACKAGE} && \\\n    \\\n    # \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 118\n    # AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\n    PYTHON_ROOT=`pip3 show transformers | grep Location: | cut -d' ' -f2` && \\\n    sed -i \\\n        -e 's|torch.distributed.is_initialized|torch.distributed.is_available|g' \\\n        ${PYTHON_ROOT}/transformers/modeling_utils.py\n    \n# add benchmark script\nCOPY huggingface-benchmark.py /usr/local/bin\n    \n# make sure it loads\nRUN pip3 show transformers \\\n    && python3 -c 'import transformers; print(transformers.__version__)'\n\n\n================================================\nFile: examples/containers/Pytorch/Dockerfile\n================================================\n#---\n# name: pytorch\n# alias: torch\n# group: pytorch\n# config: config.py\n# depends: [cuda, cudnn, numpy, onnx]\n# test: [test.sh, test.py]\n# docs: |\n#  Containers for PyTorch with CUDA support.\n#  Note that the [`l4t-pytorch`](/packages/l4t/l4t-pytorch) containers also include PyTorch, `torchvision`, and `torchaudio`.\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    # set the CUDA architectures that PyTorch extensions get built for\n    # set the torch hub model cache directory to mounted /data volume\n    ARG TORCH_CUDA_ARCH_ARGS \\\n        TORCH_VERSION \\\n        PYTORCH_BUILD_VERSION \\\n        USE_NCCL=0 \\\n        USE_GLOO=1 \\\n        USE_MPI=1 \\\n        USE_NNPACK=1 \\\n        USE_XNNPACK=1 \\\n        USE_PYTORCH_QNNPACK=1 \\\n        FORCE_BUILD=off\n        \n    ENV TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_ARGS} \\\n        TORCH_HOME=/data/models/torch\n    \n    COPY install.sh build.sh /tmp/pytorch/\n    \n    # attempt to install from pip, and fall back to building it\n    RUN /tmp/pytorch/install.sh || /tmp/pytorch/build.sh\n\n\n================================================\nFile: examples/containers/ROS/Dockerfile\n================================================\n#\n# Dockerfile for building ROS2 from source\n#\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG ROS_PACKAGE=ros_base \\\n    ROS_VERSION=humble\n\nENV ROS_DISTRO=${ROS_VERSION} \\\n    ROS_ROOT=/opt/ros/${ROS_VERSION} \\\n    ROS_PYTHON_VERSION=3 \\\n    RMW_IMPLEMENTATION=rmw_fastrtps_cpp \\\n    DEBIAN_FRONTEND=noninteractive \\\n    SHELL=/bin/bash\n    \nSHELL [\"/bin/bash\", \"-c\"] \n\n# set Python3 as default\nRUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n\n# Numpy 2.0\nRUN python3 -m pip install --no-cache-dir \"numpy>=2.0.0\"\nRUN python3 -c \"import numpy; print('NumPy version:', numpy.__version__)\"\n\n# build ROS from source\nCOPY ros2_build.sh /tmp/ros2_build.sh\nRUN /tmp/ros2_build.sh\n\n# Set the default DDS middleware to cyclonedds\n# https://github.com/ros2/rclcpp/issues/1335\n# https://docs.ros.org/en/jazzy/Installation/DDS-Implementations/Working-with-eProsima-Fast-DDS.html\n#ENV RMW_IMPLEMENTATION=rmw_cyclonedds_cpp\n\n# commands will be appended/run by the entrypoint which sources the ROS environment\nCOPY ros_entrypoint.sh ros2_install.sh /\n\nENTRYPOINT [\"/ros_entrypoint.sh\"]\nCMD [\"/bin/bash\"]\n\n\n\n================================================\nFile: examples/containers/Tensorflow/Dockerfile\n================================================\n#---\n# name: tensorflow\n# group: ml\n# depends: [cuda, cudnn, tensorrt, python, numpy, h5py, bazel, protobuf:cpp]\n# test: test.py\n# docs: |\n#   Container for TF1/TF2 with CUDA support.\n#   Note that the [`l4t-tensorflow`](/packages/l4t/l4t-tensorflow) containers are similar, with the addition of OpenCV and PyCUDA.  \n#\n#   The TensorFlow wheels used in these are from https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ARG TENSORFLOW_URL \\\n        TENSORFLOW_WHL \\\n        HDF5_DIR=\"/usr/lib/aarch64-linux-gnu/hdf5/serial/\" \\\n        MAKEFLAGS=-j$(nproc) \\\n        FORCE_BUILD\n    \n    COPY install.sh /tmp/tensorflow/\n    \n    RUN /tmp/tensorflow/install.sh\n\n\n================================================\nFile: examples/containers/Transformers/Dockerfile\n================================================\n#---\n# name: transformers\n# config: config.py\n# group: llm\n# depends: [pytorch, torchvision, huggingface_hub, rust]\n# test: [test_version.py, huggingface-benchmark.py]\n# docs: docs.md\n# notes: for quantization support in Transformers, use the bitsandbytes, AutoGPTQ, or AutoAWQ containers.\n#---\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG TRANSFORMERS_PACKAGE=transformers \\\n    TRANSFORMERS_VERSION\n\n# if you want optimum[exporters,onnxruntime] see the optimum package\n\nRUN pip3 install --no-cache-dir --verbose accelerate && \\\n    pip3 install --no-cache-dir --verbose sentencepiece && \\\n    pip3 install --no-cache-dir --verbose optimum && \\\n    \\\n    # install from pypi, git, ect (sometimes other version got installed)\n    pip3 uninstall -y transformers && \\\n    \\\n    echo \"Installing tranformers $TRANSFORMERS_VERSION (from $TRANSFORMERS_PACKAGE)\" && \\\n    pip3 install --no-cache-dir --verbose ${TRANSFORMERS_PACKAGE} && \\\n    \\\n    # \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 118\n    # AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\n    PYTHON_ROOT=`pip3 show transformers | grep Location: | cut -d' ' -f2` && \\\n    sed -i \\\n        -e 's|torch.distributed.is_initialized|torch.distributed.is_available|g' \\\n        ${PYTHON_ROOT}/transformers/modeling_utils.py\n    \n# add benchmark script\nCOPY huggingface-benchmark.py /usr/local/bin\n    \n# make sure it loads\nRUN pip3 show transformers \\\n    && python3 -c 'import transformers; print(transformers.__version__)'\n\n\n================================================\nFile: examples/containers/VILA/Dockerfile\n================================================\n#---\n# name: vila\n# group: vlm\n# depends: [pytorch, flash-attention, transformers, opencv, deepspeed:0.9.5]\n# requires: '>=35'\n# test: [test.sh, test.py]\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ADD https://api.github.com/repos/NVlabs/VILA/git/refs/heads/main /tmp/vila_version.json\n    \n    RUN git clone --branch=main --depth=1 --recursive https://github.com/NVlabs/VILA /opt/VILA && \\\n        cd /opt/VILA && \\\n        sed -i 's|torch==.*\"|torch\"|' pyproject.toml && \\\n        sed -i 's|pytorchvideo==.*\"|pytorchvideo\"|' pyproject.toml && \\\n        sed -i 's|opencv-python==.*\"|opencv-python\"|' pyproject.toml && \\\n        sed -i 's|\"bitsandbytes.*\",||' pyproject.toml && \\\n        cat pyproject.toml | grep torch && \\\n        pip3 install --verbose -e . && \\\n        pip3 install --verbose -e \".[train]\" && \\\n        pip3 install --verbose -e \".[eval]\" \n        \n    #RUN pip3 install --verbose --no-cache-dir 'transformers==4.37.2' && \\\n    #    site_pkg_path=$(python3 -c 'import site; print(site.getsitepackages()[0])') && \\\n    #    cp -rv /opt/VILA/llava/train/transformers_replace/* $site_pkg_path/transformers/ && \\\n    #    cp -rv /opt/VILA/llava/train/deepspeed_replace/* $site_pkg_path/deepspeed/\n    \n\n\n================================================\nFile: examples/containers/homeassistant-core/Dockerfile\n================================================\n#---\n# name: homeassistant-core\n# group: smart-home\n# config: config.py\n# requires: '>=34.1.0'\n# docs: docs.md\n# depends: [homeassistant-base, ciso8601, psutil-home-assistant, ffmpeg]\n# test: [go2rtc_test.sh, sqlite3_test.py, test.py]\n# notes: The `homeassistant-core` wheel that's build is saved in `/usr/src/homeassistant`\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ARG HA_VERSION \\\n        SQLITE_VERSION\n    \n    SHELL [\"/bin/bash\", \"-exo\", \"pipefail\", \"-c\"]\n    \n    WORKDIR /usr/src\n    \n    ENV S6_SERVICES_READYTIME=50 \\\n        S6_SERVICES_GRACETIME=240000 \\\n        UV_SYSTEM_PYTHON=true \\\n        UV_NO_CACHE=true \\\n        LD_PRELOAD=\"${LD_PRELOAD:-}:/usr/local/lib/libjemalloc.so.2\" \\\n        MALLOC_CONF=\"background_thread:true,metadata_thp:auto,dirty_decay_ms:20000,muzzy_decay_ms:20000\" \\\n        SQLITE_VERSION=\"${SQLITE_VERSION}\"\n    \n    COPY *.sh /tmp/homeassistant/\n    \n    RUN /tmp/homeassistant/build.sh \\\n        && ln -s /data/homeassistant /config \\\n        && curl -L https://github.com/AlexxIT/go2rtc/releases/download/v1.9.7/go2rtc_linux_arm64 --output /bin/go2rtc \\\n        && chmod +x /bin/go2rtc \\\n        && go2rtc --version\n    \n    ENV LD_LIBRARY_PATH=${LD_LIBRARY_PATH:-}:/usr/local/lib\n    \n    WORKDIR /config\n\n\n================================================\nFile: examples/containers/langchain/Dockerfile\n================================================\n#---\n# name: langchain\n# alias: langchain:main\n# group: rag\n# depends: [pytorch, llama_cpp]\n# requires: '>=34.1.0'\n# docs: docs.md\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    RUN pip3 install --no-cache-dir --verbose langchain[llm] openai\n\n\n================================================\nFile: examples/containers/llama-index/Dockerfile\n================================================\n#---\n# name: llama-index\n# alias: llama-index:main\n# group: rag\n# depends: [pytorch]\n# requires: '>=34.1.0'\n# docs: docs.md\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    RUN pip3 install --no-cache-dir --verbose \\\n            llama-index-core \\\n            llama-index-readers-file \\\n            llama-index-llms-ollama \\\n            llama-index-embeddings-huggingface\n    \n    CMD /bin/bash\n\n\n================================================\nFile: examples/containers/llama.cpp/Dockerfile\n================================================\n#---\n# name: transformers\n# config: config.py\n# group: llm\n# depends: [pytorch, torchvision, huggingface_hub, rust]\n# test: [test_version.py, huggingface-benchmark.py]\n# docs: docs.md\n# notes: for quantization support in Transformers, use the bitsandbytes, AutoGPTQ, or AutoAWQ containers.\n#---\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG TRANSFORMERS_PACKAGE=transformers \\\n    TRANSFORMERS_VERSION\n\n# if you want optimum[exporters,onnxruntime] see the optimum package\n\nRUN pip3 install --no-cache-dir --verbose accelerate && \\\n    pip3 install --no-cache-dir --verbose sentencepiece && \\\n    pip3 install --no-cache-dir --verbose optimum && \\\n    \\\n    # install from pypi, git, ect (sometimes other version got installed)\n    pip3 uninstall -y transformers && \\\n    \\\n    echo \"Installing tranformers $TRANSFORMERS_VERSION (from $TRANSFORMERS_PACKAGE)\" && \\\n    pip3 install --no-cache-dir --verbose ${TRANSFORMERS_PACKAGE} && \\\n    \\\n    # \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 118\n    # AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\n    PYTHON_ROOT=`pip3 show transformers | grep Location: | cut -d' ' -f2` && \\\n    sed -i \\\n        -e 's|torch.distributed.is_initialized|torch.distributed.is_available|g' \\\n        ${PYTHON_ROOT}/transformers/modeling_utils.py\n    \n# add benchmark script\nCOPY huggingface-benchmark.py /usr/local/bin\n    \n# make sure it loads\nRUN pip3 show transformers \\\n    && python3 -c 'import transformers; print(transformers.__version__)'\n\n\n================================================\nFile: examples/containers/llava/Dockerfile\n================================================\n#---\n# name: llava\n# group: vlm\n# docs: docs.md\n# depends: [pytorch, flash-attention, transformers]\n# requires: '>=34.1.0'\n# test: test.py\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    WORKDIR /opt\n    \n    ARG LLAVA_REPO=haotian-liu/LLaVA\n    ARG LLAVA_BRANCH=main\n    \n    ADD https://api.github.com/repos/${LLAVA_REPO}/git/refs/heads/${LLAVA_BRANCH} /tmp/llava_version.json\n    \n    RUN git clone --branch=${LLAVA_BRANCH} --depth=1 https://github.com/${LLAVA_REPO} llava && \\\n        cd llava && \\\n        sed 's|torch==.*\"|torch\"|' -i pyproject.toml && \\\n        sed 's|\"bitsandbytes.*\",||' -i pyproject.toml && \\\n        cat pyproject.toml\n    \n    COPY benchmark.py llava/llava/serve/\n    \n    RUN cd llava && \\\n        pip3 wheel --wheel-dir=dist --no-deps --verbose . && \\\n        cp dist/llava*.whl /opt\n        \n    RUN pip3 install --no-cache-dir --verbose llava*.whl\n    \n    #RUN pip3 install --verbose /opt/torch*.whl\n    #RUN pip3 install --verbose /opt/torchvision*.whl\n    \n    WORKDIR /\n\n\n================================================\nFile: examples/containers/ollama/Dockerfile\n================================================\n#---\n# name: ollama\n# group: llm\n# config: config.py\n# depends: cuda\n# requires: '>=34.1.0'\n# docs: docs.md\n#---\n    ARG BASE_IMAGE \\\n    CMAKE_CUDA_ARCHITECTURES \\\n    CUDA_VERSION_MAJOR \\\n    JETPACK_VERSION \\\n    OLLAMA_REPO \\\n    OLLAMA_BRANCH \\\n    GOLANG_VERSION \\\n    CMAKE_VERSION\n\n# build the runtime container\nFROM ${BASE_IMAGE}\n\nARG JETPACK_VERSION \\\n    CUDA_VERSION_MAJOR\n\nEXPOSE 11434\nENV OLLAMA_HOST=0.0.0.0 \\\n    OLLAMA_MODELS=/data/models/ollama/models \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n    LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda/include:${LD_LIBRARY_PATH} \\\n    JETSON_JETPACK=${JETPACK_VERSION} \\\n    CUDA_VERSION_MAJOR=${CUDA_VERSION_MAJOR}\n\nRUN apt-get update && \\\n    apt install -y curl  && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    apt-get clean\n\nCOPY nv_tegra_release /etc/nv_tegra_release\n\n#ADD https://api.github.com/repos/ollama/ollama/branches/main /tmp/ollama_version.json\nRUN curl -H \"Authorization: token ${GITHUB_TOKEN}\" \\\n    -o /tmp/ollama_version.json \\\n    https://api.github.com/repos/ollama/ollama/branches/main\n\nRUN curl -fsSL https://ollama.com/install.sh | sh\n\nRUN ln -s /usr/bin/python3 /usr/bin/python\n\nCOPY start_ollama /\n\nCMD /start_ollama && /bin/bash\n\n\n================================================\nFile: examples/containers/onnx/Dockerfile\n================================================\n#---\n# name: onnxruntime\n# group: ml\n# config: config.py\n# depends: [cuda, cudnn, tensorrt, cmake, python, numpy, onnx]\n# test: test.py\n# notes: the `onnxruntime-gpu` wheel that's built is saved in the container under `/opt`\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ARG ONNXRUNTIME_VERSION \\\n        ONNXRUNTIME_BRANCH \\\n        ONNXRUNTIME_FLAGS \\\n        FORCE_BUILD=off\n        \n    COPY install.sh build.sh /tmp/onnxruntime/\n    \n    RUN /tmp/onnxruntime/install.sh || /tmp/onnxruntime/build.sh\n\n\n================================================\nFile: examples/containers/piper/Dockerfile\n================================================\n#---\n# name: piper-tts\n# group: audio\n# depends: [onnxruntime:1.20.1]\n# test: [test_piper_phonemize.sh, test.py]\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ARG PIPER_VERSION=master \\\n        PIPER_PHONEMIZE_VERSION=master \\\n        ESPEAK_NG_VERSION=master \\\n        ONNXRUNTIME_DIR=/usr/local\n    \n    # make PIPER_CACHE a default mounted location available for downloading the models\n    # this still needs to be explicitly passed to piper.get_voices(), ensure_path_exists(), ect.\n    ENV PIPER_CACHE=/data/models/piper \\\n        PIPER_PHONEMIZE_DIR=/usr/local \\\n        ESPEAK_NG_DATA_DIR=\"/opt/espeak-ng/espeak-ng-data\"\n    \n    #ENV LD_LIBRARY_PATH=/usr/local/:$LD_LIBRARY_PATH\n    \n    WORKDIR /opt\n    \n    # install espeak-ng prerequisites\n    RUN apt-get update && \\\n        apt-get install -y --no-install-recommends \\\n            autoconf automake libtool pkg-config \\\n            libsonic-dev ronn kramdown \\\n            libpcaudio-dev \\\n        && rm -rf /var/lib/apt/lists/* \\\n        && apt-get clean\n    \n    # Even though piper-phonemize will download espeak-ng version\n    # automatically, we cannot use it because it will not use\n    # the espeak_TextToPhonemesWithTerminator:\n    # - https://github.com/rhasspy/piper-phonemize/issues/30\n    # - https://github.com/espeak-ng/espeak-ng/pull/2127\n    RUN git clone --branch ${ESPEAK_NG_VERSION} --depth 1 https://github.com/espeak-ng/espeak-ng /opt/espeak-ng && \\\n        cd /opt/espeak-ng && \\\n        git fetch origin pull/2127/head:pr-branch && \\\n        git checkout pr-branch && \\\n        git status && \\\n        ./autogen.sh && \\\n        ./configure --prefix=/usr/local && \\\n        make -j$(nproc) && \\\n        make install && \\\n        ldconfig && \\\n        ls -l $ESPEAK_NG_DATA_DIR\n    \n    # Even though piper will download a piper-phonemize version\n    # automatically, we cannot use it because it will not use\n    # our GPU accelerated ONNXRUNTIME. We need to manually build\n    # piper-phonemize to pass the appropriate flags and have it\n    # use our ORT (which is already in /usr/local/).\n    RUN git clone --branch ${PIPER_PHONEMIZE_VERSION} --depth 1 https://github.com/rhasspy/piper-phonemize /opt/piper-phonemize && \\\n        cd /opt/piper-phonemize && \\\n        cmake -B build \\\n            -DONNXRUNTIME_DIR=${ONNXRUNTIME_DIR} \\\n            -DESPEAK_NG_DIR=${ESPEAK_NG_DATA_DIR} \\\n            -DCMAKE_CXX_FLAGS=\"-I${ONNXRUNTIME_DIR}/include/onnxruntime/\" && \\\n        cmake --build build --config Release --parallel && \\\n        cmake --install build && \\\n        CPPFLAGS=\"-I${ONNXRUNTIME_DIR}/include/onnxruntime/\" \\\n        pip3 install --no-cache-dir --verbose . && \\\n        ln -s ${ESPEAK_NG_DATA_DIR} /usr/share/espeak-ng-data\n    \n    # Now we can build piper. In order to avoid it from downloading\n    # automatically piper-phonemize, we specify an installation dir.\n    RUN git clone --branch ${PIPER_VERSION} --depth 1 https://github.com/rhasspy/piper /opt/piper && \\\n        cd /opt/piper && \\\n        cmake -B build -DPIPER_PHONEMIZE_DIR=${PIPER_PHONEMIZE_DIR} -DCMAKE_CXX_FLAGS=\"-I${ONNXRUNTIME_DIR}/include/onnxruntime/\" && \\\n        cmake --build build --config Release --parallel && \\\n        cmake --install build && \\\n        cd src/python_run && \\\n        pip3 install --ignore-installed --no-cache-dir blinker && \\\n        pip3 install -r requirements_http.txt && \\\n        pip3 install --no-cache-dir --verbose --no-deps .[gpu,http]\n    \n    WORKDIR /\n\n\n================================================\nFile: examples/containers/rag/Dockerfile\n================================================\n#---\n# name: transformers\n# config: config.py\n# group: llm\n# depends: [pytorch, torchvision, huggingface_hub, rust]\n# test: [test_version.py, huggingface-benchmark.py]\n# docs: docs.md\n# notes: for quantization support in Transformers, use the bitsandbytes, AutoGPTQ, or AutoAWQ containers.\n#---\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG TRANSFORMERS_PACKAGE=transformers \\\n    TRANSFORMERS_VERSION\n\n# if you want optimum[exporters,onnxruntime] see the optimum package\n\nRUN pip3 install --no-cache-dir --verbose accelerate && \\\n    pip3 install --no-cache-dir --verbose sentencepiece && \\\n    pip3 install --no-cache-dir --verbose optimum && \\\n    \\\n    # install from pypi, git, ect (sometimes other version got installed)\n    pip3 uninstall -y transformers && \\\n    \\\n    echo \"Installing tranformers $TRANSFORMERS_VERSION (from $TRANSFORMERS_PACKAGE)\" && \\\n    pip3 install --no-cache-dir --verbose ${TRANSFORMERS_PACKAGE} && \\\n    \\\n    # \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 118\n    # AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\n    PYTHON_ROOT=`pip3 show transformers | grep Location: | cut -d' ' -f2` && \\\n    sed -i \\\n        -e 's|torch.distributed.is_initialized|torch.distributed.is_available|g' \\\n        ${PYTHON_ROOT}/transformers/modeling_utils.py\n    \n# add benchmark script\nCOPY huggingface-benchmark.py /usr/local/bin\n    \n# make sure it loads\nRUN pip3 show transformers \\\n    && python3 -c 'import transformers; print(transformers.__version__)'\n\n\n================================================\nFile: examples/containers/txtai/Dockerfile\n================================================\n#---\n# name: transformers\n# config: config.py\n# group: llm\n# depends: [pytorch, torchvision, huggingface_hub, rust]\n# test: [test_version.py, huggingface-benchmark.py]\n# docs: docs.md\n# notes: for quantization support in Transformers, use the bitsandbytes, AutoGPTQ, or AutoAWQ containers.\n#---\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n\nARG TRANSFORMERS_PACKAGE=transformers \\\n    TRANSFORMERS_VERSION\n\n# if you want optimum[exporters,onnxruntime] see the optimum package\n\nRUN pip3 install --no-cache-dir --verbose accelerate && \\\n    pip3 install --no-cache-dir --verbose sentencepiece && \\\n    pip3 install --no-cache-dir --verbose optimum && \\\n    \\\n    # install from pypi, git, ect (sometimes other version got installed)\n    pip3 uninstall -y transformers && \\\n    \\\n    echo \"Installing tranformers $TRANSFORMERS_VERSION (from $TRANSFORMERS_PACKAGE)\" && \\\n    pip3 install --no-cache-dir --verbose ${TRANSFORMERS_PACKAGE} && \\\n    \\\n    # \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 118\n    # AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\n    PYTHON_ROOT=`pip3 show transformers | grep Location: | cut -d' ' -f2` && \\\n    sed -i \\\n        -e 's|torch.distributed.is_initialized|torch.distributed.is_available|g' \\\n        ${PYTHON_ROOT}/transformers/modeling_utils.py\n    \n# add benchmark script\nCOPY huggingface-benchmark.py /usr/local/bin\n    \n# make sure it loads\nRUN pip3 show transformers \\\n    && python3 -c 'import transformers; print(transformers.__version__)'\n\n\n================================================\nFile: examples/containers/vLLM/Dockerfile\n================================================\n#---\n# name: vllm\n# group: vlm\n# config: config.py\n# depends: [pytorch, torchvision, torchaudio, transformers, triton, xformers]\n# requires: '>=34.1.0'\n# test: test.py\n# notes: https://github.com/vllm-project/vllm\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ARG VLLM_VERSION \\\n        XGRAMMAR_VERSION \\\n        FORCE_BUILD=off\n    \n    RUN apt-get update -y && apt-get install -y libnuma-dev \\\n        libsndfile1 libsndfile1-dev libprotobuf-dev libsm6 libxext6 libgl1\n    \n    COPY build.sh install.sh patches /tmp/vllm/\n    \n    RUN /tmp/vllm/install.sh || /tmp/vllm/build.sh\n\n\n================================================\nFile: examples/containers/whisper/Dockerfile\n================================================\n#---\n# name: whisper\n# group: audio\n# depends: [numba, numpy, pytorch, torchaudio, jupyterlab]\n# requires: '>=34.1.0'\n# docs: docs.md\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    WORKDIR /opt\n    \n    RUN apt-get update && \\\n        apt-get install -y --no-install-recommends \\\n              ffmpeg \\\n        && rm -rf /var/lib/apt/lists/* \\\n        && apt-get clean\n    \n    RUN pip3 install --no-cache-dir --verbose pandas scipy jiwer ipywebrtc\n    \n    ADD https://api.github.com/repos/openai/whisper/git/refs/heads/main /tmp/whisper_version.json\n    \n    # Clone the repository:\n    RUN git clone https://github.com/openai/whisper/ && \\\n        cd whisper && \\\n        sed 's|^numba.*||' -i requirements.txt && \\\n        sed 's|^numpy.*||' -i requirements.txt && \\\n        sed 's|^torch.*||' -i requirements.txt && \\\n        cat requirements.txt && \\\n        pip3 wheel --wheel-dir=dist --no-deps --verbose .\n    \n    RUN cp whisper/dist/openai_whisper*.whl /opt && \\\n        pip3 install --no-cache-dir --verbose /opt/openai_whisper*.whl\n        \n    WORKDIR /opt/whisper/\n    \n    COPY record-and-transcribe.ipynb /opt/whisper/notebooks\n    \n    RUN openssl req \\\n            -new \\\n            -newkey rsa:4096 \\\n            -days 365 \\\n            -nodes \\\n            -x509 \\\n            -keyout mykey.key \\\n            -out mycert.pem \\\n            -subj '/CN=localhost'\n    \n    CMD /bin/bash -c \"jupyter lab --ip 0.0.0.0 --port 8888  --certfile=mycert.pem --keyfile mykey.key --allow-root &> /var/log/jupyter.log\" & \\\n        echo \"allow 10 sec for JupyterLab to start @ https://$(hostname -I | cut -d' ' -f1):8888 (password nvidia)\" && \\\n        echo \"JupterLab logging location:  /var/log/jupyter.log  (inside the container)\" && \\\n        /bin/bash\n\n\n================================================\nFile: examples/containers/xtts/Dockerfile\n================================================\n#---\n# name: xtts\n# group: audio\n# depends: [transformers, torchaudio, torch2trt]\n# requires: '>=34.1.0'\n# test: [test.py, test_stream.py]\n# notes: 'Fork of coqui-ai/TTS with support for quantization and TensorRT'\n#---\n    ARG BASE_IMAGE\n    FROM ${BASE_IMAGE}\n    \n    ARG TTS_REPO=dusty-nv/TTS\n    ARG TTS_PATH=/opt/TTS\n    \n    RUN apt-get update && \\\n        apt-get install -y --no-install-recommends libsndfile1-dev && \\\n        rm -rf /var/lib/apt/lists/*  && \\\n        apt-get clean\n        \n    ADD https://api.github.com/repos/${TTS_REPO}/git/refs/heads/dev /tmp/tts_version.json\n    \n    RUN git clone https://github.com/${TTS_REPO} ${TTS_PATH} && \\\n        cd ${TTS_PATH} && \\\n        sed 's|^torch.*||' -i requirements.txt && \\\n        sed 's|^transformers.*||' -i requirements.txt && \\\n        sed 's|answer[[:space:]]*=.*|answer=\\\"y\\\"|' -i TTS/utils/manage.py && \\\n        cat requirements.txt && \\\n        \\\n        pip3 install --ignore-installed --no-cache-dir blinker && \\\n        pip3 install --no-cache-dir --verbose -r requirements.txt && \\\n        \\\n        python3 setup.py --verbose bdist_wheel && \\\n        cp dist/TTS*.whl /opt && \\\n        \\\n        mkdir -p /root/.local/share && \\\n        ln -s /data/models/tts /root/.local/share/tts\n    \n    # https://github.com/huggingface/transformers/issues/31040\n    RUN pip3 install --force-reinstall --no-cache-dir 'transformers<4.41'\n    \n    RUN pip3 install --no-cache-dir --verbose /opt/TTS*.whl && \\\n        pip3 show TTS && python3 -c 'import TTS'\n\n\n================================================\nFile: examples/endpoints/CLiP/client.py\n================================================\nimport requests\nfrom PIL import Image\nimport json\nfrom io import BytesIO\n\ndef send_image_to_server(image_path, server_url):\n    \"\"\"\n    Sends an image to the server and prints the server's response.\n\n    Args:\n    image_path (str): The file path of the image to send.\n    server_url (str): The URL of the server that will process the image.\n    \"\"\"\n    # Open the image, convert it to RGB (in case it's not in that mode)\n    image = Image.open(image_path).convert(\"RGB\")\n\n    # Convert the image to bytes\n    buffered = BytesIO()\n    image.save(buffered, format=\"JPEG\")\n    image_bytes = buffered.getvalue()\n\n    # Send a POST request to the server with the image\n    response = requests.post(\n        server_url,\n        json={\"image_bytes\": image_bytes.hex()}  # Convert bytes to hex string for JSON serialization\n    )\n\n    # Print the response from the server\n    if response.status_code == 200:\n        print(\"Server response:\", response.json())\n    else:\n        print(\"Failed to get response from the server, status code:\", response.status_code)\n\nif __name__ == \"__main__\":\n    # Specify the path to your image and the URL of the server\n    import os\n    cwd = os.getcwd()\n    image_path = f\"{cwd}/ny_example_image.jpeg\"\n    server_url = \"http://127.0.0.1:5070/predict\"\n\n    # Send the image to the server\n    send_image_to_server(image_path, server_url)\n\n\n================================================\nFile: examples/endpoints/CLiP/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/CLiP/server.py\n================================================\nimport torch, clip, multiprocessing\nfrom PIL import Image\nfrom io import BytesIO\n\nimport litserve as ls\n\n\nclass CLIPAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n        \n        # Load candidate captions\n        with open('captions.txt', 'r') as file:\n            self.candidate_captions = [line.strip() for line in file.readlines()]\n        \n        # Pre-tokenize and pre-encode text captions\n        with torch.no_grad():\n            self.text_tokens = clip.tokenize(self.candidate_captions).to(self.device)\n            self.text_embeddings = self.model.encode_text(self.text_tokens)\n\n    def decode_request(self, request):\n        # Convert request to input tensor\n        image_bytes = bytes.fromhex(request[\"image_bytes\"])\n        image = Image.open(BytesIO(image_bytes))\n        return self.preprocess(image).unsqueeze(0).to(self.device)\n\n    def predict(self, image_tensor):\n        # Compare the image against the list of captions\n        with torch.no_grad():\n            image_embedding = self.model.encode_image(image_tensor)\n\n            # Calculating similarities with pre-encoded text features\n            similarities = (100.0 * image_embedding @ self.text_embeddings.T).softmax(dim=-1)\n            max_indices = similarities.argmax(dim=-1)\n\n            return [self.candidate_captions[idx] for idx in max_indices.cpu().numpy()]\n\n    def encode_response(self, output):\n        return {\"description\": output[0]}\n\nif __name__ == \"__main__\":\n    api = CLIPAPI()\n    server = ls.LitServer(api, accelerator='auto')\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/GLiNer/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/GLiNer/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/GLiNer/server.py\n================================================\n# Import necessary libraries\nimport torch\nimport litserve as ls\nfrom gliner import GLiNER\n\n\nclass GLiNERAPI(ls.LitAPI):\n    \"\"\"\n    GLiNERAPI is a subclass of ls.LitAPI that provides methods for the GLiNER model for entity recognition.\n\n    Methods:\n        - setup(device): Initializes the model and loads it onto the specified device.\n        - decode_request(request): Extracts the input text and labels from the request.\n        - predict(data): Generates a response based on the input text and labels.\n        - encode_response(output): Encodes the generated response into a dictionary format.\n    \"\"\"\n\n    def setup(self, device):\n        \"\"\"\n        Sets up the GLiNER model for prediction.\n        \"\"\"\n        # Load the GLiNER model from the Hugging Face model hub\n        model_name = \"knowledgator/modern-gliner-bi-large-v1.0\"\n        self.model = GLiNER.from_pretrained(model_name, max_len=2048).to(device)\n\n    def decode_request(self, request):\n        \"\"\"\n        Decodes the input request to extract the input text and labels.\n        \"\"\"\n        # Extract the input text and labels from the request\n        return request[\"text\"], request.get(\"labels\", [])\n\n    def predict(self, data):\n        \"\"\"\n        Generates a prediction based on the provided input text and labels.\n        \"\"\"\n        # Use the model to predict the entities in the text with the given labels\n        text, labels = data\n        return self.model.predict_entities(text, labels, threshold=0.3)\n\n    def encode_response(self, output):\n        \"\"\"\n        Encodes the given results into a dictionary format.\n        \"\"\"\n        # Return the entities in a dictionary format\n        output = \"\\n\".join(\n            [\n                f\"- **{entity['label'].capitalize()}**: {entity['text']}\"\n                for entity in output\n            ]\n        )\n        return {\"entities\": output}\n\n\nif __name__ == \"__main__\":\n    # Create an instance of the GLiNERAPI class and run the server\n    api = GLiNERAPI()\n    server = ls.LitServer(api, track_requests=True)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/Paligemma-2/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/Paligemma-2/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/Paligemma-2/server.py\n================================================\n# Import the required libraries\nimport os\nfrom dotenv import load_dotenv\nimport torch\nfrom transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\nfrom transformers.image_utils import load_image\nimport litserve as ls\n\n\n# Load the Environment Variables from .env file\nload_dotenv()\n\n# Access token for using the model\naccess_token = os.environ.get(\"ACCESS_TOKEN\")\n\n\nclass PaliGemma2API(ls.LitAPI):\n    \"\"\"\n    PaliGemma2API is a subclass of ls.LitAPI that provides an interface to the PaliGemma2 family of models.\n\n    Methods:\n        - setup(device): Initializes the model and processor with the specified device.\n        - decode_request(request): Convert the request payload to model input.\n        - predict(model_inputs): Uses the model to generate a caption for the given input image and language.\n        - encode_response(output): Convert the model output to a response payload.\n    \"\"\"\n\n    def setup(self, device):\n        \"\"\"\n        Sets up the model and processor on the specified device.\n        \"\"\"\n        model_id = \"google/paligemma2-3b-ft-docci-448\"\n        self.device = device\n        self.model = (\n            PaliGemmaForConditionalGeneration.from_pretrained(\n                model_id, torch_dtype=torch.bfloat16, token=access_token\n            )\n            .eval()\n            .to(self.device)\n        )\n        self.processor = PaliGemmaProcessor.from_pretrained(\n            model_id, token=access_token\n        )\n\n    def decode_request(self, request):\n        \"\"\"\n        Convert the request payload to model input.\n        \"\"\"\n        # Extract the image path and language from the request\n        image = load_image(request[\"image_path\"])\n        language = request.get(\"language\", \"en\")\n\n        # Prepare the prompt for the caption generation\n        prompt = f\"<image>caption {language}\"\n\n        # Prepare the model inputs\n        return (\n            self.processor(text=prompt, images=image, return_tensors=\"pt\")\n            .to(torch.bfloat16)\n            .to(self.model.device)\n        )\n\n    def predict(self, model_inputs):\n        \"\"\"\n        Run inference and generate caption based on the provided model inputs.\n        \"\"\"\n        input_len = model_inputs[\"input_ids\"].shape[-1]\n\n        # Generate the response using the model\n        with torch.inference_mode():\n            generation = self.model.generate(\n                **model_inputs, max_new_tokens=100, do_sample=False\n            )\n            generation = generation[0][input_len:]\n            return self.processor.decode(generation, skip_special_tokens=True)\n\n    def encode_response(self, output):\n        \"\"\"\n        Convert the model output to a response payload.\n        \"\"\"\n        return {\"caption\": output}\n\n\nif __name__ == \"__main__\":\n    # Create an instance of the PaliGemma2API class and run the server\n    api = PaliGemma2API()\n    server = ls.LitServer(api, track_requests=True)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/agents-llm/client.py\n================================================\nimport requests\n\nurl = \"http://localhost:5070/v1/chat/completions\"\nheaders = {\n    \"accept\": \"application/json\",\n    \"Content-Type\": \"application/json\"\n}\ndata = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"how is the weather in London?\",\n        }\n    ],\n    \"temperature\": 0.7,\n    \"stream\": False,\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(response.json())\n\n\n================================================\nFile: examples/endpoints/agents-llm/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/agents-llm/server.py\n================================================\nfrom agents.llms import LlamaCppChatCompletion\nfrom agents.tools import get_current_weather, wikipedia_search, google_search\nfrom agents.tool_executor import need_tool_use\nimport litserve as ls\n\nclass AgentAPI(ls.LitAPI):\n    def setup(self, device):\n        self.llm = LlamaCppChatCompletion.from_default_llm(n_ctx=0)\n        self.llm.bind_tools([get_current_weather, google_search, wikipedia_search])\n\n    def predict(self, messages):\n        output = self.llm.chat_completion(messages)\n        if need_tool_use(output):\n            tool_results = self.llm.run_tools(output)\n            updated_messages = messages + tool_results\n            messages = updated_messages + [{\"role\": \"user\", \"content\": \"please answer me, based on the tool results.\"}]\n            output = self.llm.chat_completion(messages)\n        yield output.choices[0].message.content\n\nif __name__ == \"__main__\":\n    api = AgentAPI()\n    server = ls.LitServer(api, spec=ls.OpenAISpec())\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/deepseek-r1/client.py\n================================================\nimport argparse\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://127.0.0.1:5070/v1\",\n    api_key=\"lit\",\n)\n\n\ndef send_prompt_to_api(prompt: str) -> None:\n    SYSTEM_PROMPT = \"You are a helpful assistant. \\n\"\n    stream = client.chat.completions.create(\n        model=\"deepseek-r1\",\n        messages=[\n            {\"role\": \"user\", \"content\": SYSTEM_PROMPT + prompt},\n        ],\n        max_completion_tokens=512,\n        temperature=0.6,  # Usage: https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#usage-recommendations\n        stream=True,\n    )\n    for chunk in stream:\n        print(\n            f\"\\033[92m{chunk.choices[0].delta.content or ''}\\033[0m\", end=\"\", flush=True\n        )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Send a prompt to the Deepseek R1 API server and receive a response.\"\n    )\n    parser.add_argument(\n        \"-p\", \"--prompt\", required=True, help=\"The prompt to send to the API.\"\n    )\n    args = parser.parse_args()\n\n    send_prompt_to_api(args.prompt)\n\n\n================================================\nFile: examples/endpoints/deepseek-r1/model.py\n================================================\nimport re\n\nimport streamlit as st\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://127.0.0.1:8000/v1\",\n    api_key=\"lit\",\n)\n\n\ndef header():\n    st.markdown(\n        \"\"\"\n        <div style='text-align: center; margin-bottom:4'>\n        <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\">\n        </div>\n        \"\"\",\n        unsafe_allow_html=True,\n    )\n\n    st.markdown(\n        \"<h1 style='text-align: center; font-size:1.5rem'>Chat with DeepSeek-R1</h1>\",\n        unsafe_allow_html=True,\n    )\n    st.markdown(\n        \"<div style='text-align: center; margin-bottom:4'>\"\n        \"<p style='font-size:0.9rem'>DeepSeek-R1: A cutting-edge reasoning model leveraging cold-start RL to surpass predecessor limitations, rivaling OpenAI-o1 in math, code, and reasoning, while open-sourcing SOTA distilled variants for community innovation. <br/><a href='https://huggingface.co/deepseek-ai/DeepSeek-R1' target='_blank'>Read more</a></p>\"\n        \"</div>\",\n        unsafe_allow_html=True,\n    )\n\n\ndef initialize_session_state():\n    \"\"\"Initialize session state variables.\"\"\"\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n\ndef format_assistant_content(content):\n    \"\"\"Format assistant content by replacing specific tags.\"\"\"\n    return (\n        content.replace(\"<think>\\n\\n</think>\", \"\")\n        .replace(\"<think>\", \"\")\n        .replace(\"</think>\", \"\")\n    )\n\n\ndef display_chat_history(show_clear_button=False):\n    \"\"\"Display chat messages from session state.\"\"\"\n    for message in st.session_state.messages:\n        role = message[\"role\"]\n        content = message[\"content\"]\n        think_content = \"\"\n        if role == \"assistant\":\n            # extract content between <think> tags\n            pattern = r\"<think>(.*?)</think>\"\n            think_content = re.search(pattern, content, re.DOTALL).group(0)\n            content = content.replace(think_content, \"\")\n            think_content = format_assistant_content(think_content)\n        with st.chat_message(role):\n            if think_content and len(think_content) > 0:\n                with st.expander(\"Thinking complete!\"):\n                    st.markdown(think_content)\n            st.markdown(content)\n\n    if show_clear_button and st.session_state.messages:\n        _, _, right = st.columns(3)\n        right.button(\n            \"Clear Chat History\",\n            on_click=lambda: st.session_state.messages.clear(),\n            type=\"primary\",\n            icon=\":material/delete:\",\n            use_container_width=True,\n        )\n\n\ndef main():\n    # Title section\n    header()\n\n    # Initialize session state\n    initialize_session_state()\n\n    # Chat history container\n    display_chat_history(show_clear_button=False)\n\n    if prompt := st.chat_input(\"Ask something\", key=\"prompt\"):\n        # Display user message in chat message container\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n            # Add user message to chat history\n            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n        # Get response from the assistant\n        with st.chat_message(\"assistant\"):\n            messages = [*st.session_state.messages]\n            stream = client.chat.completions.create(\n                model=\"deepseek-r1\",\n                messages=messages,\n                stream=True,\n                max_completion_tokens=2048\n            )\n\n            # thinking part\n            thinking_content = \"\"\n            with st.status(\"Thinking...\", expanded=True) as status:\n                res = st.empty()\n                for chunk in stream:\n                    content = chunk.choices[0].delta.content or \"\"\n                    thinking_content += content\n\n                    if \"<think>\" in content:\n                        continue\n                    if \"</think>\" in content:\n                        content = content.replace(\"</think>\", \"\")\n                        status.update(\n                            label=\"Thinking complete!\", state=\"complete\", expanded=False\n                        )\n                        break\n                    res.markdown(format_assistant_content(thinking_content))\n\n            # response part\n            res = st.empty()\n            response_content = \"\"\n            for chunk in stream:\n                content = chunk.choices[0].delta.content or \"\"\n                response_content += content\n                res.markdown(response_content)\n\n            st.session_state.messages.append(\n                {\"role\": \"assistant\", \"content\": thinking_content + response_content}\n            )\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n================================================\nFile: examples/endpoints/deepseek-r1/server.py\n================================================\nfrom threading import Thread\n\nimport litserve as ls\nfrom litserve.specs.openai import ChatCompletionRequest\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TextIteratorStreamer,\n)\n\n\nclass DeepSeekR1API(ls.LitAPI):\n    def setup(self, device):\n        self.device = device\n\n        model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n        self.model = AutoModelForCausalLM.from_pretrained(model_id).to(self.device)\n\n        self.streamer = TextIteratorStreamer(\n            self.tokenizer,\n            skip_prompt=True,\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n\n    def decode_request(self, request: ChatCompletionRequest, context: dict):\n        # Update context with generation arguments\n        context[\"generation_args\"] = {\n            \"temperature\": request.temperature or 0.6,\n            \"max_new_tokens\": request.max_completion_tokens or 1024,\n        }\n\n        # Prepare messages for tokenization\n        messages = [\n            message.model_dump(exclude_none=True) for message in request.messages\n        ]\n\n        # Tokenize messages and prepare input IDs\n        inputs = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=True,\n            add_generation_prompt=True,\n            return_tensors=\"pt\",\n            return_dict=True,\n        ).to(self.device)\n\n        # Return model inputs\n        return inputs\n\n    def predict(self, inputs: dict, context: dict):\n        generation_kwargs = dict(\n            **inputs,\n            streamer=self.streamer,\n            eos_token_id=self.tokenizer.eos_token_id,\n            pad_token_id=self.tokenizer.pad_token_id,\n            **context[\"generation_args\"],\n        )\n        # Start generation in a separate thread\n        generation_thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n        generation_thread.start()\n\n        # Yield generated text from the streamer\n        for text in self.streamer:\n            # print(f\"\\033[92m{text or ''}\\033[0m\", end=\"\", flush=True)\n            yield text\n\n        # Ensure the generation thread has finished\n        generation_thread.join()\n\n\nif __name__ == \"__main__\":\n    server = ls.LitServer(DeepSeekR1API(), spec=ls.OpenAISpec())\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/huggingface/client.py\n================================================\nimport requests\n\ndef test_server(text):\n    # API endpoint URL\n    url = \"http://127.0.0.1:5070/predict\"\n    # Request payload\n    payload = {\"text\": text}\n    # POST request to the server\n    response = requests.post(url, json=payload)\n    # Print the response from the server\n    print(response.json())\n\nif __name__ == \"__main__\":\n    sample_text = \"I love machine learning. My experience with LitServe has been amazing!\"\n    test_server(sample_text)\n\n\n================================================\nFile: examples/endpoints/huggingface/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/huggingface/server.py\n================================================\nimport litserve as ls\nfrom transformers import pipeline\n\nclass HuggingFaceLitAPI(ls.LitAPI):\n    def setup(self, device):\n        # Load the model and tokenizer from Hugging Face Hub\n        # For example, using the `distilbert-base-uncased-finetuned-sst-2-english` model for sentiment analysis\n        # You can replace the model name with any model from the Hugging Face Hub\n        model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n        self.pipeline = pipeline(\"text-classification\", model=model_name, device=device)\n\n    def decode_request(self, request):\n        # Extract text from request\n        # This assumes the request payload is of the form: {'text': 'Your input text here'}\n        return request[\"text\"]\n\n    def batch(self, inputs):\n        # return the batched input list\n        return inputs\n\n    def predict(self, texts):\n        # Use the loaded pipeline to perform inference\n        return self.pipeline(texts)\n\n    def unbatch(self, outputs):\n        # unbatch the model output\n        return outputs\n\n    def encode_response(self, output):\n        # Format the output from the model to send as a response\n        # This example sends back the label and score of the prediction\n        return {\"label\": output[\"label\"], \"score\": output[\"score\"]}\n\nif __name__ == \"__main__\":\n    # Create an instance of your API\n    api = HuggingFaceLitAPI()\n    # Start the server, specifying the port\n    server = ls.LitServer(api, accelerator=\"cuda\", max_batch_size=16, workers_per_device=4, batch_timeout=0.01)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/llama3.2-rag-vllm/client.py\n================================================\nimport requests\nimport json\nimport argparse\n\n# Replace this URL with your exposed URL from the API builder.\n# For example:\n# SERVER_URL = 'https://8000-01hxj54gh5yry3bpaw5k8s5t5j.cloudspaces.litng.ai'\nSERVER_URL = 'http://127.0.0.1:5070'\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Send a query to the server.\")\n    parser.add_argument(\"--query\", type=str, required=True, help=\"The query text to send to the server.\")\n        \n    args = parser.parse_args()\n        \n    payload = {\n        \"query\": args.query\n    }\n        \n    try:\n        response = requests.post(f\"{SERVER_URL}/predict\", json=payload)\n        response.raise_for_status()  # Raise an exception for bad status codes\n\n        try:\n            response_data = response.json()\n        except json.decoder.JSONDecodeError:\n            # If the response is not JSON formatted\n            print(f\"Non-JSON response received:\\n{response.text}\")\n            return\n\n        # Check if response_data is a dictionary\n        if isinstance(response_data, dict):\n            # Adjust according to the actual structure of your response\n            result = response_data.get('response', response_data.get('output', response_data))\n        else:\n            # If it's not a dict, just use the response data directly\n            result = response_data\n\n        print(json.dumps(result, indent=2))\n    except requests.exceptions.RequestException as e:\n        print(f\"Error sending request: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n================================================\nFile: examples/endpoints/llama3.2-rag-vllm/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/llama3.2-rag-vllm/server.py\n================================================\nimport litserve as ls\nfrom fastembed import TextEmbedding\n\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nfrom ingestion import ingest_pdfs\nfrom retriever import Retriever\nfrom prompt_template import qa_prompt_tmpl_str\n\n\nclass DocumentChatAPI(ls.LitAPI):\n    def setup(self, device):\n        model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n        self.llm = LLM(model=model_name, max_model_len=8000)\n        embed_model = TextEmbedding(model_name=\"nomic-ai/nomic-embed-text-v1.5\")\n        ingest_pdfs(\"./data\", embed_model)\n        self.retriever = Retriever(embed_model)\n\n    def decode_request(self, request):\n        return request[\"query\"]\n\n    def predict(self, query):\n        context = self.retriever.generate_context(query)\n        prompt = qa_prompt_tmpl_str.format(context=context, query=query)\n\n        messages = [{\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": prompt}\n            ]}]\n\n        sampling_params = SamplingParams(max_tokens=8192, temperature=0.7)\n        outputs = self.llm.chat(messages=messages, sampling_params=sampling_params)\n        return outputs[0].outputs[0].text\n\n    def encode_response(self, output):\n        return {\"output\": output}\n\nif __name__ == \"__main__\":\n    api = DocumentChatAPI()\n    server = ls.LitServer(api)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/llama3.2-vision/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/llama3.2-vision/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/llama3.2-vision/server.py\n================================================\nfrom model import Llama3\nimport litserve as ls\n\nclass Llama3VisionAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model = Llama3(device)\n\n    def decode_request(self, request):\n        return self.model.apply_chat_template(request.messages)\n\n    def predict(self, inputs, context):\n        yield self.model(inputs)\n\n    def encode_response(self, outputs):\n        for output in outputs:\n            yield {\"role\": \"assistant\", \"content\": self.model.decode_tokens(output)}\n\nif __name__ == \"__main__\":\n    api = Llama3VisionAPI()\n    server = ls.LitServer(api, spec=ls.OpenAISpec())\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/miniCPM/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/miniCPM/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/miniCPM/server.py\n================================================\nimport litserve as ls\nimport torch\nfrom io import BytesIO\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nclass MiniCPMAPI(ls.LitAPI):\n    def setup(self, device):\n        # Load the model\n        self.model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\n            attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\n        self.model = self.model.eval().cuda()\n        self.tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n\n\n    def decode_request(self, request):\n        # Extract file from request\n        return (request[\"content\"].file, request[\"prompt\"])\n\n    def predict(self, input):\n        # Pass the image and prompt to the model\n        image = Image.open(input[0]).convert('RGB')\n        msgs = [{'role': 'user', 'content': [image, input[1]]}]\n\n        res = self.model.chat(\n            image=None,\n            msgs=msgs,\n            tokenizer=self.tokenizer\n        )\n        return res\n\n    def encode_response(self, result):\n        return result\n\n# Starting the server\nif __name__ == \"__main__\":\n    api = MiniCPMAPI()\n    server = ls.LitServer(api, timeout=False)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/ocr/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/ocr/model.py\n================================================\nimport litserve as ls\nimport whisper\n\nclass WhisperLitAPI(ls.LitAPI):\n    def setup(self, device):\n        # Load the OpenAI Whisper model. You can specify other models like \"base\", \"small\", etc.\n        self.model = whisper.load_model(\"large\", device='cuda')\n    \n    def decode_request(self, request):\n        # Assuming the request sends the path to the audio file\n        # In a more robust implementation, you would handle audio data directly.\n        return request[\"audio_path\"]\n    \n    def predict(self, audio_path):\n        # Process the audio file and return the transcription result\n        result = self.model.transcribe(audio_path)\n        return result\n    \n    def encode_response(self, output):\n        # Return the transcription text\n        return {\"transcription\": output[\"text\"]}\n\nif __name__ == \"__main__\":\n    api = WhisperLitAPI()\n    server = ls.LitServer(api, accelerator=\"gpu\", timeout=1000, workers_per_device=2)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/ocr/server.py\n================================================\n# Import necessary libraries\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport litserve as ls\n\n\nclass OCR2API(ls.LitAPI):\n    \"\"\"\n    OCR2API is a subclass of ls.LitAPI that provides an interface to the GOT-OCR2.0 model for various OCR tasks.\n\n    Methods:\n        - setup(device): Called once at startup for the task-specific setup.\n        - decode_request(request): Convert the request payload to model input.\n        - predict(inputs): Uses the model to generate OCR text from the input image.\n        - encode_response(output): Convert the model output to a response payload.\n    \"\"\"\n\n    def setup(self, device):\n        \"\"\"\n        Set up the OCR model for the task.\n        \"\"\"\n        # Set up model and specify the device\n        self.device = device\n        self.model = AutoModelForImageTextToText.from_pretrained(\n            \"stepfun-ai/GOT-OCR-2.0-hf\", device_map=self.device\n        )\n        self.processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n\n    def decode_request(self, request):\n        \"\"\"\n        Convert the request payload to model input.\n        \"\"\"\n        # Extract the image path and color from the request\n        image_path = request.get(\"image_path\")\n        color = request.get(\"color\", None)\n\n        # Load the image from the path\n        image = Image.open(image_path)\n\n        # Prepare the model input by processing the image\n        return self.processor(image, return_tensors=\"pt\", color=color).to(self.device)\n\n    def predict(self, inputs):\n        \"\"\"\n        Run inference and get the model output.\n        \"\"\"\n        # Run inference on the image to get the text output\n        with torch.inference_mode():\n            generate_ids = self.model.generate(\n                **inputs,\n                do_sample=False,\n                tokenizer=self.processor.tokenizer,\n                stop_strings=\"<|im_end|>\",\n                max_new_tokens=4096\n            )\n            return self.processor.decode(\n                generate_ids[0, inputs[\"input_ids\"].shape[1] :],\n                skip_special_tokens=True,\n            )\n\n    def encode_response(self, output):\n        \"\"\"\n        Convert the model output to a response payload.\n        \"\"\"\n        # Return the text output in the response\n        return {\"text\": output}\n\n\nif __name__ == \"__main__\":\n    # Create an instance of the OCR2API and run the server\n    api = OCR2API()\n    server = ls.LitServer(api, track_requests=True)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/qwen2.5/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/qwen2.5/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/qwen2.5/server.py\n================================================\nfrom fastapi import HTTPException\nfrom stock_researcher import research_financials, research_news, stock_researcher, load_model\nimport litserve as ls\n\nmodel_id = \"Qwen/Qwen2.5-7B-Instruct\"\n\nclass StockAnalyst(ls.LitAPI):\n    def setup(self, device):\n        # Using a self hosted open-source model with OpenAI API compatible interface\n        self.model = model_id\n\n    def decode_request(self, request: dict):\n        # Query containing the stock name to research\n        return request[\"query\"]\n\n    def predict(self, query: str):\n        try:\n            # 1. Find financial info\n            messages, financials = research_financials(query, self.model)\n            # 2. Research news about stocks\n            tool_calls, tool_final_result = research_news(financials, query, self.model)\n            # 3. Analyze data\n            yield from stock_researcher(tool_final_result, tool_calls, messages, self.model)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"Stock analyst ran into an error\")\n\n    def encode_response(self, response):\n        for chunk in response:\n            yield chunk\n\nif __name__ == \"__main__\":\n    load_model(model_id)\n    api = StockAnalyst()\n    server = ls.LitServer(api, workers_per_device=8, accelerator=\"cpu\", timeout=False, stream=True)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/sentence-transformers/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/sentence-transformers/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/sentence-transformers/server.py\n================================================\nfrom sentence_transformers import SentenceTransformer\nimport litserve as ls\n\nclass EmbeddingAPI(ls.LitAPI):\n    def setup(self, device):\n        self.instruction = \"Represent this sentence for searching relevant passages: \"\n        self.model = SentenceTransformer('BAAI/bge-large-en-v1.5', device=device)\n\n    def decode_request(self, request):\n        return request[\"input\"]\n\n    def predict(self, query):\n        return self.model.encode([self.instruction + query], normalize_embeddings=True)\n\n    def encode_response(self, output):\n        return {\"embedding\": output[0].tolist()}\n\nif __name__ == \"__main__\":\n    api = EmbeddingAPI()\n    server = ls.LitServer(api)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/tensorTorch/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/tensorTorch/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/tensorTorch/server.py\n================================================\nimport litserve as ls\nimport torch\nimport tensorflow as tf\n\n# Define a PyTorch model\nclass PyTorchModel(torch.nn.Module):\n    def forward(self, x):\n        return x * 2\n\n# Define a TensorFlow model\nclass TensorFlowModel(tf.Module):\n    @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n    def __call__(self, x):\n        return x + 3\n\n# (STEP 1) - DEFINE THE API\nclass ComplexLitAPI(ls.LitAPI):\n    def setup(self, device):\n        # Setup is called once at startup. Load multiple models and initialize resources.\n        self.model1 = PyTorchModel().to(device)  # Load PyTorch model\n        self.model2 = TensorFlowModel()  # Load TensorFlow model\n\n    def decode_request(self, request):\n        input_data = request[\"input\"]\n        transformed_data = input_data / 2\n        return transformed_data\n\n    def predict(self, x):\n        # Run inference through both models and combine their outputs.\n        output1 = self.model1(torch.tensor(x)).item()  # Run PyTorch model\n        output2 = self.model2(tf.constant(x)).numpy()  # Run TensorFlow model\n        combined_output = output1 + output2  # Combine outputs from both models\n        return {\"output\": combined_output}\n\n    def encode_response(self, output):\n        # Convert the model output to a response payload.\n        return {\"final_output\": output}\n\n# (STEP 2) - START THE SERVER\nif __name__ == \"__main__\":\n    # Scale with advanced features (batching, GPUs, etc...)\n    server = ls.LitServer(ComplexLitAPI(), accelerator=\"auto\", max_batch_size=2, batch_timeout=0.10)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/whisper/client.py\n================================================\nimport requests\nimport os\nimport argparse\n\ndef transcribe_audio(audio_file_path):\n    url = \"http://127.0.0.1:8000/predict\"\n    response = requests.post(url, json={\"audio_path\": audio_file_path})\n    print(response.json())\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Transcribe audio files\")\n    parser.add_argument(\"--file\", help=\"Filename of the audio file to transcribe\", type=str, default='mlk.mp3')\n    args = parser.parse_args()\n    \n    # call the model with the file\n    audio_path = os.path.join(os.getcwd(), 'audio_samples', args.file)\n    transcribe_audio(audio_path)\n\n\n================================================\nFile: examples/endpoints/whisper/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/whisper/server.py\n================================================\nfrom model import Llama3\nimport litserve as ls\n\nclass Llama3VisionAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model = Llama3(device)\n\n    def decode_request(self, request):\n        return self.model.apply_chat_template(request.messages)\n\n    def predict(self, inputs, context):\n        yield self.model(inputs)\n\n    def encode_response(self, outputs):\n        for output in outputs:\n            yield {\"role\": \"assistant\", \"content\": self.model.decode_tokens(output)}\n\nif __name__ == \"__main__\":\n    api = Llama3VisionAPI()\n    server = ls.LitServer(api, spec=ls.OpenAISpec())\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/xgboost/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/xgboost/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/xgboost/server.py\n================================================\nimport joblib, numpy as np\nimport litserve as ls\n\nclass XGBoostAPI(ls.LitAPI):\n    def setup(self, device):\n        self.model = joblib.load(\"xgboost_model.joblib\")\n\n    def decode_request(self, request):\n        x = np.asarray(request[\"input\"])\n        x = np.expand_dims(x, 0)\n        return x\n\n    def predict(self, x):\n        return self.model.predict(x)\n\n    def encode_response(self, output):\n        return {\"class_idx\": int(output)}\n\nif __name__ == \"__main__\":\n    api = XGBoostAPI()\n    server = ls.LitServer(api)\n    server.run(port=5070)\n\n\n================================================\nFile: examples/endpoints/xtts/client.py\n================================================\nimport base64\nimport requests\nfrom rich import print\n\n\n# encode an image to base64\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\nbase64_image = encode_image(\"image.jpg\")\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": f\"What is this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                },\n            ],\n        }\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.2,\n}\n\nresponse = requests.post(\"http://localhost:5070/v1/chat/completions\", json=payload)\nprint(response.json()[\"choices\"][0])\n\n\n================================================\nFile: examples/endpoints/xtts/model.py\n================================================\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nfrom litserve.specs.openai import ChatMessage\nimport base64, torch\nfrom typing import List\nfrom io import BytesIO\nfrom PIL import Image\n\ndef decode_base64_image(base64_image_str):\n    # Strip the prefix (e.g., 'data:image/jpeg;base64,')\n    base64_data = base64_image_str.split(\",\")[1]\n    image_data = base64.b64decode(base64_data)\n    image = Image.open(BytesIO(image_data))\n    return image\n\n\nclass Llama3:\n    def __init__(self, device):\n        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\n        self.model = MllamaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16,device_map=\"auto\",)\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.device = device\n\n    def apply_chat_template(self, messages: List[ChatMessage]):\n        final_messages = []\n        image = None\n        for message in messages:\n            msg = {}\n            if message.role == \"system\":\n                msg[\"role\"] = \"system\"\n                msg[\"content\"] = message.content\n            elif message.role == \"user\":\n                msg[\"role\"] = \"user\"\n                content = message.content\n                final_content = []\n                if isinstance(content, list):\n                    for i, content in enumerate(content):\n                        if content.type == \"text\":\n                            final_content.append(content.dict())\n                        elif content.type == \"image_url\":\n                            url = content.image_url.url\n                            image = decode_base64_image(url)\n                            final_content.append({\"type\": \"image\"})\n                    msg[\"content\"] = final_content\n                else:\n                    msg[\"content\"] = content\n            elif message.role == \"assistant\":\n                content = message.content\n                msg[\"role\"] = \"assistant\"\n                msg[\"content\"] = content\n            final_messages.append(msg)\n        prompt = self.processor.apply_chat_template(\n            final_messages, tokenize=False, add_generation_prompt=True\n        )\n        return prompt, image\n\n    def __call__(self, inputs):\n        prompt, image = inputs\n        inputs = self.processor(image, prompt, return_tensors=\"pt\").to(self.model.device)\n        generation_args = {\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.2,\n            \"do_sample\": False,\n        }\n\n        generate_ids = self.model.generate(\n            **inputs,\n            **generation_args,\n        )\n        return inputs, generate_ids\n\n    def decode_tokens(self, outputs):\n        inputs, generate_ids = outputs\n        generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1] :]\n        response = self.processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        return response\n\n\n================================================\nFile: examples/endpoints/xtts/server.py\n================================================\nimport io, base64, torch\nimport numpy as np\nimport soundfile as sf\nimport litserve as ls\nfrom TTS.api import TTS\n\nSPEAKER_WAV_FILE = \"/teamspace/studios/this_studio/speakers/obama.wav\"\nLANGUAGE = \"en\"\n\nclass TTSTextToWavLitAPI(ls.LitAPI):\n    def setup(self, device):\n        self.tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n        print('setup complete...')\n\n    def decode_request(self, request):\n        return request[\"text\"]\n\n    def predict(self, text):\n        wav = self.tts.tts(text=text, speaker_wav=SPEAKER_WAV_FILE, language=LANGUAGE)\n        if isinstance(wav, torch.Tensor):\n            wav = wav.numpy()\n\n        # Write the numpy array to an in-memory buffer in WAV format\n        audio_buffer = io.BytesIO()\n        sf.write(audio_buffer, wav, samplerate=22050, format='WAV')\n        audio_buffer.seek(0)\n        audio_data = audio_buffer.getvalue()\n        audio_buffer.close()\n\n        return {\"audio_content\": audio_data}\n\n    def encode_response(self, prediction):\n        audio_content_base64 = base64.b64encode(prediction[\"audio_content\"]).decode(\"utf-8\")\n        return {\"audio_content\": audio_content_base64, \"content_type\": \"audio/wav\"}\n\nif __name__ == \"__main__\":\n    api = TTSTextToWavLitAPI()\n    server = ls.LitServer(api)\n    server.run(port=5070)\n\n\n================================================\nFile: solo_server/__init__.py\n================================================\n# solo_server/__init__.py\n\n\n\n================================================\nFile: solo_server/cli.py\n================================================\nimport typer\nimport requests\nimport json\nfrom solo_server.main import setup\nfrom solo_server.commands import serve, status, stop, test, download_hf as download, models_list as models\n\napp = typer.Typer()\n\n# Register commands\napp.command()(setup)\napp.command()(serve.serve)\napp.command()(status.status)\napp.command()(models.list)\napp.command()(test.test)\napp.command()(stop.stop)\napp.command()(download.download)\n\nif __name__ == \"__main__\":\n    app()\n\n\n\n================================================\nFile: solo_server/finetune_script.py\n================================================\nimport json\nfrom datasets import Dataset\nfrom unsloth import FastLanguageModel, is_bfloat16_supported, standardize_sharegpt\nfrom pathlib import Path\nimport typer\nfrom peft import LoraConfig, TaskType\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport torch\n\ndef run_training(\n    data_path: str,\n    output_dir: str,\n    epochs: int,\n    batch_size: int,\n    learning_rate: float,\n    lora_r: int,\n    lora_alpha: int,\n    lora_dropout: float,\n):\n    \"\"\"Run the finetuning process\"\"\"\n\n    # Check GPU compatibility\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name()\n        compute_capability = torch.cuda.get_device_capability()\n        print(f\"Found GPU: {gpu_name} with compute capability {compute_capability}\")\n        \n        # Use 8-bit quantization for older GPUs\n        use_4bit = compute_capability[0] >= 8  # Use 4-bit only for Ampere (8.0) and newer\n    else:\n        print(\"No GPU found, using CPU mode\")\n        use_4bit = False\n\n    try:\n        print(\"Initializing model and tokenizer...\")\n        # Initialize model with appropriate quantization\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            model_name=\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\",\n            max_seq_length=2048,\n            dtype=None,\n            load_in_4bit=use_4bit,  # Use 4-bit quantization only for compatible GPUs\n            load_in_8bit=not use_4bit,  # Use 8-bit quantization for older GPUs\n        )\n        print(\"Model and tokenizer initialized successfully\")\n\n    except Exception as e:\n        print(f\"Error initializing model: {str(e)}\")\n        raise\n\n    try:\n        print(\"Applying PEFT configuration...\")\n        model = FastLanguageModel.get_peft_model(\n            model, \n            r=lora_r,\n            target_modules=[\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                \"gate_proj\", \"up_proj\", \"down_proj\",\n            ],\n            lora_alpha=lora_alpha,\n            lora_dropout=lora_dropout,\n            use_gradient_checkpointing=\"unsloth\",\n            use_rslora=False,\n            random_state=3407,\n        )\n        print(\"PEFT configuration applied successfully\")\n\n    except Exception as e:\n        print(f\"Error applying PEFT configuration: {str(e)}\")\n        raise\n\n    with open(data_path) as f:\n        raw_data = json.load(f)\n\n    dataset = prepare_dataset(raw_data, tokenizer)\n\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=4,\n        learning_rate=learning_rate,\n        logging_steps=10,\n        fp16=is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        warmup_ratio=0.03,\n        weight_decay=0.01,\n        optim=\"adamw_8bit\",\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        report_to=\"none\",\n    )\n\n    # Initialize SFT trainer with eval_dataset\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_dataset=dataset,\n        dataset_text_field=\"text\",\n        max_seq_length=2048,\n        dataset_num_proc=2,\n        args=training_args,\n        packing=False,\n    )\n\n    # Train\n    trainer.train()\n\n    # Replace the saving code with this:\n    print(\"Saving model...\")\n    try:\n        merged_path = Path(output_dir) / \"merged_model\"\n        print(\"Merging and saving full model...\")\n        model.save_pretrained_merged(\n            merged_path,\n            tokenizer,\n            save_method=\"merged_16bit\",  #\n        )\n        print(f\"âœ“ Saved merged model to {merged_path}\")\n    except Exception as e:\n        print(f\"Warning: Could not save merged model: {e}\")\n        print(\"Continuing with GGUF conversion...\")\n\n    # Save GGUF version\n    try:\n        gguf_path = Path(output_dir) / \"gguf\"\n        gguf_path.mkdir(exist_ok=True)\n        print(\"Converting model to GGUF format...\")\n        \n        # Use the adapter model for GGUF conversion\n        model.save_pretrained_gguf(\n            str(gguf_path / \"model\"),\n            tokenizer,\n            quantization_method=\"q4_k_m\",\n        )\n    except Exception as e:\n        print(f\"Warning: Could not save GGUF model: {e}\")\n\n    print(\"Training and saving completed!\")\n    print(tokenizer._ollama_modelfile)\n    print(tokenizer._ollama_modelfile.read())\n\n\ndef format_instruction(question: str, answer: str) -> str:\n    \"\"\"Format a single Q&A pair into instruction format\"\"\"\n    return f\"\"\"You are a helpful assistant. Based on the following question, provide a relevant answer:\n\n### Question:\n{question}\n\n### Response:\n{answer}\"\"\"\n\ndef prepare_dataset(raw_data: dict, tokenizer):\n    \"\"\"Prepare dataset from raw data\"\"\"\n    formatted_data = []\n    \n    for item in raw_data[\"data\"]:\n        data_dict = json.loads(item[\"data\"])\n        formatted_text = format_instruction(\n            data_dict[\"question\"],\n            data_dict[\"answer\"]\n        )\n        formatted_data.append({\"text\": formatted_text + tokenizer.eos_token})\n    # Create dataset\n    dataset = Dataset.from_list(formatted_data)\n\n    return dataset\n\nif __name__ == \"__main__\":\n    app = typer.Typer()\n    \n    @app.command()\n    def main(\n        data_path: str = typer.Option(..., \"--data-path\", help=\"Path to the JSON data file\"),\n        output_dir: str = typer.Option(..., \"--output-dir\", help=\"Directory to save the model\"),\n        epochs: int = typer.Option(..., \"--epochs\", help=\"Number of training epochs\"),\n        batch_size: int = typer.Option(..., \"--batch-size\", help=\"Training batch size\"),\n        learning_rate: float = typer.Option(..., \"--learning-rate\", help=\"Learning rate\"),\n        lora_r: int = typer.Option(..., \"--lora-r\", help=\"LoRA attention dimension\"),\n        lora_alpha: int = typer.Option(..., \"--lora-alpha\", help=\"LoRA alpha parameter\"),\n        lora_dropout: float = typer.Option(..., \"--lora-dropout\", help=\"LoRA dropout value\"),\n    ):\n        run_training(\n            data_path=data_path,\n            output_dir=output_dir,\n            epochs=epochs,\n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            lora_r=lora_r,\n            lora_alpha=lora_alpha,\n            lora_dropout=lora_dropout,\n        )\n    \n    app() \n\n\n================================================\nFile: solo_server/main.py\n================================================\nimport os\nimport sys\nimport json\nimport typer\nimport subprocess\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Optional, List\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.prompt import Prompt, Confirm\n\nfrom solo_server.config import CONFIG_PATH\nfrom solo_server.utils.hardware import hardware_info\nfrom solo_server.utils.nvidia import is_cuda_toolkit_installed, check_nvidia_toolkit\nfrom solo_server.config.config_loader import get_server_config\nfrom solo_server.utils.docker_utils import start_docker_engine\n\nconsole = Console()\n\nclass Domain(str, Enum):\n    PERSONAL = \"Personal\"\n    EDUCATION = \"Education\"\n    AGRICULTURE = \"Agriculture\"\n    SOFTWARE = \"Software\"\n    HEALTHCARE = \"Healthcare\"\n    FORENSICS = \"Forensics\"\n    ROBOTICS = \"Robotics\"\n    ENTERPRISE = \"Enterprise\"\n    CUSTOM = \"Custom\"\n\nclass Role(str, Enum):\n    STUDENT = \"Student\"\n    TEACHER = \"Teacher\"\n    FARM_MANAGER = \"Farm Manager\"\n    DEVELOPER = \"Full-Stack Developer\"\n    DOCTOR = \"Doctor\"\n    OTHER = \"Other\"\n\nclass ServerType(str, Enum):\n    OLLAMA = \"ollama\"\n    VLLM = \"vllm\"\n    LLAMACPP = \"llama.cpp\"\n\ndef setup():\n    \"\"\"\n    Set up Solo Server environment with interactive prompts and saves configuration to config.json.\n    \"\"\"\n    typer.echo(\"\\nðŸ’¾ Setting up Solo Server...\\n\")\n    \n    # Check system info and display hardware info\n    typer.echo(\"ðŸ” Checking system information...\")\n    cpu_model, cpu_cores, memory_gb, gpu_vendor, gpu_model, gpu_memory, compute_backend, os_name = hardware_info(typer)\n    \n    # GPU Check and Configuration\n    use_gpu = False\n    if gpu_vendor != \"None\":\n        typer.echo(f\"\\nðŸ–¥ï¸  Detected GPU: {gpu_model} ({gpu_vendor})\")\n        \n        # Check for proper GPU drivers/toolkit\n        drivers_installed = False\n        \n        if gpu_vendor == \"NVIDIA\":\n            drivers_installed = is_cuda_toolkit_installed() and check_nvidia_toolkit(os_name)\n            \n            if drivers_installed:\n                typer.echo(\"âœ… NVIDIA GPU drivers and toolkit are correctly installed.\")\n                use_gpu = Confirm.ask(\"Would you like to use GPU for inference?\", default=True)\n            else:\n                typer.echo(\"âŒ NVIDIA GPU drivers or toolkit are not properly installed.\")\n                install_drivers = Confirm.ask(\"Would you like to install the required drivers?\", default=True)\n                \n                if install_drivers:\n                    typer.echo(\"\\nðŸ“¥ Installing NVIDIA CUDA Toolkit...\")\n                    if os_name == \"Windows\":\n                        # Open CUDA download page in browser\n                        typer.echo(\"Opening NVIDIA CUDA Toolkit download page in your browser...\")\n                        subprocess.Popen([\"start\", \"https://developer.nvidia.com/cuda-downloads\"], shell=True)\n                        typer.echo(\"Please follow the installation instructions and run 'solo setup' again after installation.\")\n                    elif os_name == \"Linux\":\n                        typer.echo(\"For Linux, we recommend installing CUDA toolkit using package manager:\")\n                        typer.echo(\"  Ubuntu/Debian: sudo apt install nvidia-cuda-toolkit\")\n                        typer.echo(\"  CentOS/RHEL: sudo yum install cuda\")\n                        typer.echo(\"Please install the appropriate package and run 'solo setup' again.\")\n                    elif os_name == \"Darwin\":\n                        typer.echo(\"macOS with NVIDIA GPUs is not fully supported. Please use CPU inference.\")\n                    \n                    # Exit without completing setup\n                    return\n        \n        elif gpu_vendor == \"AMD\":\n            # Check for AMD drivers (ROCm)\n            try:\n                rocm_check = subprocess.run([\"rocm-smi\"], capture_output=True, text=True)\n                drivers_installed = rocm_check.returncode == 0\n            except FileNotFoundError:\n                drivers_installed = False\n            \n            if drivers_installed:\n                typer.echo(\"âœ… AMD GPU drivers are correctly installed.\")\n                use_gpu = Confirm.ask(\"Would you like to use GPU for inference?\", default=True)\n            else:\n                typer.echo(\"âŒ AMD GPU drivers are not properly installed.\")\n                install_drivers = Confirm.ask(\"Would you like to install the required drivers?\", default=True)\n                \n                if install_drivers:\n                    typer.echo(\"\\nðŸ“¥ Installing AMD ROCm...\")\n                    if os_name == \"Windows\":\n                        typer.echo(\"Opening AMD ROCm download page in your browser...\")\n                        subprocess.Popen([\"start\", \"https://www.amd.com/en/graphics/servers-solutions-rocm\"], shell=True)\n                    elif os_name == \"Linux\":\n                        typer.echo(\"For Linux, please follow AMD's ROCm installation guide:\")\n                        typer.echo(\"  https://docs.amd.com/en/latest/deploy/linux/index.html\")\n                    typer.echo(\"Please follow the installation instructions and run 'solo setup' again after installation.\")\n                    \n                    # Exit without completing setup\n                    return\n        \n        elif gpu_vendor == \"Apple\":\n            typer.echo(\"âœ… Apple Silicon GPU detected. Using built-in Metal acceleration.\")\n            use_gpu = True\n    else:\n        typer.echo(\"\\nâš ï¸  No GPU detected. Using CPU for inference.\")\n    \n    # Domain Selection\n    typer.echo(\"\\nðŸ¢ Choose the domain that best describes your field:\")\n    for i, domain in enumerate(Domain, 1):\n        typer.echo(f\"  {i}. {domain.value}\")\n    \n    domain_choice = int(Prompt.ask(\"Enter the number of your domain\", default=\"1\"))\n    domain = list(Domain)[domain_choice - 1] if 1 <= domain_choice <= len(Domain) else Domain.PERSONAL\n    \n    # If custom domain, ask for specific domain\n    custom_domain = None\n    if domain == Domain.CUSTOM:\n        custom_domain = Prompt.ask(\"Enter your custom domain\")\n    \n    # Role Selection\n    typer.echo(\"\\nðŸ‘¤ What is your role in your domain?\")\n    for i, role in enumerate(Role, 1):\n        typer.echo(f\"  {i}. {role.value}\")\n    \n    role_choice = int(Prompt.ask(\"Enter the number of your role\", default=\"1\"))\n    role = list(Role)[role_choice - 1] if 1 <= role_choice <= len(Role) else Role.OTHER\n    \n    # If other role, ask for specific role\n    custom_role = None\n    if role == Role.OTHER:\n        custom_role = Prompt.ask(\"Enter your specific role\")\n    \n    # Server Selection\n    typer.echo(\"\\nðŸ–¥ï¸  Select a server type:\")\n    for i, server in enumerate(ServerType, 1):\n        if server == ServerType.VLLM:\n            description = \"(Best for high-performance GPU inference)\"\n        elif server == ServerType.OLLAMA:\n            description = \"(Good balance of performance and ease of use)\"\n        else:  # LLAMACPP\n            description = \"(Best for CPU or lower-resource machines)\"\n        typer.echo(f\"  {i}. {server.value} {description}\")\n    \n    server_choice = int(Prompt.ask(\"Enter the number of your preferred server\", default=\"1\"))\n    server = list(ServerType)[server_choice - 1] if 1 <= server_choice <= len(ServerType) else ServerType.OLLAMA\n    \n    # Ask for HuggingFace token for vLLM or llama.cpp setup\n    if server in [ServerType.VLLM, ServerType.LLAMACPP]:\n        typer.echo(\"A HuggingFace token is recommended for downloading gated models.\")\n        \n        # Check for existing token in environment variable\n        hf_token = os.getenv('HUGGING_FACE_TOKEN', '')\n        \n        if not hf_token:  # If not in env, try config file\n            if os.path.exists(CONFIG_PATH):\n                try:\n                    with open(CONFIG_PATH, 'r') as f:\n                        config_data = json.load(f)\n                        hf_token = config_data.get('hugging_face', {}).get('token', '')\n                except (json.JSONDecodeError, FileNotFoundError):\n                    pass\n        \n        if not hf_token:\n            if os_name in [\"Linux\", \"Windows\"]:\n                typer.echo(\"Use Ctrl + Shift + V to paste your token.\")\n            hf_token = typer.prompt(\"Please add your HuggingFace token\", hide_input=True, default=\"\", show_default=False)\n    else:\n        hf_token = \"\"\n    \n    # Save configuration\n    config = {}\n    if os.path.exists(CONFIG_PATH):\n        try:\n            with open(CONFIG_PATH, 'r') as f:\n                config = json.load(f)\n        except json.JSONDecodeError:\n            config = {}\n    \n    # Update config with new values\n    if 'hardware' not in config:\n        config['hardware'] = {}\n    \n    config['hardware'].update({\n        'use_gpu': use_gpu,\n        'cpu_model': cpu_model,\n        'cpu_cores': cpu_cores,\n        'memory_gb': memory_gb,\n        'gpu_vendor': gpu_vendor,\n        'gpu_model': gpu_model,\n        'gpu_memory': gpu_memory,\n        'compute_backend': compute_backend,\n        'os': os_name\n    })\n    \n    if 'user' not in config:\n        config['user'] = {}\n    \n    config['user'].update({\n        'domain': custom_domain if domain == Domain.CUSTOM else domain.value,\n        'role': custom_role if role == Role.OTHER else role.value\n    })\n    \n    if 'server' not in config:\n        config['server'] = {}\n    \n    config['server'].update({\n        'type': server.value\n    })\n    \n    # Save HuggingFace token if provided\n    if hf_token:\n        config['hugging_face'] = {'token': hf_token}\n    \n    # Setup environment based on server type\n    if server == ServerType.OLLAMA or server == ServerType.VLLM:\n        typer.echo(f\"\\nðŸ³ Setting up Docker environment for Solo Server...\")\n        \n        # Check if Docker is installed and running\n        docker_running = False\n        try:\n            # Check if Docker is installed\n            subprocess.run([\"docker\", \"--version\"], check=True, capture_output=True)\n            \n            # Try to get Docker info to check if it's running\n            try:\n                subprocess.run([\"docker\", \"info\"], check=True, capture_output=True)\n                docker_running = True\n                typer.echo(\"âœ… Docker is running.\")\n            except subprocess.CalledProcessError:\n                # Docker is installed but not running - try to start it\n                typer.echo(\"âš ï¸  Docker is installed but not running. Trying to start Docker...\")\n                docker_running = start_docker_engine(os_name)\n                \n                if not docker_running:\n                    typer.echo(\"âŒ Could not start Docker automatically.\")\n                    typer.echo(\"Please start Docker manually and run 'solo setup' again.\")\n                    return\n        except FileNotFoundError:\n            typer.echo(\"âŒ Docker is not installed on your system.\")\n            typer.echo(\"Please install Docker Desktop from https://www.docker.com/products/docker-desktop/\")\n            typer.echo(\"After installation, run 'solo setup' again.\")\n            return\n        \n        # Pull the appropriate Docker image based on server type\n        try:\n            if server == ServerType.OLLAMA:\n                server_config = get_server_config('ollama')\n                image = server_config.get('images', {}).get('default', \"ollama/ollama\")\n                if gpu_vendor == \"AMD\" and use_gpu:\n                    image = server_config.get('images', {}).get('amd', \"ollama/ollama:rocm\")\n                \n                typer.echo(f\"\\nðŸ“¥ Pulling Docker image: {image}\")\n                subprocess.run([\"docker\", \"pull\", image], check=True)\n                \n            elif server == ServerType.VLLM:\n                server_config = get_server_config('vllm')\n                if gpu_vendor == \"NVIDIA\" and use_gpu:\n                    image = server_config.get('images', {}).get('nvidia', \"vllm/vllm-openai:latest\")\n                elif gpu_vendor == \"AMD\" and use_gpu:\n                    image = server_config.get('images', {}).get('amd', \"rocm/vllm\")\n                elif cpu_model == \"Apple\":\n                    image = server_config.get('images', {}).get('apple', \"getsolo/vllm-arm\")\n                else:\n                    image = server_config.get('images', {}).get('cpu', \"getsolo/vllm-cpu\")\n                \n                typer.echo(f\"\\nðŸ“¥ Pulling Docker image: {image}\")\n                subprocess.run([\"docker\", \"pull\", image], check=True)\n        \n        except subprocess.CalledProcessError as e:\n            typer.echo(f\"\\nâŒ Error setting up Docker environment: {e}\", err=True)\n            typer.echo(\"Please check your Docker configuration and run 'solo setup' again.\")\n            return\n        except Exception as e:\n            typer.echo(f\"\\nâŒ An unexpected error occurred: {e}\", err=True)\n            return\n            \n    elif server == ServerType.LLAMACPP:\n        typer.echo(\"\\nâš™ï¸  Setting up environment...\")\n        # For llama.cpp, we don't need Docker, but we need to install the Python package\n        try:\n            import llama_cpp\n        except ImportError:\n            typer.echo(\"ðŸ“¥ Installing server package...\")\n            \n            # Check if the user is using uv for package management\n            is_uv_available = subprocess.run([\"uv\", \"--version\"], check=False, capture_output=True)\n            if is_uv_available.returncode == 0:\n                using_uv = Confirm.ask(\"Are you using uv for Python package management?\", default=False)\n            else:\n                using_uv = False\n            \n            # Save package manager info to config\n            if 'environment' not in config:\n                config['environment'] = {}\n            config['environment']['package_manager'] = 'uv' if using_uv else 'pip'\n            \n            # setup_llama_cpp_server for hardware-specific installation\n            from solo_server.utils.llama_cpp_utils import setup_llama_cpp_server\n\n            if setup_llama_cpp_server(use_gpu, gpu_vendor, os_name, using_uv):\n                typer.echo(\"âœ… Server package installed successfully with hardware optimizations\")\n            else:\n                typer.echo(\"âŒ Failed to install package. Please check your Python environment.\")\n                return\n\n    # Create configuration directory if it doesn't exist\n    os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)\n\n    # Save configuration to file\n    with open(CONFIG_PATH, 'w') as f:\n        json.dump(config, f, indent=4)\n    \n    typer.echo(f\"\\nâœ… Configuration saved to {CONFIG_PATH}\")\n    typer.echo(\"ðŸŽ‰ Solo Server setup completed successfully!\")\n    typer.echo(f\"Use 'solo serve -m model_name' to start serving your model.\")\n\n\n\n================================================\nFile: solo_server/utils.py\n================================================\nimport subprocess\nimport typer\n\ndef handle_error(func):\n    \"\"\"Decorator for handling errors in CLI commands.\"\"\"\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except subprocess.CalledProcessError as e:\n            typer.echo(f\"âŒ Error: {e}\", err=True)\n        except Exception as e:\n            typer.echo(f\"âš ï¸ Unexpected error: {e}\", err=True)\n    return wrapper\n\ndef run_command(command):\n    \"\"\"Executes shell commands safely.\"\"\"\n    try:\n        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        raise typer.Exit(code=1)\n\n\n\n================================================\nFile: solo_server/commands/__init__.py\n================================================\n\n\n\n================================================\nFile: solo_server/commands/benchmark.py\n================================================\nimport time\nfrom typing import List, Tuple\nfrom datetime import datetime\nfrom llama_cpp import Llama\nfrom pydantic import BaseModel, Field, field_validator\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.progress import track\nimport typer\nimport requests\nimport json\n\nconsole = Console()\n\nclass Message(BaseModel):\n    role: str\n    content: str\n\nclass LlamaResponse(BaseModel):\n    model: str\n    created_at: datetime\n    message: Message\n    done: bool\n    total_duration: float\n    load_duration: float = 0.0\n    eval_count: int\n    eval_duration: float\n\ndef load_model(model_path: str) -> Tuple[Llama, float]:\n    console.print(Panel.fit(f\"[cyan]Loading model: {model_path}[/]\", title=\"[bold magenta]Solo Server[/]\"))\n    start_time = time.time()\n    model = Llama(model_path=model_path)\n    load_duration = time.time() - start_time\n    return model, load_duration\n\ndef api_response(model: str, prompt: str, url: str, server_type:str = None) -> dict:\n    payload = {\n        \"model\": model,\n        \"prompt\": prompt,\n    }\n\n    if server_type == \"ollama\":\n        payload[\"model\"] = model.lower()\n        payload[\"stream\"] = False\n    \n    if server_type == \"vllm\":\n        payload = {\n            \"model\": model,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ]\n        }\n        \n    headers = {\"Content-Type\": \"application/json\"}\n    start_time = time.time()\n    try:\n        response = requests.post(url, data=json.dumps(payload), headers=headers)\n        response.raise_for_status()\n        data = response.json()\n        # Add eval_duration if not present\n        if \"eval_duration\" not in data:\n            data[\"eval_duration\"] = time.time() - start_time\n        return data\n    except requests.exceptions.RequestException as e:\n        return {\"error\": str(e)}\n\ndef run_benchmark(server_type: str, model: object, model_name: str, prompt: str, load_duration: float) -> LlamaResponse:\n    content = \"\"\n    if server_type == \"llama.cpp\":\n        start_time = time.time()\n        response = model(prompt, stop=[\"\\n\"], echo=False)\n        eval_duration = time.time() - start_time\n        content = response[\"choices\"][0][\"text\"]\n        print(content)\n    else:\n        url = \"http://localhost:11434/api/generate\" if server_type == \"ollama\" else \"http://localhost:8000/v1/chat/completions\"\n        response = api_response(model_name, prompt, url, server_type)\n\n        if server_type == \"vllm\":\n            if \"choices\" in response and \"message\" in response[\"choices\"][0]:\n                content = response[\"choices\"][0][\"message\"][\"content\"]\n            else:\n                content = response[\"choices\"][0][\"text\"]\n            print(content)\n            eval_duration = response.get(\"eval_duration\", 0.0) \n        else:\n            content = response.get(\"response\", \"\")\n            load_duration = response.get(\"load_duration\", 0.0) * 1e-9  # Convert nanoseconds to seconds\n            eval_duration = response.get(\"eval_duration\", 0.0) * 1e-9  # Convert nanoseconds to seconds\n            \n    message = Message(role=\"assistant\", content=content)\n\n    return LlamaResponse(\n        model=model_name,\n        created_at=datetime.now(),\n        message=message,\n        done=True,\n        load_duration=load_duration,\n        total_duration=load_duration + eval_duration,\n        eval_count=len(content.split()),\n        eval_duration=eval_duration,\n    )\n\ndef inference_stats(model_response: LlamaResponse):\n    # Add checks for zero duration\n    response_ts = 0.0 if model_response.eval_duration == 0 else model_response.eval_count / model_response.eval_duration\n    total_ts = 0.0 if model_response.total_duration == 0 else model_response.eval_count / model_response.total_duration\n\n    console.print(\n        Panel.fit(\n            f\"[bold magenta]{model_response.model}[/]\\n\"\n            f\"[green]Response:[/] {response_ts:.2f} tokens/s\\n\"\n            f\"[blue]Total:[/] {total_ts:.2f} tokens/s\\n\\n\"\n            f\"[yellow]Stats:[/]\\n\"\n            f\" - Response tokens: {model_response.eval_count}\\n\"\n            f\" - Model load time: {model_response.load_duration:.2f}s\\n\"\n            f\" - Response time: {model_response.eval_duration:.2f}s\\n\"\n            f\" - Total time: {model_response.total_duration:.2f}s\",\n            title=\"[bold cyan]Benchmark Results[/]\",\n        )\n    )\n\ndef average_stats(responses: List[LlamaResponse]):\n    if not responses:\n        console.print(\"[red]No stats to average.[/]\")\n        return\n\n    avg_response = LlamaResponse(\n        model=responses[0].model,\n        created_at=datetime.now(),\n        message=Message(role=\"system\", content=f\"Average stats across {len(responses)} runs\"),\n        done=True,\n        total_duration=sum(r.total_duration for r in responses) / len(responses),\n        load_duration=sum(r.load_duration for r in responses) / len(responses),\n        eval_count=sum(r.eval_count for r in responses) // len(responses),\n        eval_duration=sum(r.eval_duration for r in responses) / len(responses),\n    )\n    inference_stats(avg_response)\n\ndef benchmark(\n    server_type: str = typer.Option(None, \"-s\", help=\"Type of server (e.g., ollama, vllm, llama.cpp).\"),\n    model_name: str = typer.Option(None, \"-m\", help=\"Name of the model.\"),\n    prompts: List[str] = typer.Option([\"Why is the sky blue?\", \"Write a report on the financials of Apple Inc.\", \n                                       \"Tell me about San Francisco\"], \"-p\", help=\"List of prompts to use for benchmarking.\"),\n):\n    if not server_type:\n        server_type = typer.prompt(\"Enter server type (ollama, vllm, llama.cpp)\")\n    if not model_name:\n        model_name = typer.prompt(\"Enter model name\")\n\n    console.print(f\"\\n[bold cyan]Starting Solo Server Benchmark for {server_type} with model {model_name}...[/]\")\n\n    model = None\n    load_duration = 0.0\n    if server_type == \"llama.cpp\":\n        model, load_duration = load_model(model_name)\n    responses: List[LlamaResponse] = []\n    for prompt in track(prompts, description=\"[cyan]Running benchmarks...\"):\n        response = run_benchmark(server_type, model, model_name, prompt, load_duration)\n        responses.append(response)\n        inference_stats(response)\n    \n    average_stats(responses)\n\n\n\n================================================\nFile: solo_server/commands/download_hf.py\n================================================\nimport typer\nfrom huggingface_hub import snapshot_download\nfrom rich.console import Console\nimport os\nimport json\nfrom solo_server.config import CONFIG_PATH\nimport subprocess\n\nconsole = Console()\n\ndef download(model: str) -> None:\n    \"\"\"\n    Downloads a Hugging Face model using the huggingface repo id.\n    \"\"\"\n    console.print(f\"ðŸš€ Downloading model: [bold]{model}[/bold]...\")\n    try:\n        model_path = snapshot_download(repo_id=model)\n        console.print(f\"âœ… Model downloaded successfully: [bold]{model_path}[/bold]\")\n    except Exception as e:\n        console.print(f\"âŒ Failed to download model: {e}\", style=\"bold red\")\n    except KeyboardInterrupt:\n        console.print(\"âŒ Download cancelled by user.\", style=\"bold red\")\n\n\n\n================================================\nFile: solo_server/commands/finetune.py\n================================================\nimport typer\nimport requests\nimport json\nfrom typing import Optional\nfrom pathlib import Path\nimport subprocess\nimport os\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\nfrom rich.text import Text\nfrom rich.box import ROUNDED\nfrom solo_server.config import CONFIG_PATH\n\nBASE_URL = \"https://api.starfishdata.ai/v1\"\n\ndef get_starfish_api_key() -> str:\n    \"\"\"Get Starfish API key from environment or config file\"\"\"\n    # First check environment variable\n    api_key = os.getenv('STARFISH_API_KEY', '')\n\n    if not api_key:  # If not in env, try config file\n        if os.path.exists(CONFIG_PATH):\n            with open(CONFIG_PATH, 'r') as f:\n                config = json.load(f)\n                api_key = config.get('starfish', {}).get('api_key', '')\n\n    if not api_key:\n        if os.name in [\"Linux\", \"Windows\"]:\n            typer.echo(\"Use Ctrl + Shift + V to paste your token.\")\n        api_key = typer.prompt(\"Please enter your Starfish API key\")\n        \n        # Save token if provided\n        if api_key:\n            if os.path.exists(CONFIG_PATH):\n                with open(CONFIG_PATH, 'r') as f:\n                    config = json.load(f)\n            else:\n                config = {}\n            \n            config['starfish'] = {'api_key': api_key}\n            with open(CONFIG_PATH, 'w') as f:\n                json.dump(config, f, indent=4)\n\n    return api_key\n\ndef gen(\n    prompt: str,\n    num_records: Optional[int] = typer.Option(100, \"--num-records\", \"-n\", help=\"Number of records to generate\"),\n    model: Optional[str] = typer.Option(\"gpt-4o-mini-2024-07-18\", \"--model\", \"-m\", help=\"Model to use for generation\")\n):\n    \"\"\"\n    Generate synthetic data using StarfishData API.\n\n    Example:\n        solo finetune gen \"Generate customer service conversations about product returns\"\n    \"\"\"\n    api_key = get_starfish_api_key()\n    if not api_key:\n        typer.echo(\"âŒ Starfish API key is required\", err=True)\n        raise typer.Exit(1)\n\n    data = {\n        \"prompt\": prompt,\n        \"numOfRecords\": num_records,\n        \"model\": model\n    }\n\n    headers = {\n        'Content-Type': 'application/json',\n        'x-api-key': api_key\n    }\n\n    try:\n        response = requests.post(\n            f'{BASE_URL}/generateData',\n            headers=headers,\n            data=json.dumps(data)\n        )\n        response.raise_for_status()\n        \n        result = response.json()\n        console = Console()\n        \n        # Create a table\n        table = Table(show_header=False, box=ROUNDED)\n        table.add_column(\"Key\", style=\"cyan\")\n        table.add_column(\"Value\", style=\"green\")\n        \n        table.add_row(\"Job ID\", result.get('jobId'))\n        table.add_row(\"Project ID\", result.get('projectId'))\n        \n        # Create a panel with success message and table\n        content = [\n            Text(\"âœ… Successfully started data generation\", style=\"bold green\"),\n            \"\",  # Empty line\n            Text(\"Available commands:\", style=\"yellow\"),\n            Text(f\"â€¢ Check status:  solo finetune status {result.get('jobId')}\", style=\"blue\"),\n            Text(f\"â€¢ Download data: solo finetune download {result.get('projectId')}\", style=\"blue\")\n        ]\n        \n        panel = Panel(\n            \"\\n\".join(str(item) for item in content),\n            title=\"[bold magenta]Generation Details[/]\",\n            border_style=\"bright_blue\"\n        )\n        console.print(panel)\n    except requests.exceptions.RequestException as e:\n        typer.echo(f\"âŒ Error: {str(e)}\", err=True)\n\ndef status(job_id: str):\n    \"\"\"\n    Check the status of a data generation job.\n\n    Example:\n        solo finetune status \"job-123-456\"\n    \"\"\"\n    api_key = get_starfish_api_key()\n    if not api_key:\n        typer.echo(\"âŒ Starfish API key is required\", err=True)\n        raise typer.Exit(1)\n\n    headers = {\n        'Content-Type': 'application/json',\n        'x-api-key': api_key\n    }\n\n    data = {\n        \"jobId\": job_id\n    }\n\n    try:\n        response = requests.post(\n            f'{BASE_URL}/jobStatus',\n            headers=headers,\n            data=json.dumps(data)\n        )\n        response.raise_for_status()\n        \n        result = response.json()\n        status = result.get('status', 'UNKNOWN')\n        typer.echo(f\"ðŸ“Š Data generation status: {status}\")\n        \n        if status == \"COMPLETE\":\n            typer.echo(f\"âœ… Data generation completed, Now you can download the data\")\n        elif status == \"FAILED\":\n            typer.echo(f\"âŒ Error: {result.get('error')}\")\n    except requests.exceptions.RequestException as e:\n        typer.echo(f\"âŒ Error: {str(e)}\", err=True)\n\ndef download(\n    project_id: str,\n    output: Optional[str] = typer.Option(\"data.json\", \"--output\", \"-o\", help=\"Output file path\")\n):\n    \"\"\"\n    Download generated data for a project.\n\n    Example:\n        solo finetune download \"project-123-456\" --output my_data.json\n    \"\"\"\n    api_key = get_starfish_api_key()\n    if not api_key:\n        typer.echo(\"âŒ Starfish API key is required\", err=True)\n        raise typer.Exit(1)\n\n    headers = {\n        'Content-Type': 'application/json',\n        'x-api-key': api_key\n    }\n\n    data = {\n        \"projectId\": project_id\n    }\n\n    try:\n        response = requests.post(\n            f'{BASE_URL}/data',\n            headers=headers,\n            data=json.dumps(data)\n        )\n        response.raise_for_status()\n        \n        result = response.json()\n        \n        # Save the data to a file\n        with open(output, 'w') as f:\n            json.dump(result, f, indent=2)\n            \n        typer.echo(f\"âœ… Successfully downloaded data to {output}\")\n        typer.echo(f\"ðŸ“Š Number of records: {len(result['data'])}\")  \n    except requests.exceptions.RequestException as e:\n        typer.echo(f\"âŒ Error: {str(e)}\", err=True)\n    except IOError as e:\n        typer.echo(f\"âŒ Error writing to file: {str(e)}\", err=True)\n\ndef run(\n    data_path: str = typer.Argument(..., help=\"Path to the JSON data file\"),\n    output_dir: str = typer.Option(\"./finetuned_model\", \"--output-dir\", \"-o\", help=\"Directory to save the finetuned model\"),\n    batch_size: int = typer.Option(1, \"--batch-size\", \"-b\", help=\"Training batch size\"),\n    epochs: int = typer.Option(2, \"--epochs\", \"-e\", help=\"Number of training epochs\"),\n    learning_rate: float = typer.Option(2e-4, \"--learning-rate\", \"-lr\", help=\"Learning rate\"),\n    lora_r: int = typer.Option(8, \"--lora-r\", help=\"LoRA attention dimension\"),\n    lora_alpha: int = typer.Option(8, \"--lora-alpha\", help=\"LoRA alpha parameter\"),\n    lora_dropout: float = typer.Option(0.02, \"--lora-dropout\", help=\"LoRA dropout value\"),\n    rebuild_image: bool = typer.Option(False, \"--rebuild-image\", help=\"Force rebuild the Docker image\"),\n):\n    \"\"\"\n    Finetune a model on generated data using unsloth with LoRA in a Docker container.\n\n    Example:\n        solo finetune run data.json --output-dir ./my_model --batch-size 8\n    \"\"\"\n    try:\n        # Convert paths to absolute paths\n        data_path = os.path.abspath(data_path)\n        output_dir = os.path.abspath(output_dir)\n        \n        # Ensure output directory exists\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Check if container exists (running or stopped)\n        container_exists = subprocess.run(\n            [\"docker\", \"ps\", \"-aq\", \"-f\", \"name=solo-finetune\"],\n            capture_output=True,\n            text=True\n        ).stdout.strip()\n\n        docker_finetune = \"getsolo/finetune:latest\"\n        if container_exists:\n            # Check if container is running\n            is_running = subprocess.run(\n                [\"docker\", \"ps\", \"-q\", \"-f\", \"name=solo-finetune\"],\n                capture_output=True,\n                text=True\n            ).stdout.strip()\n            \n            if is_running:\n                typer.echo(\"âœ… Finetune is already running\")\n            else:\n                subprocess.run([\"docker\", \"start\", \"solo-finetune\"], check=True)\n        else:\n            # Check if image exists\n            image_exists = subprocess.run(\n                [\"docker\", \"images\", \"-q\", docker_finetune],\n                capture_output=True,\n                text=True\n            ).stdout.strip()\n\n            if not image_exists or rebuild_image:\n                typer.echo(\"ðŸ“¥ Pulling finetune image...\")\n                try:\n                    subprocess.run([\"docker\", \"pull\", docker_finetune], check=True)\n                except subprocess.CalledProcessError as e:\n                    typer.echo(f\"âŒ Error: {str(e)}\", err=True)\n                    raise typer.Exit(1)\n\n        # Prepare arguments for the training script\n        training_args = {\n            \"data_path\": \"/app/data.json\",\n            \"output_dir\": \"/app/output\",\n            \"epochs\": epochs,\n            \"batch_size\": batch_size,\n            \"learning_rate\": learning_rate,\n            \"lora_r\": lora_r,\n            \"lora_alpha\": lora_alpha,\n            \"lora_dropout\": lora_dropout,\n        }\n        \n        # Convert arguments to command line format\n        args_list = []\n        for key, value in training_args.items():\n            args_list.extend([f\"--{key.replace('_', '-')}\", str(value)])\n\n        # Run the finetuning command in the container\n        docker_cmd = [\n            \"docker\", \"run\",\n            \"--name\", \"solo-finetune\",\n            \"--gpus\", \"all\",  # Enable GPU access\n            \"-v\", f\"{data_path}:/app/data.json:ro\",  # Mount data file\n            \"-v\", f\"{output_dir}:/app/output\",  # Mount output directory\n            docker_finetune,\n            \"python\", \"./finetune_script.py\",\n            *args_list\n        ]\n\n        typer.echo(\"ðŸš€ Starting finetuning process...\")\n        subprocess.run(docker_cmd, check=True)\n        \n        typer.echo(\"âœ… Finetuning completed successfully!\")\n        typer.echo(f\"ðŸ“ Model saved to: {output_dir}\")\n        typer.echo(f\"ðŸ“ GGUF Model converted and saved to {os.path.join(output_dir, 'gguf_path')}\")\n\n    except subprocess.CalledProcessError as e:\n        typer.echo(f\"âŒ Error during Docker operation: {str(e)}\", err=True)\n        raise typer.Exit(1)\n    except Exception as e:\n        typer.echo(f\"âŒ Error: {str(e)}\", err=True)\n        raise typer.Exit(1)\n\n\n================================================\nFile: solo_server/commands/models_list.py\n================================================\nimport typer\nimport os\nimport json\nimport subprocess\nimport glob\nimport time\nfrom pathlib import Path\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\nfrom datetime import datetime\n\nfrom solo_server.config import CONFIG_PATH\nfrom solo_server.config.config_loader import get_server_config\n\nconsole = Console()\n\ndef list():\n    \"\"\"\n    List all downloaded models available in HuggingFace cache and Ollama.\n    \"\"\"\n    typer.echo(\"\\nðŸ” Scanning for available models...\")\n    \n    # Initialize tables\n    hf_table = Table(title=\"HuggingFace Models\")\n    hf_table.add_column(\"MODEL\", style=\"cyan\")\n    hf_table.add_column(\"SIZE\", style=\"green\")\n    hf_table.add_column(\"LAST MODIFIED\", style=\"yellow\")\n    \n    ollama_table = Table(title=\"Ollama Models\")\n    ollama_table.add_column(\"NAME\", style=\"cyan\")\n    ollama_table.add_column(\"SIZE\", style=\"green\")\n    ollama_table.add_column(\"MODIFIED\", style=\"yellow\")\n    ollama_table.add_column(\"TAGS\", style=\"magenta\")\n    \n    # Check for HuggingFace models in cache\n    hf_models_found = False\n    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n    \n    if os.path.exists(cache_dir):\n        # Look for models (typically in model directories containing .bin, .gguf, .safetensors files)\n        model_extensions = ['.bin', '.gguf', '.safetensors']\n        \n        # Track models to avoid duplicates\n        processed_models = set()\n        \n        # Scan HuggingFace cache directory\n        for root, dirs, files in os.walk(cache_dir):\n            model_files = []\n            \n            # Find model files with specific extensions\n            for ext in model_extensions:\n                model_files.extend(glob.glob(os.path.join(root, f\"*{ext}\")))\n            \n            if model_files:\n                # Try to extract model name from path\n                model_name = None\n                model_path = Path(root)\n                \n                # Extract model name from models--org--name pattern\n                path_str = str(model_path)\n                if \"models--\" in path_str:\n                    # Use more reliable extraction based on path components\n                    path_parts = path_str.split(os.sep)\n                    for part in path_parts:\n                        if part.startswith(\"models--\"):\n                            # Convert \"models--org--name\" to \"org/name\"\n                            model_parts = part.split(\"--\")\n                            if len(model_parts) >= 3:\n                                model_name = f\"{model_parts[1]}/{model_parts[2]}\"\n                                break\n                \n                # Skip if we couldn't determine model name\n                if not model_name:\n                    continue\n                \n                # Skip if we've already processed this model (avoid duplicates)\n                if model_name in processed_models:\n                    continue\n                \n                processed_models.add(model_name)\n                \n                # Find the largest model file (likely the main model)\n                if model_files:\n                    # Pick the largest relevant file\n                    largest_file = max(model_files, key=os.path.getsize)\n                    size = os.path.getsize(largest_file)\n                    size_str = _format_size(size)\n                    mod_time = os.path.getmtime(largest_file)\n                    mod_time_str = datetime.fromtimestamp(mod_time).strftime(\"%Y-%m-%d %H:%M\")\n                    \n                    # Add to table with only model name, size, and modification date\n                    hf_table.add_row(model_name, size_str, mod_time_str)\n                    hf_models_found = True\n    \n    # Check for Ollama models\n    ollama_models_found = False\n    ollama_container = get_server_config('ollama').get('container_name', 'solo-ollama')\n    \n    try:\n        # Check if Docker is running\n        docker_running = False\n        try:\n            # Capture Docker info and suppress output\n            subprocess.run([\"docker\", \"info\"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n            docker_running = True\n        except (subprocess.CalledProcessError, FileNotFoundError):\n            pass\n        \n        if docker_running:\n            # Check if Ollama container exists (running or stopped)\n            container_exists = subprocess.run(\n                [\"docker\", \"ps\", \"-a\", \"-q\", \"-f\", f\"name={ollama_container}\"],\n                capture_output=True,\n                text=True\n            ).stdout.strip()\n            \n            # Variable to track if we started the container and need to stop it\n            container_started = False\n            \n            if container_exists:\n                # Check if container is running\n                container_running = subprocess.run(\n                    [\"docker\", \"ps\", \"-q\", \"-f\", f\"name={ollama_container}\"],\n                    capture_output=True,\n                    text=True\n                ).stdout.strip()\n                \n                if not container_running:\n                    # Container exists but is not running - start it\n                    try:\n                        # Start the container\n                        subprocess.run(\n                            [\"docker\", \"start\", ollama_container],\n                            check=True,\n                            stdout=subprocess.DEVNULL,  # Suppress stdout\n                            stderr=subprocess.PIPE     # Only capture stderr for errors\n                        )\n                        container_started = True\n                        \n                        # Wait for container to be ready (up to 10 seconds)\n                        max_wait = 10\n                        ready = False\n                        for _ in range(max_wait):\n                            try:\n                                # Try to run a simple command to check if container is ready\n                                subprocess.run(\n                                    [\"docker\", \"exec\", ollama_container, \"ollama\", \"list\"],\n                                    check=True,\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE\n                                )\n                                ready = True\n                                break\n                            except subprocess.CalledProcessError:\n                                # Container not ready yet\n                                time.sleep(1)\n                        \n                        if not ready:\n                            typer.echo(\"âš ï¸  container started but not ready in time\")\n                            if container_started:\n                                subprocess.run([\"docker\", \"stop\", ollama_container], check=False, \n                                               stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n                            return\n                        \n                    except subprocess.CalledProcessError as e:\n                        typer.echo(f\"âŒ Failed to start Ollama container: {e}\", err=True)\n                        return\n                \n                # Now get list of models from Ollama \n                try:\n                    models_output = subprocess.run(\n                        [\"docker\", \"exec\", ollama_container, \"ollama\", \"list\"],\n                        capture_output=True,\n                        text=True,\n                        check=True\n                    ).stdout.strip()\n                    \n                    if models_output:\n                        # Parse the output - typical format is:\n                        # NAME                    ID              SIZE    MODIFIED\n                        # llama3.2                7e0c91e2d847    5.8 GB  6 days ago\n                        lines = models_output.split('\\n')\n                        if len(lines) > 1:  # Skip header\n                            for line in lines[1:]:\n                                parts = line.split()\n                                if len(parts) >= 4:\n                                    # Extract name (first part)\n                                    name = parts[0]\n                                    \n                                    # Extract model_id (second part)\n                                    model_id = parts[1]\n                                    \n                                    # Extract size (considering it may be \"807 MB\" or \"1.1 GB\")\n                                    # Look for size unit (MB, GB, etc.) to identify the size parts\n                                    size_idx = -1\n                                    for i, part in enumerate(parts[2:], 2):\n                                        if part.upper() in ['B', 'KB', 'MB', 'GB', 'TB']:\n                                            size_idx = i\n                                            break\n                                    \n                                    # If size unit found, combine with the value before it\n                                    if size_idx > 2:  # Found size unit after the value\n                                        size = f\"{parts[size_idx-1]} {parts[size_idx]}\"\n                                        # Modified starts after the size parts\n                                        modified = ' '.join(parts[size_idx+1:])\n                                    else:\n                                        # Default fallback if parsing fails\n                                        size = parts[2]\n                                        modified = ' '.join(parts[3:])\n                                    \n                                    # Check for tags\n                                    tags = \"\"\n                                    if \":\" in name:\n                                        name, tags = name.split(\":\", 1)\n                                    ollama_table.add_row(name, size, modified, tags)\n                                    ollama_models_found = True\n                finally:\n                    # Stop the container if we started it\n                    if container_started:\n                        subprocess.run([\"docker\", \"stop\", ollama_container], check=False,\n                                      stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n    except Exception as e:\n        typer.echo(f\"âš ï¸  Error checking Ollama models: {e}\", err=True)\n        # Ensure container is stopped if we started it and an error occurred\n        if docker_running and container_exists and container_started:\n            subprocess.run([\"docker\", \"stop\", ollama_container], check=False,\n                          stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n    \n    # Display results\n    if hf_models_found:\n        console.print(hf_table)\n    else:\n        typer.echo(\"No HuggingFace models found in cache.\")\n    \n    if ollama_models_found:\n        console.print(ollama_table)\n    \n    # Show hints if no models found\n    if not hf_models_found and not ollama_models_found:\n        typer.echo(\"\\nâ„¹ï¸  You can download models using:\")\n        typer.echo(\"  â€¢ solo download -m <huggingface-model-id>  : Download from HuggingFace\")\n        typer.echo(\"  â€¢ solo serve -s ollama -m <model-name>     : Download and serve with Ollama\")\n\ndef _format_size(size_bytes):\n    \"\"\"Format file size in human-readable format\"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n        if size_bytes < 1024 or unit == 'TB':\n            return f\"{size_bytes:.2f} {unit}\"\n        size_bytes /= 1024\n\n\n\n================================================\nFile: solo_server/commands/serve.py\n================================================\nimport typer\nimport os\nimport json\nimport subprocess\nfrom datetime import datetime\n\nfrom enum import Enum\nfrom typing import Optional\nfrom solo_server.config import CONFIG_PATH\nfrom solo_server.config.config_loader import get_server_config\nfrom solo_server.utils.hardware import detect_hardware\nfrom solo_server.utils.server_utils import (start_vllm_server, \n                                            start_ollama_server, \n                                            start_llama_cpp_server, \n                                            is_huggingface_repo, \n                                            pull_model_from_huggingface,\n                                            check_ollama_model_exists,\n                                            pull_ollama_model,\n                                            start_ui)\nfrom solo_server.utils.docker_utils import start_docker_engine\n\nclass ServerType(str, Enum):\n    OLLAMA = \"ollama\"\n    VLLM = \"vllm\"\n    LLAMACPP = \"llama.cpp\"\n\ndef serve(\n    model: Optional[str] = typer.Option(None, \"--model\", \"-m\", help=\"\"\"Model name or path. Can be:\n    - HuggingFace repo ID (e.g., 'meta-llama/Llama-3.2-1B-Instruct')\n    - Ollama model Registry (e.g., 'llama3.2')\n    - Local path to a model file (e.g., '/path/to/model.gguf')\n    If not specified, the default model from configuration will be used.\"\"\"),\n    server: Optional[str] = typer.Option(None, \"--server\", \"-s\", help=\"Server type (ollama, vllm, llama.cpp)\"), \n    port: Optional[int] = typer.Option(None, \"--port\", \"-p\", help=\"Port to run the server on\"),\n    ui: Optional[bool] = typer.Option(True, \"--ui\", help=\"Start the UI for the server\")\n):\n    \"\"\"Start a model server with the specified model.\n    \n    If no server is specified, uses the server type from configuration.\n    To set up your configuration, run 'solo setup' first.\n    \"\"\"\n    \n    # Check if config file exists\n    if not os.path.exists(CONFIG_PATH):\n        typer.echo(\"âŒ Configuration file not found. Please run 'solo setup' first.\", err=True)\n        typer.echo(\"Run 'solo setup' to complete the Solo Server setup and then try again.\")\n        raise typer.Exit(code=1)\n    \n    # Load configuration\n    with open(CONFIG_PATH, 'r') as f:\n        config = json.load(f)\n    \n    # Extract hardware info from config\n    hardware_config = config.get('hardware', {})\n    use_gpu = hardware_config.get('use_gpu', False)\n    cpu_model = hardware_config.get('cpu_model')\n    gpu_vendor = hardware_config.get('gpu_vendor')\n    os_name = hardware_config.get('os')\n    \n    # If hardware info isn't in config, detect it\n    if not cpu_model or not gpu_vendor or not os_name:\n        cpu_model, _, _, gpu_vendor, _, _, _, os_name = detect_hardware()\n    \n    # Only enable GPU if configured and supported\n    gpu_enabled = use_gpu and gpu_vendor in [\"NVIDIA\", \"AMD\", \"Apple Silicon\"]\n    \n    # Use server from config if not specified\n    if not server:\n        server = config.get('server', {}).get('type', ServerType.OLLAMA.value)\n    else:\n        # Normalize server name\n        server = server.lower()\n    \n    # Validate server type\n    if server not in [s.value for s in ServerType]:\n        typer.echo(f\"âŒ Invalid server type: {server}. Choose from: {', '.join([s.value for s in ServerType])}\", err=True)\n        raise typer.Exit(code=1)\n    \n    # Get server configurations from YAML\n    vllm_config = get_server_config('vllm')\n    ollama_config = get_server_config('ollama')\n    llama_cpp_config = get_server_config('llama_cpp')\n    \n    # Set default models based on server type\n    if not model:\n        if server == ServerType.VLLM.value:\n            model = vllm_config.get('default_model', \"meta-llama/Llama-3.2-1B-Instruct\")\n        elif server == ServerType.OLLAMA.value:\n            model = ollama_config.get('default_model', \"llama3.2\")\n        elif server == ServerType.LLAMACPP.value:\n            model = llama_cpp_config.get('default_model', \"bartowski/Llama-3.2-1B-Instruct-GGUF/llama-3.2-1B-Instruct-Q4_K_M.gguf\")\n    \n    if not port:\n        if server == ServerType.VLLM.value:\n            port = vllm_config.get('default_port', 5070)\n        elif server == ServerType.OLLAMA.value:\n            port = ollama_config.get('default_port', 5070)\n        elif server == ServerType.LLAMACPP.value:\n            port = llama_cpp_config.get('default_port', 5070)\n    \n    # Check Docker is installed and running for Docker-based servers\n    if server in [ServerType.VLLM.value, ServerType.OLLAMA.value]:\n        # Check if Docker is installed\n        docker_installed = True\n        try:\n            subprocess.run([\"docker\", \"--version\"], check=True, capture_output=True)\n        except FileNotFoundError:\n            docker_installed = False\n            typer.echo(\"âŒ Docker is not installed on your system.\", err=True)\n            typer.echo(\"Please install Docker Desktop from https://www.docker.com/products/docker-desktop/\")\n            typer.echo(\"After installation, run 'solo setup'.\")\n            raise typer.Exit(code=1)\n        \n        # Check if Docker is running\n        docker_running = False\n        try:\n            subprocess.run([\"docker\", \"info\"], check=True, capture_output=True)\n            docker_running = True\n        except subprocess.CalledProcessError:\n            docker_running = False\n            typer.echo(\"âš ï¸  Docker is installed but not running. Trying to start Docker...\")\n            docker_running = start_docker_engine(os_name)\n            \n            if not docker_running:\n                typer.echo(\"âŒ Could not start Docker automatically.\", err=True)\n                typer.echo(\"Please start Docker manually and run 'solo serve' again.\")\n                raise typer.Exit(code=1)\n    \n    # Start the appropriate server\n    typer.echo(f\"\\nStarting Solo server...\")\n    success = False\n    original_model_name = model\n    server_pretty_name = server.capitalize()\n    \n    if server == ServerType.VLLM.value:\n        try:\n            success = start_vllm_server(gpu_enabled, cpu_model, gpu_vendor, os_name, port, model)\n            # Display container logs command\n            if success:\n                typer.echo(f\"Use 'docker logs -f {vllm_config.get('container_name', 'solo-vllm')}' to view the logs.\")\n        except Exception as e:\n            typer.echo(f\"âŒ Failed to start Solo Server: {e}\", err=True)\n            raise typer.Exit(code=1)\n        \n    elif server == ServerType.OLLAMA.value:\n        # Start Ollama server\n        if not start_ollama_server(gpu_enabled, gpu_vendor, port):\n            typer.echo(\"âŒ Failed to start Solo Server!\", err=True)\n            raise typer.Exit(code=1)\n        \n        # Pull the model if not already available\n        try:\n            # Check if model exists\n            container_name = ollama_config.get('container_name', 'solo-ollama')\n            \n            # Check if this is a HuggingFace model\n            if is_huggingface_repo(model):\n                # Pull from HuggingFace\n                model = pull_model_from_huggingface(container_name, model)\n            else:\n                # Pull or use existing Ollama model\n                model = pull_ollama_model(container_name, model)\n                        \n            success = True\n        except subprocess.CalledProcessError as e:\n            typer.echo(f\"âŒ Failed to pull model: {e}\", err=True)\n            raise typer.Exit(code=1)\n            \n    elif server == ServerType.LLAMACPP.value:\n        # Start llama.cpp server with the specified model\n        success = start_llama_cpp_server(os_name, model_path=model, port=port)\n        if not success:\n            typer.echo(\"âŒ Failed to start Solo server\", err=True)\n            raise typer.Exit(code=1)\n    \n    # Display server information in the requested format\n    if success:\n        # Get formatted model name for display\n        display_model = original_model_name\n        if is_huggingface_repo(original_model_name):\n            # For HF models, get the repository name for display\n            display_model = original_model_name.split('/')[-1] if '/' in original_model_name else original_model_name\n        \n        # Save model information to config file\n        # Update config with active model information\n        config['active_model'] = {\n            'server': server,\n            'name': display_model,\n            'full_model_name': original_model_name,  # Save the complete model name\n            'port': port,  # Save the server port for the UI to use\n            'last_used': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n        \n        # Make sure server section exists\n        if 'server' not in config:\n            config['server'] = {}\n            \n        # Update server type in config\n        config['server']['type'] = server\n        \n        # Save the specific server config with port\n        if server not in config['server']:\n            config['server'][server] = {}\n            \n        config['server'][server]['default_port'] = port\n        \n        # Save updated config\n        with open(CONFIG_PATH, 'w') as f:\n            json.dump(config, f, indent=4)\n        \n        # Print server information\n        typer.secho(\"âœ… Solo Server is running\", fg=typer.colors.BRIGHT_GREEN, bold=True)\n        typer.secho(f\"Model  - {display_model}\", fg=typer.colors.BRIGHT_CYAN, bold=True)\n        typer.secho(f\"Access Server at - http://localhost:{port}\", fg=typer.colors.BRIGHT_CYAN, bold=True)\n        \n        # Get container name based on server type\n        if server == ServerType.VLLM.value:\n            container_name = vllm_config.get('container_name', 'solo-vllm')\n        elif server == ServerType.OLLAMA.value:\n            container_name = ollama_config.get('container_name', 'solo-ollama')\n        else:  # llama.cpp doesn't have a container\n            container_name = None\n            \n        # Start UI container if enabled\n        if ui:\n            typer.echo(\"\\nStarting Solo UI...\")\n            ui_port = 9000  # Default UI port\n            \n            # Start the UI container\n            ui_success = start_ui(server, container_name=container_name)\n            \n            if ui_success:\n                typer.secho(\"âœ… Solo UI is running\", fg=typer.colors.BRIGHT_GREEN, bold=True)\n                typer.secho(f\"Access UI at - http://localhost:{ui_port}\", fg=typer.colors.BRIGHT_CYAN, bold=True)\n            else:\n                typer.echo(\"âš ï¸ Failed to start UI automatically.\")\n                typer.echo(f\"You can manually access the server at http://localhost:{port}\")\n                typer.echo(f\"Or use 'solo test' to test the server.\")\n        else:\n            typer.secho(f\"UI not started. Use 'solo test' to test the server or '--ui' flag to start the UI.\", fg=typer.colors.BRIGHT_MAGENTA)\n\n\n\n================================================\nFile: solo_server/commands/status.py\n================================================\nimport typer\nimport subprocess\nimport os\nimport json\nimport socket\nimport psutil\nfrom pathlib import Path\n\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\n\nfrom solo_server.config import CONFIG_PATH\nfrom solo_server.config.config_loader import get_server_config\nfrom solo_server.utils.llama_cpp_utils import find_process_by_port\n\nconsole = Console()\n\ndef status():\n    \"\"\"Check running models, system status, and configuration.\"\"\"\n    \n    # Check if config file exists\n    if not os.path.exists(CONFIG_PATH):\n        typer.echo(\"âŒ Configuration file not found. Please run 'solo setup' first.\")\n        return\n    \n    # Load configuration\n    with open(CONFIG_PATH, 'r') as f:\n        config = json.load(f)\n    \n    # Display configuration in one consolidated table\n    typer.echo(\"\\nðŸ“Š Solo Server Configuration:\")\n    \n    # Create a single configuration table\n    config_table = Table(title=\"Configuration\")\n    config_table.add_column(\"CATEGORY\", style=\"cyan\")\n    config_table.add_column(\"PROPERTY\", style=\"blue\")\n    config_table.add_column(\"VALUE\", style=\"green\")\n    \n    # Track current category to add spacing\n    current_category = None\n    \n    # Hardware Configuration\n    hardware_config = config.get('hardware', {})\n    if hardware_config:\n        current_category = \"Hardware\"\n        config_table.add_row(\"Hardware\", \"CPU Model\", hardware_config.get('cpu_model', 'Not available'))\n        config_table.add_row(\"Hardware\", \"CPU Cores\", str(hardware_config.get('cpu_cores', 'Not available')))\n        config_table.add_row(\"Hardware\", \"Memory (GB)\", str(hardware_config.get('memory_gb', 'Not available')))\n        config_table.add_row(\"Hardware\", \"GPU Vendor\", hardware_config.get('gpu_vendor', 'None'))\n        config_table.add_row(\"Hardware\", \"GPU Model\", hardware_config.get('gpu_model', 'None'))\n        config_table.add_row(\"Hardware\", \"GPU Memory\", str(hardware_config.get('gpu_memory', 'Not available')))\n        config_table.add_row(\"Hardware\", \"GPU Enabled\", \"Yes\" if hardware_config.get('use_gpu', False) else \"No\")\n        config_table.add_row(\"Hardware\", \"Operating System\", hardware_config.get('os', 'Not available'))\n    \n    # Server Configuration - add empty row for spacing\n    config_table.add_row(\"\", \"\", \"\")\n    current_category = \"Server\"\n    \n    server_type = config.get('server', {}).get('type', 'Not set')\n    server_config = get_server_config(server_type)\n    \n    config_table.add_row(\"Server\", \"Default Server\", server_type)\n    config_table.add_row(\"Server\", \"Default Port\", str(server_config.get('default_port', '5070')))\n    config_table.add_row(\"Server\", \"Default Model\", server_config.get('default_model', 'Not set'))\n    \n    # User Info - add empty row for spacing\n    config_table.add_row(\"\", \"\", \"\")\n    current_category = \"User\"\n    \n    user_config = config.get('user', {})\n    if user_config:\n        config_table.add_row(\"User\", \"Domain\", user_config.get('domain', 'Not set'))\n        config_table.add_row(\"User\", \"Role\", user_config.get('role', 'Not set'))\n    \n    # Remove active model section from config table\n    console.print(config_table)\n    \n    # Check for running services\n    running_services = []\n    \n    # Check for Docker\n    docker_installed = False\n    try:\n        subprocess.run([\"docker\", \"--version\"], capture_output=True, check=True)\n        docker_installed = True\n    except (subprocess.CalledProcessError, FileNotFoundError):\n        pass\n    \n    # Check for running containers if Docker is installed\n    containers = []\n    if docker_installed:\n        try:\n            # Check if docker is running\n            docker_running = False\n            try:\n                subprocess.run([\"docker\", \"ps\"], capture_output=True, check=True)\n                docker_running = True\n            except subprocess.CalledProcessError:\n                pass\n            \n            if docker_running:\n                # Check for running solo containers\n                container_result = subprocess.run([\"docker\", \"ps\", \"-f\", \"name=solo*\", \"--format\", \"{{json .}}\"],\n                                            capture_output=True, text=True, check=True)\n                \n                if container_result.stdout.strip():\n                    for line in container_result.stdout.strip().split('\\n'):\n                        container = json.loads(line)\n                        containers.append({\n                            'name': container['Names'],\n                            'status': container['Status'],\n                            'ports': container['Ports']\n                        })\n        except Exception as e:\n            pass\n    \n    # Check for llama.cpp server\n    try:\n        # Check default port 5070\n        default_port = 5070\n        is_port_used = False\n        \n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                s.settimeout(0.2)\n                result = s.connect_ex(('127.0.0.1', default_port))\n                is_port_used = (result == 0)\n        except:\n            pass\n        \n        if is_port_used:\n            process = find_process_by_port(default_port)\n            if process:\n                cmd_line = \" \".join(process.cmdline()) if hasattr(process, 'cmdline') else \"\"\n                # Check if this is a llama.cpp server\n                if \"llama_cpp.server\" in cmd_line:\n                    # Get model name from command line\n                    model_name = \"Unknown\"\n                    try:\n                        cmd_parts = cmd_line.split()\n                        for i, part in enumerate(cmd_parts):\n                            if part == \"--model\" and i+1 < len(cmd_parts):\n                                model_path = cmd_parts[i+1]\n                                model_name = os.path.basename(model_path)\n                    except:\n                        pass\n                    \n                    # Use model name from config if available\n                    if config.get('active_model', {}).get('server') == 'llama.cpp':\n                        model_name = config.get('active_model', {}).get('name', model_name)\n                    \n                    running_services.append([\n                        \"llama.cpp\",\n                        model_name,\n                        f\"http://localhost:{default_port}\",\n                        \"Running\"\n                    ])\n                # Docker containers would be checked separately\n                elif not any(\"solo-vllm\" in container['name'] or \"solo-ollama\" in container['name'] for container in containers):\n                    # Unknown service on this port\n                    running_services.append([\n                        \"Unknown Service\",\n                        \"N/A\",\n                        f\"http://localhost:{default_port}\",\n                        \"Port in use\"\n                    ])\n    except Exception as e:\n        pass\n    \n    # Check for vLLM and Ollama in containers\n    for container in containers:\n        # vLLM container\n        if \"solo-vllm\" in container['name']:\n            # Extract port from the ports string correctly\n            port = \"5070\"  # Default\n            ports_str = container['ports']\n            \n            # Parse port mapping properly\n            # Format can be like \"0.0.0.0:5070->8000/tcp\"\n            if ports_str and \"->\" in ports_str:\n                try:\n                    # Extract the external port (before ->)\n                    port_part = ports_str.split(\"->\")[0]\n                    if \":\" in port_part:\n                        port = port_part.split(\":\")[1]\n                except:\n                    pass\n            \n            # Get model name from config if available\n            model_name = \"Unknown\"\n            if config.get('active_model', {}).get('server') == 'vllm':\n                model_name = config.get('active_model', {}).get('name', model_name)\n            \n            running_services.append([\n                \"vLLM\",\n                model_name,\n                f\"http://localhost:{port}\",\n                \"Running\"\n            ])\n        \n        # Ollama container\n        elif \"solo-ollama\" in container['name']:\n            # Extract port from the ports string correctly\n            port = \"5070\"  # Default\n            ports_str = container['ports']\n            \n            # Parse port mapping properly\n            # Format can be like \"0.0.0.0:5070->11434/tcp\"\n            if ports_str and \"->\" in ports_str:\n                try:\n                    # Extract the external port (before ->)\n                    port_part = ports_str.split(\"->\")[0]\n                    if \":\" in port_part:\n                        port = port_part.split(\":\")[1]\n                except:\n                    pass\n            \n            # Get model name from config if available\n            model_name = \"Unknown\"\n            if config.get('active_model', {}).get('server') == 'ollama':\n                model_name = config.get('active_model', {}).get('name', model_name)\n            \n            running_services.append([\n                \"Ollama\",\n                model_name,\n                f\"http://localhost:{port}\",\n                \"Running\"\n            ])\n    \n    # Display running services\n    if running_services:\n        typer.echo(\"\\nðŸš€ Running Services:\")\n        services_table = Table(title=\"Running Services\")\n        services_table.add_column(\"SERVICE\", style=\"cyan\")\n        services_table.add_column(\"MODEL\", style=\"magenta\")\n        services_table.add_column(\"URL\", style=\"yellow\")\n        services_table.add_column(\"STATUS\", style=\"green\")\n        for service in running_services:\n            services_table.add_row(*service)\n        console.print(services_table)\n    else:\n        typer.echo(\"\\nâš ï¸  No Solo services running.\")\n\n\n\n================================================\nFile: solo_server/commands/stop.py\n================================================\nimport typer\nimport subprocess\nimport socket\nimport psutil\nfrom rich.console import Console\nimport time\nimport os\nimport signal\n\nfrom solo_server.utils.llama_cpp_utils import find_process_by_port\nfrom solo_server.config.config_loader import get_server_config\n\nconsole = Console()\n\ndef stop(name: str = typer.Option(\"\", help=\"Server type to stop (e.g., 'ollama', 'vllm', 'llama.cpp')\")):\n    \"\"\"\n    Stops Solo Server services. If a server type is specified (e.g., 'ollama', 'vllm', 'llama.cpp'),\n    only that specific service will be stopped. Otherwise, all Solo services will be stopped.\n    \"\"\"\n    typer.echo(\"\\nðŸ” Checking running Solo servers...\")\n    \n    # Track what we found and stopped\n    found_services = []\n    stopped_services = []\n    \n    # Check for Docker-based services (ollama, vllm)\n    try:\n        # Check if docker is running\n        docker_running = False\n        try:\n            subprocess.run([\"docker\", \"info\"], \n                          check=True, \n                          stdout=subprocess.PIPE, \n                          stderr=subprocess.PIPE)\n            docker_running = True\n        except (subprocess.CalledProcessError, FileNotFoundError):\n            pass\n            \n        if docker_running:\n            # Check for running solo containers\n            containers = []\n            container_result = subprocess.run(\n                [\"docker\", \"ps\", \"-f\", \"name=solo*\", \"--format\", \"{{.Names}}\"],\n                check=True,\n                capture_output=True,\n                text=True\n            ).stdout.strip()\n            \n            if container_result:\n                containers = container_result.split('\\n')\n                for container in containers:\n                    if \"vllm\" in container:\n                        found_services.append({\"type\": \"vLLM\", \"id\": container})\n                    elif \"ollama\" in container:\n                        found_services.append({\"type\": \"Ollama\", \"id\": container})\n                    elif \"ui\" in container or container == \"solo-ui\":\n                        found_services.append({\"type\": \"UI\", \"id\": container})\n                    else:\n                        found_services.append({\"type\": \"Unknown Docker container\", \"id\": container})\n    except Exception as e:\n        typer.echo(f\"âš ï¸  Error checking Docker containers: {e}\", err=True)\n    \n    # Check for llama.cpp process running on port 5070 (or other ports)\n    default_ports = [5070]  # Add other ports if needed\n    \n    for port in default_ports:\n        try:\n            # Check if port is in use\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                s.settimeout(0.2)\n                result = s.connect_ex(('127.0.0.1', port))\n                if result == 0:  # Port is in use\n                    process = find_process_by_port(port)\n                    if process:\n                        cmd_line = \" \".join(process.cmdline()) if hasattr(process, 'cmdline') else \"\"\n                        # Check if this is a llama.cpp server\n                        if \"llama_cpp.server\" in cmd_line:\n                            found_services.append({\"type\": \"llama.cpp\", \"id\": process.pid, \"process\": process})\n        except Exception as e:\n            pass\n    \n    # Display what was found\n    if found_services:\n        typer.echo(f\"Found {len(found_services)} running Solo services:\")\n        for service in found_services:\n            if service[\"type\"] == \"llama.cpp\":\n                typer.echo(f\"  â€¢ {service['type']} (PID: {service['id']})\")\n            else:\n                typer.echo(f\"  â€¢ {service['type']} container: {service['id']}\")\n        \n        typer.echo(\"\\nðŸ›‘ Stopping Solo Server...\")\n        \n        # Filter services based on name if provided\n        services_to_stop = []\n        if name:\n            name = name.lower()\n            for service in found_services:\n                if (name == \"llama.cpp\" and service[\"type\"] == \"llama.cpp\") or \\\n                   (name == \"vllm\" and service[\"type\"] == \"vLLM\") or \\\n                   (name == \"ollama\" and service[\"type\"] == \"Ollama\") or \\\n                   (name == \"ui\" and service[\"type\"] == \"UI\") or \\\n                   (name in service[\"id\"].lower()):\n                    services_to_stop.append(service)\n            \n            if not services_to_stop:\n                typer.echo(f\"âŒ No running {name} services found.\")\n                return\n        else:\n            services_to_stop = found_services\n        \n        # Stop services\n        for service in services_to_stop:\n            try:\n                if service[\"type\"] == \"llama.cpp\":\n                    # Stop the process\n                    process = service[\"process\"]\n                    process.terminate()\n                    # Wait briefly to see if it terminates gracefully\n                    try:\n                        process.wait(timeout=5)\n                    except psutil.TimeoutExpired:\n                        # Force kill if it didn't terminate gracefully\n                        process.kill()\n                    \n                    typer.echo(f\"âœ… Stopped {service['type']} (PID: {service['id']})\")\n                    stopped_services.append(service)\n                else:\n                    # Docker container\n                    subprocess.run(\n                        [\"docker\", \"stop\", service[\"id\"]],\n                        check=True,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE\n                    )\n                    typer.echo(f\"âœ… Stopped {service['type']} container: {service['id']}\")\n                    stopped_services.append(service)\n            except Exception as e:\n                typer.echo(f\"âŒ Failed to stop {service['type']}: {e}\", err=True)\n        \n        # Summarize what was stopped\n        if stopped_services:\n            total_stopped = len(stopped_services)\n            typer.echo(f\"âœ… Successfully stopped {total_stopped} Solo service{'s' if total_stopped > 1 else ''}.\")\n        else:\n            typer.echo(\"\\nâš ï¸  No services were stopped due to errors.\")\n    else:\n        typer.echo(\"âœ… No running Solo Server found.\")\n\n\n\n================================================\nFile: solo_server/commands/test.py\n================================================\nimport typer\nimport json\nimport os\nimport requests\nimport time\nfrom enum import Enum\nfrom typing import Optional\nfrom solo_server.config import CONFIG_PATH\nfrom solo_server.config.config_loader import get_server_config\n\nclass ServerType(str, Enum):\n    OLLAMA = \"ollama\"\n    VLLM = \"vllm\"\n    LLAMACPP = \"llama.cpp\"\n\ndef test(\n    timeout: Optional[int] = typer.Option(None, \"--timeout\", \"-t\", help=\"Request timeout in seconds. Default is 30s for vLLM/Llama.cpp and 120s for Ollama.\")\n):\n    \"\"\"\n    Test if the Solo server is running correctly.\n    Performs an inference test to verify server functionality.\n    \"\"\"\n    typer.echo(\"Testing Solo server connection...\")\n    \n    # Check if config file exists\n    if not os.path.exists(CONFIG_PATH):\n        typer.echo(\"âŒ Configuration file not found. Please run 'solo setup' first.\", err=True)\n        return False\n    \n    # Load configuration\n    with open(CONFIG_PATH, 'r') as f:\n        config = json.load(f)\n    \n    # Get active model information\n    active_model = config.get('active_model', {})\n    server_type = active_model.get('server')\n    model_name = active_model.get('name')\n    full_model_name = active_model.get('full_model_name')  # Get the full model name if available\n    \n    # If no active model is set, use the default server type\n    if not server_type:\n        server_type = config.get('server', {}).get('type', ServerType.OLLAMA.value)\n        typer.echo(f\"No active model found, using default server type: {server_type}\")\n    \n    # Get server configurations\n    vllm_config = get_server_config('vllm')\n    ollama_config = get_server_config('ollama')\n    llama_cpp_config = get_server_config('llama_cpp')\n    \n    # Get port for the given server type\n    port = None\n    if server_type == ServerType.VLLM.value:\n        port = vllm_config.get('default_port', 5070)\n    elif server_type == ServerType.OLLAMA.value:\n        port = ollama_config.get('default_port', 5070)\n    elif server_type == ServerType.LLAMACPP.value:\n        port = llama_cpp_config.get('default_port', 5070)\n    else:\n        typer.echo(f\"âŒ Unknown server type: {server_type}\", err=True)\n        return False\n    \n    # Create base URL\n    base_url = f\"http://localhost:{port}\"\n    \n    # Test connection with an actual inference request\n    typer.echo(f\"Checking server at {base_url}...\")\n    \n    # Simple test prompt\n    test_prompt = \"What is machine learning? Keep it very brief.\"\n    \n    try:\n        # Different API endpoints and payload formats for each server type\n        if server_type == ServerType.VLLM.value or server_type == ServerType.LLAMACPP.value:\n            # Use OpenAI-compatible chat completion API\n            endpoint = f\"{base_url}/v1/chat/completions\"\n            # For vLLM, use the full model name when available\n            api_model_name = full_model_name if full_model_name and server_type == ServerType.VLLM.value else model_name\n            payload = {\n                \"model\": api_model_name if api_model_name else \"default\",\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": test_prompt}\n                ],\n                \"max_tokens\": 50,\n                \"temperature\": 0.7\n            }\n            # Use default timeout for these servers\n            request_timeout = 30\n        elif server_type == ServerType.OLLAMA.value:\n            # Use Ollama generate API\n            endpoint = f\"{base_url}/api/generate\"\n            payload = {\n                \"model\": model_name if model_name else \"default\",\n                \"prompt\": test_prompt,\n                \"stream\": False,\n                \"options\": {\n                    \"temperature\": 0.7,\n                    \"num_predict\": 50\n                }\n            }\n            # Increase timeout for Ollama server as it may take longer to respond\n            request_timeout = 120\n        \n        # Use command line timeout if provided\n        if timeout is not None:\n            request_timeout = timeout\n        \n        # Indicate test is in progress\n        with typer.progressbar(length=100, label=\"Testing inference\") as progress:\n            # Update progress to 30%\n            for _ in range(30):\n                progress.update(1)\n                time.sleep(0.01)\n            \n            # Make the API request\n            start_time = time.time()\n            response = requests.post(endpoint, json=payload, timeout=request_timeout)\n            inference_time = time.time() - start_time\n            \n            # Update progress to 100%\n            for _ in range(70):\n                progress.update(1)\n                time.sleep(0.01)\n        \n        # Process response\n        if response.status_code == 200:\n            try:\n                response_json = response.json()\n                \n                # Extract generated text based on server type\n                generated_text = \"\"\n                if server_type == ServerType.VLLM.value or server_type == ServerType.LLAMACPP.value:\n                    if \"choices\" in response_json and len(response_json[\"choices\"]) > 0:\n                        if \"message\" in response_json[\"choices\"][0]:\n                            generated_text = response_json[\"choices\"][0][\"message\"].get(\"content\", \"\")\n                elif server_type == ServerType.OLLAMA.value:\n                    # Ollama can return different response formats\n                    if \"response\" in response_json:\n                        generated_text = response_json.get(\"response\", \"\")\n                    elif \"message\" in response_json:\n                        # Some versions might use this format\n                        generated_text = response_json.get(\"message\", {}).get(\"content\", \"\")\n                    elif \"content\" in response_json:\n                        # Fallback for other possible formats\n                        generated_text = response_json.get(\"content\", \"\")\n                \n                # Display results\n                typer.secho(\"âœ… Server is running and responded to inference request\", fg=typer.colors.BRIGHT_GREEN, bold=True)\n                if model_name:\n                    typer.secho(f\"Model  - {model_name}\", fg=typer.colors.BRIGHT_BLUE)\n                typer.secho(f\"URL    - {base_url}\", fg=typer.colors.BRIGHT_CYAN)\n                typer.secho(f\"Inference time: {inference_time:.2f} seconds\", fg=typer.colors.BRIGHT_CYAN)\n                \n                # Print generated text\n                if generated_text:\n                    typer.echo(\"\\nTest prompt: \" + test_prompt)\n                    typer.echo(\"Response:\")\n                    typer.secho(generated_text.strip(), fg=typer.colors.WHITE)\n                else:\n                    typer.echo(\"\\nServer responded but no generated text was found in the response.\")\n                \n                return True\n            except Exception as e:\n                typer.secho(f\"âœ… Server is running but response parsing failed: {str(e)}\", fg=typer.colors.BRIGHT_YELLOW, err=True)\n                typer.echo(f\"Raw response: {response.text[:200]}...\")\n                return True\n        else:\n            typer.secho(f\"âŒ Server returned status code {response.status_code}\", fg=typer.colors.BRIGHT_RED, err=True)\n            try:\n                error_msg = response.json()\n                typer.echo(f\"Error: {json.dumps(error_msg, indent=2)}\")\n            except:\n                typer.echo(f\"Response: {response.text[:200]}...\")\n            return False\n            \n    except requests.exceptions.ConnectionError:\n        typer.secho(\"âŒ Failed to connect to server. Server is not running.\", fg=typer.colors.BRIGHT_RED, err=True)\n        typer.echo(\"Start the server using 'solo serve' command or wait for the server to start automatically.\")\n        return False\n    except requests.exceptions.Timeout:\n        typer.secho(\"âŒ Inference request timed out. The server might be overloaded or the model is too large.\", fg=typer.colors.BRIGHT_RED, err=True)\n        typer.echo(f\"Try running 'solo test --timeout 240' to increase the timeout or use a smaller model.\")\n        return False\n    except Exception as e:\n        typer.secho(f\"âŒ Error testing server: {str(e)}\", fg=typer.colors.BRIGHT_RED, err=True)\n        return False\n\n\n\n================================================\nFile: solo_server/config/__init__.py\n================================================\nimport os\nimport yaml\nfrom pathlib import Path\nfrom solo_server.config.config_loader import load_config, get_path_config\n\n# Load path configuration from YAML\npath_config = get_path_config()\n\n# Expand user directory in paths\nCONFIG_DIR = os.path.expanduser(path_config.get('config_dir', '~/.solo_server'))\nCONFIG_PATH = os.path.join(CONFIG_DIR, path_config.get('config_file', 'config.json'))\n\nif not os.path.exists(CONFIG_DIR):\n    os.makedirs(CONFIG_DIR)\n\n\n================================================\nFile: solo_server/config/config.yaml\n================================================\n# Server Configuration\nservers:\n  vllm:\n    default_model: \"unsloth/Llama-3.2-1B-Instruct\"\n    default_port: 5070\n    max_model_len: 4096\n    gpu_memory_utilization: 0.95\n    container_name: \"solo-vllm\"\n    images:\n      nvidia: \"vllm/vllm-openai:latest\"\n      amd: \"rocm/vllm\"\n      apple: \"getsolo/vllm-arm\"\n      cpu: \"getsolo/vllm-cpu\"\n  \n  ollama:\n    default_model: \"llama3.2:1b\"\n    default_port: 5070\n    container_name: \"solo-ollama\"\n    images:\n      default: \"ollama/ollama\"\n      amd: \"ollama/ollama:rocm\"\n  \n  llama.cpp:\n    default_model: \"bartowski/Llama-3.2-1B-Instruct-GGUF/llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n    default_port: 5070\n    cmake_args:\n      nvidia: \"-DGGML_CUDA=on\"\n      amd: \"-DGGML_HIPBLAS=on\"\n      apple_silicon: \"-DGGML_METAL=on\"\n\n# Paths\npaths:\n  config_dir: \"~/.solo_server\"\n  config_file: \"config.json\"\n  logs_dir: \"logs\"\n\n# Timeouts\ntimeouts:\n  server_start: 30\n  docker_check: 30 \n\n\n================================================\nFile: solo_server/config/config_loader.py\n================================================\nimport os\nimport yaml\nfrom pathlib import Path\n\ndef get_config_path():\n    \"\"\"Get the path to the config.yaml file.\"\"\"\n    # Get the directory where this file is located\n    current_dir = Path(__file__).parent\n    return os.path.join(current_dir, \"config.yaml\")\n\ndef load_config():\n    \"\"\"Load configuration from YAML file.\"\"\"\n    config_path = get_config_path()\n    with open(config_path, 'r') as file:\n        config = yaml.safe_load(file)\n    return config\n\ndef get_server_config(server_type):\n    \"\"\"Get configuration for a specific server type.\"\"\"\n    config = load_config()\n    return config.get('servers', {}).get(server_type, {})\n\ndef get_path_config():\n    \"\"\"Get path configuration.\"\"\"\n    config = load_config()\n    return config.get('paths', {})\n\ndef get_timeout_config():\n    \"\"\"Get timeout configuration.\"\"\"\n    config = load_config()\n    return config.get('timeouts', {}) \n\n\n================================================\nFile: solo_server/utils/__init__.py\n================================================\n# solo_server/__init__.py\n\n\n\n================================================\nFile: solo_server/utils/docker_utils.py\n================================================\nimport subprocess\nimport time\nimport typer\n\ndef start_docker_engine(os_name):\n    \"\"\"\n    Attempts to start the Docker engine based on the OS.\n    \"\"\"\n    typer.echo(\"Starting the Docker engine...\")\n    try:\n        if os_name == \"Windows\":\n            try:\n                subprocess.run([\"sc\", \"start\", \"docker\"], check=True, capture_output=True)\n            except subprocess.CalledProcessError:\n                typer.echo(\"Docker service is not registered. Trying to start Docker Desktop...\", err=True)\n\n                # Run PowerShell command to get Docker path\n                result = subprocess.run(\n                    [\"powershell\", \"-Command\", \"(Get-Command docker | Select-Object -ExpandProperty Source)\"],\n                    capture_output=True,\n                    text=True\n                )\n\n                docker_path = result.stdout.strip()\n                if \"Docker\" in docker_path:\n                    # Find the second occurrence of 'Docker'\n                    parts = docker_path.split(\"\\\\\")\n                    docker_index = [i for i, part in enumerate(parts) if part.lower() == \"docker\"]\n\n                    if len(docker_index) >= 2:\n                        docker_desktop_path = \"\\\\\".join(parts[:docker_index[1] + 1]) + \"\\\\Docker Desktop.exe\"\n\n                        typer.echo(f\"Starting Docker Desktop from: {docker_desktop_path}\")\n                        subprocess.run([\"powershell\", \"-Command\", f\"Start-Process '{docker_desktop_path}' -Verb RunAs\"], check=True)\n                    else:\n                        typer.echo(\"âŒ Could not determine Docker Desktop path.\", err=True)\n                else:\n                    typer.echo(\"âŒ Docker is not installed or incorrectly configured.\", err=True)\n\n        elif os_name == \"Linux\":\n            try:\n                # First try systemctl for system Docker service\n                subprocess.run([\"sudo\", \"systemctl\", \"start\", \"docker\"], check=True, capture_output=True)\n            except subprocess.CalledProcessError:\n                try:\n                    # If systemctl fails, try starting Docker Desktop\n                    subprocess.run([\"systemctl\", \"--user\", \"start\", \"docker-desktop\"], check=True, capture_output=True)\n                except subprocess.CalledProcessError:\n                    typer.echo(\"âŒ Failed to start Docker. Please start manually\", err=True)\n\n        elif os_name == \"Darwin\":  # macOS\n            subprocess.run([\"open\", \"/Applications/Docker.app\"], check=True, capture_output=True)\n\n        # Wait for Docker to start\n        timeout = 30\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            try:\n                subprocess.run([\"docker\", \"info\"], check=True, capture_output=True)\n                typer.echo(\"âœ… Docker is running\")\n                return True\n            except subprocess.CalledProcessError:\n                time.sleep(5)\n\n        typer.echo(\"âŒ Docker did not start within the timeout period.\", err=True)\n        return False\n\n    except subprocess.CalledProcessError:\n        typer.echo(\"âŒ Failed to start Docker. Please start Docker with admin privileges manually.\", err=True)\n        return False\n\n\n================================================\nFile: solo_server/utils/hardware.py\n================================================\nimport platform\nimport psutil\nimport GPUtil\nimport typer\nimport subprocess\nimport os\nimport json\nfrom typing import Tuple\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom solo_server.config import CONFIG_PATH\n\nconsole = Console()\n\ndef detect_hardware() -> Tuple[str, int, float, str, str, float, str, str]:\n    # OS Info\n    os_name = platform.system()\n    \n    # CPU Info\n    cpu_model = \"Unknown\"\n    cpu_cores = psutil.cpu_count(logical=False)\n    \n    if os_name == \"Windows\":\n        cpu_model = platform.processor()\n    elif os_name == \"Linux\":\n        try:\n            cpu_model = subprocess.check_output(\"lscpu | grep 'Model name'\", shell=True, text=True).split(\":\")[1].strip()\n        except:\n            cpu_model = \"Unknown Linux CPU\"\n    elif os_name == \"Darwin\":\n        try:\n            cpu_model = subprocess.check_output(\"sysctl -n machdep.cpu.brand_string\", shell=True, text=True).strip()\n        except:\n            cpu_model = \"Unknown Mac CPU\"\n    \n    # Memory Info\n    memory_gb = round(psutil.virtual_memory().total / (1024**3), 2)\n    \n    # GPU Info\n    gpu_vendor = \"None\"\n    gpu_model = \"None\"\n    compute_backend = \"CPU\"\n    gpu_memory = 0\n\n    gpus = GPUtil.getGPUs()\n    if gpus:\n        gpu = gpus[0]  # Get first GPU\n        gpu_model = gpu.name\n        gpu_memory = round(gpu.memoryTotal, 2)  # GPU memory in GB\n        if \"NVIDIA\" in gpu_model:\n            gpu_vendor = \"NVIDIA\"\n            compute_backend = \"CUDA\"\n        elif \"AMD\" in gpu_model:\n            gpu_vendor = \"AMD\"\n            compute_backend = \"HIP\"\n        elif \"Intel\" in gpu_model:\n            gpu_vendor = \"Intel\"\n            compute_backend = \"OpenCL\"\n        elif \"Apple Silicon\" in gpu_model:\n            gpu_vendor = \"Apple Silicon\"\n            compute_backend = \"Metal\"\n        else:\n            gpu_vendor = \"Unknown\"\n            compute_backend = \"CPU\"\n\n    return cpu_model, cpu_cores, memory_gb, gpu_vendor, gpu_model, gpu_memory, compute_backend, os_name\n\ndef recommended_server(memory_gb, gpu_vendor, gpu_memory) -> str:\n    \"\"\"\n    Determines the recommended server based on hardware specifications.\n    Returns the recommended server type after displaying the recommendation.\n    \"\"\"\n    # vLLM recommendation criteria\n    if (gpu_vendor in [\"NVIDIA\",\"AMD\",\"Intel\"] and gpu_memory >= 8) and (memory_gb >= 16):\n        typer.echo(f\"\\nâœ¨ vLLM Recommended for your system\")\n        return \"vLLM\"\n    \n    # Ollama recommendation criteria\n    elif (gpu_vendor in [\"NVIDIA\", \"AMD\"] and gpu_memory >= 6) or (memory_gb >= 16):\n        typer.echo(f\"\\nâœ¨ Ollama is recommended for your system\")\n        return \"ollama\"\n    \n    # Llama.cpp recommendation criteria\n    else:\n        typer.echo(\"\\nâœ¨ Llama.cpp is recommended for your system\")\n        return \"llama.cpp\"\n\n\n    \ndef hardware_info(typer):\n\n    cpu_model, cpu_cores, memory_gb, gpu_vendor, gpu_model, gpu_memory, compute_backend, os_name = detect_hardware()\n    panel = Panel.fit(\n        f\"[bold]Operating System:[/] {os_name}\\n\"\n        f\"[bold]CPU:[/] {cpu_model}\\n\"\n        f\"[bold]CPU Cores:[/] {cpu_cores}\\n\"\n        f\"[bold]Memory:[/] {memory_gb}GB\\n\"\n        f\"[bold]GPU:[/] {gpu_vendor}\\n\"\n        f\"[bold]GPU Model:[/] {gpu_model}\\n\"\n        f\"[bold]GPU Memory:[/] {gpu_memory}GB\\n\"\n        f\"[bold]Compute Backend:[/] {compute_backend}\",\n        title=\"[bold cyan]System Information[/]\"\n    )\n    console.print(panel)\n    return cpu_model, cpu_cores, memory_gb, gpu_vendor, gpu_model, gpu_memory, compute_backend, os_name\n\n\n================================================\nFile: solo_server/utils/hf_utils.py\n================================================\nimport re\nimport typer\nfrom huggingface_hub import list_repo_files\n\ndef get_available_models(repo_id: str, suffix: list[str] | str = \".gguf\") -> list:\n    \"\"\"\n    Fetch the list of available models from a Hugging Face repository.\n\n    :param repo_id: The repository ID on Hugging Face (e.g., \"TheBloke/Llama-2-7B-GGUF\")\n    :param suffix: String or list of strings of file extensions to filter (e.g., [\".gguf\", \".bin\"])\n    :return: List of model files in the repository\n    \"\"\"\n    try:\n        files = list_repo_files(repo_id)\n        # Convert single suffix to list for uniform handling\n        suffixes = [suffix] if isinstance(suffix, str) else suffix\n        # Filter for files with specified suffixes\n        model_files = [f for f in files if any(f.endswith(s) for s in suffixes)]\n        return model_files\n     \n    except Exception as e:\n        typer.echo(f\"Error fetching models from {repo_id}: {e}\")\n        return []\n    \n\ndef select_best_model_file(model_files):\n    \"\"\"\n    Select the most appropriate model file based on quantization level.\n    \n    Prioritizes:\n    1. q4_k_M if available\n    2. Any q4 model\n    3. q5_k_M if available\n    4. Any q5 model\n    5. q8_k_M if available\n    6. Any q8 model\n    7. First available model\n    \n    Parameters:\n    model_files (list): List of available model files\n    \n    Returns:\n    str: The selected model file name\n    \"\"\"\n    if not model_files:\n        return None\n    \n    if len(model_files) == 1:\n        return model_files[0]\n    \n    # Define regex patterns for different quantization levels\n    q4_km_pattern = re.compile(r'q4_k_m', re.IGNORECASE)\n    q4_pattern = re.compile(r'q4', re.IGNORECASE)\n    q5_km_pattern = re.compile(r'q5_k_m', re.IGNORECASE)\n    q5_pattern = re.compile(r'q5', re.IGNORECASE)\n    q8_km_pattern = re.compile(r'q8_k_m', re.IGNORECASE)\n    q8_pattern = re.compile(r'q8', re.IGNORECASE)\n    \n    # Check for q4_k_M models\n    q4_km_models = [f for f in model_files if q4_km_pattern.search(f)]\n    if q4_km_models:\n        return q4_km_models[0]\n    \n    # Check for any q4 models\n    q4_models = [f for f in model_files if q4_pattern.search(f)]\n    if q4_models:\n        return q4_models[0]\n    \n    # Check for q5_k_M models\n    q5_km_models = [f for f in model_files if q5_km_pattern.search(f)]\n    if q5_km_models:\n        return q5_km_models[0]\n    \n    # Check for any q5 models\n    q5_models = [f for f in model_files if q5_pattern.search(f)]\n    if q5_models:\n        return q5_models[0]\n    \n    # Check for q8_k_M models\n    q8_km_models = [f for f in model_files if q8_km_pattern.search(f)]\n    if q8_km_models:\n        return q8_km_models[0]\n    \n    # Check for any q8 models\n    q8_models = [f for f in model_files if q8_pattern.search(f)]\n    if q8_models:\n        return q8_models[0]\n    \n    # If no specific quantization found, return the first model\n    return model_files[0]\n\n\n================================================\nFile: solo_server/utils/llama_cpp_utils.py\n================================================\nimport os\nimport sys\nimport json\nimport time\nimport typer\nimport shutil\nimport subprocess\nimport socket\nimport psutil\n\nfrom solo_server.config import CONFIG_PATH\nfrom solo_server.utils.nvidia import is_cuda_toolkit_installed\nfrom solo_server.config.config_loader import get_server_config\nfrom solo_server.utils.hf_utils import get_available_models, select_best_model_file\n\ndef is_uv_available():\n    return shutil.which(\"uv\") is not None\n\ndef is_port_in_use(port: int) -> bool:\n    \"\"\"Check if a port is already in use.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex(('localhost', port)) == 0\n\ndef find_process_by_port(port: int):\n    \"\"\"Find a process using the specified port.\"\"\"\n    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n        try:\n            for conn in proc.connections(kind='inet'):\n                if conn.laddr.port == port:\n                    return proc\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            pass\n    return None\n# No process to stop, so consider it a success\n\ndef preprocess_model_path(model_path: str, hf_token: str = None) -> tuple[str, str]:\n    \"\"\"\n    Preprocess the model path to determine if it's a repo ID or direct GGUF path.\n    Returns tuple of (hf_repo_id, model_pattern).\n    \"\"\"\n    if model_path.endswith('.gguf'):\n        # Direct GGUF file path\n        parts = model_path.split('/')\n        repo_id = '/'.join(parts[:-1]) if '/' in model_path else None\n        return repo_id, parts[-1] if parts else model_path\n    else:\n        os.environ['HUGGING_FACE_TOKEN'] = hf_token\n        model_files = get_available_models(model_path, suffix=\".gguf\")\n        if model_files:\n                # Auto-select best model if there are multiple\n                best_model = select_best_model_file(model_files)\n        # Repo ID format - auto-append quantization pattern\n        return model_path, best_model\n\ndef is_llama_cpp_installed():\n    \"\"\"Check if llama_cpp is installed.\"\"\"\n    try:\n        import importlib.util\n        return importlib.util.find_spec(\"llama_cpp\") is not None\n    except ImportError:\n        return False\n\ndef setup_llama_cpp_server(gpu_enabled: bool, gpu_vendor: str = None, os_name: str = None, use_uv: bool = False):\n    \"\"\"\n    Setup llama_cpp_python server using system config.\n\n    Parameters:\n    gpu_enabled (bool): Whether GPU is enabled.\n    gpu_vendor (str, optional): The GPU vendor (e.g., NVIDIA, AMD, Apple Silicon).\n    os_name (str, optional): The name of the operating system.\n    install_only (bool, optional): If True, only install the library without starting the server.\n    using_uv (bool, optional): If provided, skips the uv confirmation prompt and uses this value.\n    \"\"\"\n    # Load llama.cpp configuration from YAML\n    llama_cpp_config = get_server_config('llama_cpp')\n    # Set CMAKE_ARGS based on hardware and OS\n    cmake_args = []\n    use_gpu_build = False\n    \n    if gpu_enabled:\n        if gpu_vendor == \"NVIDIA\":\n            if is_cuda_toolkit_installed():\n                use_gpu_build = True\n                cmake_args.append(llama_cpp_config.get('cmake_args', {}).get('nvidia', \"-DGGML_CUDA=on\"))\n            else:\n                typer.echo(\"âš ï¸ NVIDIA CUDA Toolkit not properly configured. Will try CPU-only build instead.\", err=True)\n        elif gpu_vendor == \"AMD\":\n            use_gpu_build = True\n            cmake_args.append(llama_cpp_config.get('cmake_args', {}).get('amd', \"-DGGML_HIPBLAS=on\"))\n        elif gpu_vendor == \"Apple Silicon\":\n            use_gpu_build = True\n            cmake_args.append(llama_cpp_config.get('cmake_args', {}).get('apple_silicon', \"-DGGML_METAL=on\"))\n  \n    cmake_args_str = \" \".join(cmake_args)\n\n    try:\n        env = os.environ.copy()\n        if use_gpu_build:\n            env[\"CMAKE_ARGS\"] = cmake_args_str\n            typer.echo(f\"Attempting GPU-accelerated build with: {cmake_args_str}\")\n        else:\n            typer.echo(\"Installing CPU-only version of llama-cpp-python\")\n        \n        typer.echo(f\"Using {'uv' if use_uv else 'pip'} as package manager\")\n        # Install llama-cpp-python using the Python interpreter\n        if use_uv:\n            installer_cmd = [\"uv\", \"pip\", \"install\", \"--no-cache-dir\", \"llama-cpp-python[server]\"]\n        else:\n            installer_cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"llama-cpp-python[server]\"]\n\n        try:\n            subprocess.check_call(installer_cmd, env=env)\n        except subprocess.CalledProcessError as e:\n            if use_gpu_build:\n                typer.echo(\"âŒ GPU-accelerated build failed. Falling back to CPU-only build...\", err=True)\n                # Clear CMAKE_ARGS for CPU-only build\n                env.pop(\"CMAKE_ARGS\", None)\n                subprocess.check_call(installer_cmd, env=env)\n            else:\n                raise e\n    except Exception as e:\n        typer.echo(f\"âŒ Failed to install package: {e}\", err=True)\n        return False\n    return True\n            \n\n\n================================================\nFile: solo_server/utils/nvidia.py\n================================================\nimport subprocess\nimport typer\n\ndef is_cuda_toolkit_installed():\n    \"\"\"Check if CUDA Toolkit is installed by running nvcc command.\"\"\"\n    try:\n        result = subprocess.run([\"nvcc\", \"--version\"], capture_output=True, text=True)\n        if result.returncode == 0:\n            return True\n        else:\n            return False\n    except FileNotFoundError:\n        return False\n                \ndef check_nvidia_toolkit(os_name) -> bool:\n    \"\"\"\n    Checks if NVIDIA toolkit is properly installed based on the operating system.\n    \"\"\"\n    if os_name == \"Linux\":\n        try:\n            result = subprocess.run(\"docker info | grep -i nvidia\", \n                                 shell=True, \n                                 check=True, \n                                 capture_output=True, \n                                 text=True)\n            return bool(result.stdout.strip())\n        except subprocess.CalledProcessError:\n            return False\n    elif os_name == \"Windows\":\n        try:\n            subprocess.run(\"nvidia-smi\", \n                         check=True, \n                         capture_output=True, \n                         text=True)\n            return True\n        except subprocess.CalledProcessError:\n            return False\n    return False\n    \n\ndef install_nvidia_toolkit_linux():\n    \"\"\"\n    Installs the NVIDIA Container Toolkit on Linux (Debian & Ubuntu).\n    \"\"\"\n    typer.echo(\"Configuring the repository\")\n    try:\n        subprocess.run(\"curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\", shell=True, check=True)\n        subprocess.run(\"curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\", shell=True, check=True)\n        subprocess.run(\"sudo apt-get update\", shell=True, check=True)\n\n        typer.echo(\"Installing Nvidia Container Toolkit\")\n        subprocess.run(\"sudo apt-get install -y nvidia-container-toolkit\", shell=True, check=True)\n        subprocess.run(\"sudo nvidia-ctk runtime configure --runtime=docker\", shell=True, check=True)\n        subprocess.run(\"sudo systemctl restart docker\", shell=True, check=True)\n\n        typer.echo(\"NVIDIA Container Toolkit installed successfully on Linux.\")\n    except subprocess.CalledProcessError as e:\n        typer.echo(f\"Failed to install NVIDIA Container Toolkit on Linux. Error: {e}\", err=True)\n\n\ndef install_nvidia_toolkit_windows():\n    \"\"\"\n    Provide a structured step-by-step guide for Windows users to configure\n    their system for NVIDIA GPU support, including driver & CUDA installation.\n    \"\"\"\n    # Step-by-step instructions\n    typer.secho(\"\\n========================================\", fg=typer.colors.CYAN)\n    typer.secho(\" Windows NVIDIA GPU Setup \", fg=typer.colors.CYAN, bold=True)\n    typer.secho(\"========================================\\n\", fg=typer.colors.CYAN)\n\n    typer.echo(\"Follow these steps to enable NVIDIA GPU support on Windows:\\n\")\n\n    steps = [\n        (\"Step 1: Install or Update NVIDIA Drivers\", \"https://www.nvidia.com/Download/index.aspx\"),\n        (\"Step 2: Install the NVIDIA CUDA Toolkit\", \"https://developer.nvidia.com/cuda-downloads\")\n    ]\n    for step_num, (step_title, link) in enumerate(steps, start=1):\n        typer.secho(f\"{step_title}\", fg=typer.colors.BRIGHT_GREEN)\n        typer.echo(f\"   Link: {link}\\n\")\n\n    typer.echo(\"Once you've completed the above steps:\")\n    typer.echo(\" - Ensure Docker Desktop is installed and running.\")\n    typer.echo(\" - Enable 'Use the WSL 2 based engine' in Docker Desktop settings.\\n\")\n    \n    typer.secho(\"âš ï¸  Please restart Solo Server after installing the required tools.\", fg=typer.colors.YELLOW)\n    raise typer.Exit(1)\n\n\n\n================================================\nFile: solo_server/utils/server_utils.py\n================================================\nimport os \nimport json\nimport typer\nimport click\nimport sys\nimport time\nimport shutil\nimport subprocess\nfrom solo_server.config import CONFIG_PATH\nfrom solo_server.utils.hf_utils import select_best_model_file\nfrom solo_server.config.config_loader import load_config, get_server_config, get_timeout_config\nfrom solo_server.utils.llama_cpp_utils import (is_port_in_use, \n                                              find_process_by_port,\n                                              preprocess_model_path, \n                                              is_llama_cpp_installed)\n\ndef start_ui(server_type: str, container_name: str = None) -> bool:\n    \"\"\"\n    Start the UI Docker container and connect it to the model server.\n    \n    Args:\n        server_type (str): The server type (ollama, vllm, llama.cpp)\n        container_name (str, optional): The name of the server container\n    \n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    try:\n        # Define the UI container name\n        ui_container_name = \"solo-ui\"\n        ui_port = 9000\n        \n        # Check if the UI container already exists\n        container_exists = subprocess.run(\n            [\"docker\", \"ps\", \"-aq\", \"-f\", f\"name={ui_container_name}\"], \n            capture_output=True, \n            text=True\n        ).stdout.strip()\n        \n        # Stop and remove existing container if it's running\n        if container_exists:\n            subprocess.run([\"docker\", \"stop\", ui_container_name], check=False, capture_output=True)\n            subprocess.run([\"docker\", \"rm\", ui_container_name], check=False, capture_output=True)\n        \n        # Check if port is available\n        if is_port_in_use(ui_port):\n            typer.echo(f\"âš ï¸ Port {ui_port} is already in use.\", err=True)\n            return False\n\n        # Get server port from config for connecting to the server\n        server_port = None\n        if server_type == 'ollama':\n            server_config = get_server_config('ollama')\n            server_port = server_config.get('default_port', 11434)\n        elif server_type == 'vllm':\n            server_config = get_server_config('vllm')\n            server_port = server_config.get('default_port', 8000)\n        elif server_type == 'llama.cpp':\n            server_config = get_server_config('llama_cpp')\n            server_port = server_config.get('default_port', 8080)\n\n        # Read config.json to get active model information\n        config_path = os.path.expanduser(\"~/.solo_server/config.json\")\n        if os.path.exists(config_path):\n            with open(config_path, 'r') as f:\n                config = json.load(f)\n                \n            active_model = config.get('active_model', {})\n            # Use the port specified in active_model if available\n            if active_model and 'port' in active_model:\n                server_port = active_model.get('port')\n                \n        typer.echo(f\"Using server port: {server_port}\")\n\n        # Check if solo-network exists, create if not\n        network_exists = subprocess.run(\n            [\"docker\", \"network\", \"inspect\", \"solo-network\"],\n            capture_output=True,\n            text=True\n        ).returncode == 0\n        \n        if not network_exists:\n            subprocess.run([\"docker\", \"network\", \"create\", \"solo-network\"], check=True, capture_output=True)\n            typer.echo(\"Created docker network: solo-network\")\n        \n        # Connect the server container to the solo-network if it exists and container name is provided\n        if container_name and server_type != 'llama.cpp':\n            try:\n                # Check if container is connected to network\n                network_connected = subprocess.run(\n                    [\"docker\", \"network\", \"inspect\", \"solo-network\", \"-f\", f\"{{{{.Containers}}}}\"],\n                    capture_output=True, \n                    text=True\n                ).stdout\n                \n                # If not connected, connect it\n                if container_name not in network_connected:\n                    subprocess.run(\n                        [\"docker\", \"network\", \"connect\", \"solo-network\", container_name], \n                        capture_output=True,\n                        text=True\n                    )\n                    typer.echo(f\"Connected Solo server to Solo UI\")\n            except Exception as e:\n                # If connecting fails, we can still try to run the UI\n                typer.echo(f\"Note: Could not connect server to network: {e}\")\n        \n        # Check if aiaio image exists locally\n        image_exists = subprocess.run(\n            [\"docker\", \"images\", \"-q\", \"getsolo/aiaio\"], \n            capture_output=True, \n            text=True\n        ).stdout.strip()\n        \n        # If image doesn't exist locally, pull it\n        if not image_exists:\n            typer.echo(\"Setting up...\")\n            try:\n                # Try to pull the image from Docker Hub\n                subprocess.run(\n                    [\"docker\", \"pull\", \"getsolo/aiaio:latest\"],\n                    capture_output=True,\n                    text=True,\n                    check=True\n                )\n            except subprocess.CalledProcessError:\n                typer.echo(\"Failed to pull from Docker Hub, checking if image exists locally\")\n                image_exists = subprocess.run(\n                    [\"docker\", \"images\", \"-q\", \"getsolo/aiaio:latest\"], \n                    capture_output=True, \n                    text=True\n                ).stdout.strip()\n                \n                if not image_exists:\n                    typer.echo(f\"âŒ Failed to find UI image\", err=True)\n                    return False\n        \n        # Start the AIAIO UI container\n        run_cmd = [\n            \"docker\", \"run\", \"-d\",\n            \"--name\", ui_container_name,\n            \"-p\", f\"{ui_port}:9000\",  # Map external port to container's internal port\n            \"--network\", \"solo-network\",\n            \"-v\", f\"{os.path.expanduser('~')}/.solo_server:/root/.solo_server\"\n        ]\n        \n        # For llama.cpp which runs on the host, we need to add extra_hosts setting\n        # to allow the container to access the host's IP\n        if server_type == 'llama.cpp':\n            run_cmd.extend([\"--add-host\", \"host.docker.internal:host-gateway\"])\n        \n        # Add environment variables for server configuration\n        run_cmd.extend([\n            \"-e\", f\"SOLO_SERVER_TYPE={server_type}\",\n            \"-e\", f\"SOLO_SERVER_PORT={server_port}\"\n        ])\n        \n        # Add the image name (try both aiaio:latest and aiaio/latest)\n        if image_exists:\n            run_cmd.append(\"getsolo/aiaio:latest\") \n        else:\n            run_cmd.append(\"getsolo/aiaio:latest\")\n        \n        result = subprocess.run(run_cmd, capture_output=True, text=True)\n        \n        if result.returncode != 0:\n            typer.echo(f\"âŒ Failed to start UI: {result.stderr}\", err=True)\n            return False\n        \n        # Wait briefly to ensure container is running\n        time.sleep(2)\n        \n        # Check if container is running\n        is_running = subprocess.run(\n            [\"docker\", \"ps\", \"-q\", \"-f\", f\"name={ui_container_name}\"],\n            capture_output=True,\n            text=True\n        ).stdout.strip()\n        \n        if not is_running:\n            typer.echo(\"âŒ UI container failed to start\", err=True)\n            return False\n            \n        return True\n    \n    except Exception as e:\n        typer.echo(f\"âŒ Error starting UI: {e}\", err=True)\n        return False\n\ndef start_vllm_server(gpu_enabled: bool, cpu: str = None, gpu_vendor: str = None, \n                      os_name:str = None, port: int = None, model: str = None):\n    \"\"\"Setup vLLM server with Docker\"\"\"\n    # Load vLLM configuration from YAML\n    vllm_config = get_server_config('vllm')\n    timeout_config = get_timeout_config()\n    \n    # Use default values from config if not provided\n    port = port or vllm_config.get('default_port', 8000)\n    model = model or vllm_config.get('default_model', \"meta-llama/Llama-3.2-1B-Instruct\")\n    container_name = vllm_config.get('container_name', 'solo-vllm')\n    \n    # Initialize container_exists flag\n    container_exists = False\n    try:\n        # Check if container exists (running or stopped)\n        container_exists = subprocess.run(\n            [\"docker\", \"ps\", \"-aq\", \"-f\", f\"name={container_name}\"], \n            capture_output=True, \n            text=True\n        ).stdout.strip()\n\n        if container_exists:\n            # Check if container is running\n            check_cmd = [\"docker\", \"ps\", \"-q\", \"-f\", f\"name={container_name}\"]\n            is_running = subprocess.run(check_cmd, capture_output=True, text=True).stdout.strip()\n            if is_running:\n                subprocess.run([\"docker\", \"stop\", container_name], check=True, capture_output=True)\n                subprocess.run([\"docker\", \"rm\", container_name], check=True, capture_output=True)\n                container_exists = False\n            else:\n                subprocess.run([\"docker\", \"rm\", container_name], check=True, capture_output=True)\n                container_exists = False\n                   \n        if not container_exists:\n            # Check if port is available\n            import socket\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            try:\n                # Try to bind to the port to check if it's available\n                sock.bind(('127.0.0.1', port))\n                sock.close()\n            except socket.error:\n                typer.echo(f\"âŒ Port {port} is already in use, please try a different port\", err=True)\n                typer.echo(f\"Run 'solo stop' to stop all running servers.\")\n                return False\n            \n            docker_run_cmd = [\n                \"docker\", \"run\", \"-d\",\n                \"--name\", container_name,\n                \"-p\", f\"{port}:8000\",\n                \"--ipc=host\"\n            ]\n\n            # If model is provided, use it directly\n            if model:\n                # Determine if it's a local path or HuggingFace model\n                if os.path.exists(os.path.expanduser(model)):\n                    model_source = \"local\"\n                    model_name = os.path.abspath(os.path.expanduser(model))\n                    \n                    # Add volume mount for local model\n                    local_model_dir = os.path.dirname(model_name)\n                    local_model_dir = local_model_dir.replace('\\\\', '/')\n                    container_model_dir = \"/models\"\n                    model_path = os.path.join(container_model_dir, os.path.basename(model_name)).replace('\\\\', '/')\n                    docker_run_cmd += [\n                        \"-v\", f\"{local_model_dir}:{container_model_dir}\"\n                    ]\n                else:\n                    model_source = \"huggingface\"\n                    model_name = model\n    \n                    # Get HuggingFace token from config file \n                    hf_token = os.getenv('HUGGING_FACE_TOKEN', '')\n                    if not hf_token:  # If not in env, try config file\n                        if os.path.exists(CONFIG_PATH):\n                            with open(CONFIG_PATH, 'r') as f:\n                                config = json.load(f)\n                                hf_token = config.get('hugging_face', {}).get('token', '')\n\n                    # Add volume mount for HuggingFace cache\n                    docker_run_cmd += [ \n                        \"--env\", f\"HUGGING_FACE_HUB_TOKEN={hf_token}\",\n                        \"-v\", f\"{os.path.expanduser('~')}/.cache/huggingface:/root/.cache/huggingface\"\n                    ]\n            \n            # Get appropriate docker image from config\n            if gpu_vendor == \"NVIDIA\" and gpu_enabled:\n                image = vllm_config.get('images', {}).get('nvidia', \"vllm/vllm-openai:latest\")\n                docker_run_cmd += [\"--gpus\", \"all\"]\n            elif gpu_vendor == \"AMD\" and gpu_enabled:\n                image = vllm_config.get('images', {}).get('amd', \"rocm/vllm\")\n                docker_run_cmd += [\n                    \"--network=host\",\n                    \"--group-add=video\", \n                    \"--cap-add=SYS_PTRACE\",\n                    \"--security-opt\", \"seccomp=unconfined\",\n                    \"--device\", \"/dev/kfd\",\n                    \"--device\", \"/dev/dri\"\n                ]\n            elif cpu == \"Apple\":\n                image = vllm_config.get('images', {}).get('apple', \"getsolo/vllm-arm\")\n            elif cpu in [\"Intel\", \"AMD\"]:\n                image = vllm_config.get('images', {}).get('cpu', \"getsolo/vllm-cpu\")\n            else:\n                typer.echo(\"âŒ Solo server vLLM currently do not support your machine\", err=True)\n                return False\n\n            # Check if image exists\n            image_exists = subprocess.run(\n                [\"docker\", \"images\", \"-q\", image],\n                capture_output=True,\n                text=True\n            ).stdout.strip()\n\n            if not image_exists:\n                typer.echo(f\"âŒ Solo server is not setup. Please run 'solo setup' first.\", err=True)\n                return False\n\n            docker_run_cmd.append(image)\n\n            if gpu_vendor == \"NVIDIA\" and gpu_enabled:\n                # Check GPU compute capability\n                gpu_info = subprocess.run(\n                    [\"nvidia-smi\", \"--query-gpu=name,compute_cap\", \"--format=csv\"],\n                    capture_output=True,\n                    text=True\n                ).stdout.strip().split('\\n')[-1]\n                compute_cap = float(gpu_info.split(',')[-1].strip())\n\n            # Add vLLM arguments after the image name\n            if model_source == \"huggingface\":\n                docker_run_cmd += [\"--model\", model_name]\n            else:\n                docker_run_cmd += [\n                    \"--model\", model_path,\n                ]\n\n            # Get max_model_len from config\n            max_model_len = vllm_config.get('max_model_len', 4096)\n            docker_run_cmd += [\"--max_model_len\", str(max_model_len)]\n\n            if gpu_vendor == \"NVIDIA\":\n                # Get GPU memory utilization from config\n                gpu_memory_utilization = vllm_config.get('gpu_memory_utilization', 0.85)\n                docker_run_cmd += [\n                    \"--gpu_memory_utilization\", str(gpu_memory_utilization)\n                ]\n                if 5 < compute_cap < 8:\n                    docker_run_cmd += [\"--dtype\", \"half\"]\n\n            subprocess.run(docker_run_cmd, check=True, capture_output=True)\n            \n            # Check docker logs for any errors\n            try:\n                logs = subprocess.run(\n                    [\"docker\", \"logs\", container_name],\n                    capture_output=True,\n                    text=True,\n                    check=True\n                )\n                if logs.stderr:\n                    typer.echo(f\"âš ï¸ Server logs show errors:\\n{logs.stderr}\", err=True)\n                if logs.stdout:\n                    typer.echo(f\"Server logs:\\n{logs.stdout}\")\n            except subprocess.CalledProcessError as e:\n                typer.echo(f\"âŒ Failed to fetch docker logs: {e}\", err=True)\n\n        # Wait for container to be ready with timeout\n        server_timeout = timeout_config.get('server_start', 30)\n        start_time = time.time()\n        while time.time() - start_time < server_timeout:\n            try:\n                subprocess.run(\n                    [\"docker\", \"exec\", container_name, \"ps\", \"aux\"],\n                    check=True,\n                    capture_output=True,\n                )\n                return True\n            except subprocess.CalledProcessError:\n                time.sleep(1)\n        \n        typer.echo(\"âŒ vLLM server failed to start within timeout\", err=True)\n        return False\n\n    except subprocess.CalledProcessError as e:\n        typer.echo(f\"âŒ Docker command failed: {e}\", err=True)\n        # Cleanup on failure\n        if container_exists:\n            subprocess.run([\"docker\", \"stop\", container_name], check=False)\n        raise typer.Exit(code=1)\n    except Exception as e:\n        typer.echo(f\"âŒ Unexpected error: {e}\", err=True)\n        return False\n\ndef start_ollama_server(gpu_enabled: bool = False, gpu_vendor: str = None, port: int = None):\n    \"\"\"Setup solo-server Ollama environment.\"\"\"\n    # Load Ollama configuration from YAML\n    ollama_config = get_server_config('ollama')\n    timeout_config = get_timeout_config()\n    \n    # Use default values from config if not provided\n    port = port or ollama_config.get('default_port', 11434)\n    container_name = ollama_config.get('container_name', 'solo-ollama')\n    \n    # Initialize container_exists flag\n    container_exists = False\n\n    try:\n        # Check if container exists (running or stopped)\n        container_exists = subprocess.run(\n            [\"docker\", \"ps\", \"-aq\", \"-f\", f\"name={container_name}\"], \n            capture_output=True, \n            text=True\n        ).stdout.strip()\n\n        if container_exists:\n            # Check if container is running\n            check_cmd = [\"docker\", \"ps\", \"-q\", \"-f\", f\"name={container_name}\"]\n            is_running = subprocess.run(check_cmd, capture_output=True, text=True).stdout.strip()\n            if not is_running:\n                subprocess.run([\"docker\", \"rm\", container_name], check=True, capture_output=True)\n                container_exists = False\n            else:\n                subprocess.run([\"docker\", \"stop\", container_name], check=True, capture_output=True)\n                subprocess.run([\"docker\", \"rm\", container_name], check=True, capture_output=True)\n                container_exists = False\n\n        if not container_exists:\n            # port availability check\n            import socket\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            try:\n                # Try to bind to the port to check if it's available\n                sock.bind(('127.0.0.1', port))\n                sock.close()\n            except socket.error:\n                typer.echo(f\"âŒ Port {port} is already in use, please try a different port\", err=True)\n                typer.echo(f\"Run 'solo stop' to stop all running servers.\")\n                return False\n                \n            # Get appropriate docker image from config\n            if gpu_vendor == \"AMD\" and gpu_enabled:\n                image = ollama_config.get('images', {}).get('amd', \"ollama/ollama:rocm\")\n            else:\n                image = ollama_config.get('images', {}).get('default', \"ollama/ollama\")\n\n            # Check if Ollama image exists\n            try:\n                subprocess.run([\"docker\", \"image\", \"inspect\", image], check=True, capture_output=True)\n            except subprocess.CalledProcessError:\n                typer.echo(\"âŒ Solo server is not setup. Please run 'solo setup' first\", err=True)\n                return False\n\n            # Start Ollama container\n            docker_run_cmd = [\"docker\", \"run\", \"-d\", \"--name\", container_name, \"-p\", f\"{port}:11434\"]\n            \n            # Check if local ollama directory exists\n            home_dir = os.path.expanduser(\"~\")\n            local_ollama_dir = os.path.join(home_dir, \".ollama\")\n            \n            if os.path.exists(local_ollama_dir) and os.path.isdir(local_ollama_dir):\n                typer.echo(f\"Found existing Ollama directory at {local_ollama_dir}\")\n                # Use local directory instead of volume\n                docker_run_cmd.extend([\"-v\", f\"{local_ollama_dir}:/root/.ollama\"])\n            else:\n                typer.echo(\"No existing Ollama directory found. Creating a new Docker volume.\")\n                # Use Docker volume for storage\n                docker_run_cmd.extend([\"-v\", \"ollama:/root/.ollama\"])\n            \n            if gpu_vendor == \"NVIDIA\" and gpu_enabled:\n                docker_run_cmd += [\"--gpus\", \"all\"]\n            elif gpu_vendor == \"AMD\" and gpu_enabled:\n                docker_run_cmd += [\"--device\", \"/dev/kfd\", \"--device\", \"/dev/dri\"]\n            \n            docker_run_cmd.append(image)\n            subprocess.run(docker_run_cmd, check=True, capture_output=True)\n\n        # Wait for container to be ready with timeout\n        server_timeout = timeout_config.get('server_start', 30)\n        start_time = time.time()\n        while time.time() - start_time < server_timeout:\n            try:\n                subprocess.run(\n                    [\"docker\", \"exec\", container_name, \"ollama\", \"list\"],\n                    check=True,\n                    capture_output=True,\n                )\n                return True\n            except subprocess.CalledProcessError:\n                time.sleep(1)\n        \n        typer.echo(\"âŒ Solo server failed to start within timeout\", err=True)\n        return False\n\n    except subprocess.CalledProcessError as e:\n        typer.echo(f\"âŒ Docker command failed: {e}\", err=True)\n        # Cleanup on failure\n        if container_exists:\n            subprocess.run([\"docker\", \"stop\", container_name], check=False)\n        raise typer.Exit(code=1)\n    except Exception as e:\n        typer.echo(f\"âŒ Unexpected error: {e}\", err=True)\n        return False\n    \n\ndef start_llama_cpp_server(os_name: str = None, model_path: str = None, port: int = None):\n    \"\"\"\n    Start the llama.cpp server.\n    \n    Parameters:\n    os_name (str, optional): The name of the operating system.\n    model_path (str, optional): Path to the model file or HuggingFace repo ID.\n    port (int, optional): Port to run the server on.\n    \"\"\"\n    # Check if llama_cpp is installed\n    if not is_llama_cpp_installed():\n        typer.echo(\"âŒ Server not found. Please run 'solo setup' first.\", err=True)\n        return False\n        \n    # Load llama.cpp configuration from YAML\n    llama_cpp_config = get_server_config('llama_cpp')\n    \n    # Use default values from config if not provided\n    port = port or llama_cpp_config.get('default_port', 8080)\n    model_path = model_path or llama_cpp_config.get('default_model')\n    \n    try:\n        # Check if port is already in use\n        if is_port_in_use(port):\n            typer.echo(f\"âŒ Port {port} is already in use, please try a different port\", err=True)\n            typer.echo(f\"Run 'solo stop' to stop all running servers.\")\n            return False\n        \n        # If no model path is provided, prompt the user\n        if not model_path:\n            typer.echo(\"Please provide the path to your GGUF model file or a HuggingFace repo ID.\")\n            model_path = typer.prompt(\"Enter the model path or repo ID\")\n            \n        # Get HuggingFace token if needed\n        hf_token = os.getenv('HUGGING_FACE_TOKEN', '')\n        if not hf_token and not os.path.exists(model_path):  # Only check for token if not a local file\n            if os.path.exists(CONFIG_PATH):\n                with open(CONFIG_PATH, 'r') as f:\n                    config = json.load(f)\n                    hf_token = config.get('hugging_face', {}).get('token', '')\n        \n        # Determine if this is a repo ID or direct path\n        hf_repo_id, model_pattern = preprocess_model_path(model_path, hf_token)\n\n        # Build server command\n        server_cmd = [\n            sys.executable, \"-m\", \"llama_cpp.server\",\n            \"--host\", \"0.0.0.0\",\n            \"--port\", str(port)\n        ]\n        \n        if hf_repo_id and not os.path.exists(model_path):\n            # This is a HuggingFace repo ID\n            typer.echo(f\"Using HuggingFace repo: {hf_repo_id}\")\n            server_cmd.extend([\"--hf_model_repo_id\", hf_repo_id])\n            server_cmd.extend([\"--model\", model_pattern])\n        else:\n            # Direct model path\n            model_path = os.path.abspath(os.path.expanduser(model_path))\n            if not os.path.exists(model_path):\n                typer.echo(f\"âŒ Model file not found: {model_path}\", err=True)\n                return False\n            server_cmd.extend([\"--model\", model_path])\n        \n        if os_name == \"Windows\":\n            # Create a log file for capturing output\n            log_dir = os.path.join(os.path.expanduser(\"~\"), \".solo_server\", \"logs\")\n            os.makedirs(log_dir, exist_ok=True)\n            log_file = os.path.join(log_dir, \"llama_cpp_server.log\")\n            \n            # Start the server in a new console window and keep it open with a pause command\n            cmd_str = \" \".join(server_cmd) + \" & pause\"\n            process = subprocess.Popen(\n                f'start cmd /k \"{cmd_str}\"',\n                shell=True\n            )\n            typer.echo(f\"Solo Server is running in a new terminal window. Use ctrl + c to stop.\")\n        else:\n            # For Unix-like systems, use terminal-specific commands\n            if os_name == \"Darwin\":  # macOS\n                # For macOS, use AppleScript to keep the Terminal window open\n                escaped_cmd = \" \".join(server_cmd).replace('\"', '\\\\\"')\n                script = f'tell app \"Terminal\" to do script \"{escaped_cmd} ; echo \\\\\\\"\\\\\\\\nServer is running. Press Ctrl+C to stop.\\\\\\\"; bash\"'\n                typer.echo(f\"Debug: Executing AppleScript with command: {escaped_cmd}\")\n                terminal_cmd = [\"osascript\", \"-e\", script]\n                try:\n                    result = subprocess.Popen(terminal_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                    stdout, stderr = result.communicate(timeout=5)\n                    if stderr:\n                        typer.echo(f\"Debug: AppleScript stderr: {stderr.decode('utf-8')}\")\n                    typer.echo(\"Server is running in a new Terminal window\")\n                except Exception as e:\n                    typer.echo(f\"Warning: Issue launching Terminal: {e}\")\n                    # Fallback to background process\n                    typer.echo(\"Falling back to background process...\")\n                    process = subprocess.Popen(\n                        server_cmd,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                        start_new_session=True\n                    )\n                    typer.echo(f\"Server is running in the background. Process ID: {process.pid}\")\n                    return True\n            else:  # Linux and other Unix-like systems\n                # Try to detect the terminal and keep it open\n                if shutil.which(\"gnome-terminal\"):\n                    terminal_cmd = [\"gnome-terminal\", \"--\", \"bash\", \"-c\", f\"{' '.join(server_cmd)}; echo '\\\\nServer is running. Press Ctrl+C to stop.'; exec bash\"]\n                    subprocess.Popen(terminal_cmd)\n                elif shutil.which(\"xterm\"):\n                    terminal_cmd = [\"xterm\", \"-e\", f\"{' '.join(server_cmd)}; echo '\\\\nServer is running. Press Ctrl+C to stop.'; exec bash\"]\n                    subprocess.Popen(terminal_cmd)\n                elif shutil.which(\"konsole\"):\n                    terminal_cmd = [\"konsole\", \"-e\", f\"bash -c '{' '.join(server_cmd)}; echo \\\"\\\\nServer is running. Press Ctrl+C to stop.\\\"; exec bash'\"]\n                    subprocess.Popen(terminal_cmd)\n                else:\n                    # Fallback to background process if no terminal is found\n                    process = subprocess.Popen(\n                        server_cmd,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE,\n                        start_new_session=True\n                    )\n                    typer.echo(f\"Server is running in the background. Process ID: {process.pid}\")\n                \n                typer.echo(\"Server is running in a new terminal window\")\n        \n        # Wait for the server to start\n        time.sleep(2)\n        return True\n        \n    except Exception as e:\n        typer.echo(f\"âŒ Failed to start Solo server: {e}\", err=True)\n        return False\n\ndef is_huggingface_repo(model: str) -> bool:\n    \"\"\"Check if the model string is a HuggingFace repository ID.\"\"\"\n    return model.startswith(\"hf://\") or model.startswith(\"hf.co/\") or \"/\" in model and not model.startswith(\"ollama/\")\n\ndef check_ollama_model_exists(container_name: str, model: str) -> tuple[bool, str]:\n    \"\"\"\n    Check if a model exists in Ollama.\n    \n    Args:\n        container_name (str): The name of the Ollama container\n        model (str): The model name to check\n        \n    Returns:\n        tuple[bool, str]: A tuple containing (exists, model_name)\n            - exists (bool): True if the model exists, False otherwise\n            - model_name (str): The full model name with tag if it exists, otherwise the original model name\n    \"\"\"\n    try:\n        # Get the list of models from Ollama\n        model_exists = subprocess.run(\n            [\"docker\", \"exec\", container_name, \"ollama\", \"list\"],\n            capture_output=True,\n            text=True,\n            check=True\n        ).stdout\n        \n        # Check if the model has a tag\n        has_tag = ':' in model\n        if has_tag:\n            # If the model has a tag, check for exact match\n            if model in model_exists:\n                return True, model\n        else:\n            # If the model doesn't have a tag, check for the model with :latest tag\n            model_with_latest = f\"{model}:latest\"\n            if model_with_latest in model_exists:\n                return True, model_with_latest\n                    \n        # Model not found\n        return False, model\n    except subprocess.CalledProcessError:\n        # Error running the command\n        return False, model\n\ndef pull_ollama_model(container_name: str, model: str) -> str:\n    \"\"\"\n    Pull a model from Ollama.\n    \n    Args:\n        container_name (str): The name of the Ollama container\n        model (str): The model name to pull\n        \n    Returns:\n        str: The model name after pulling (may include tag)\n        \n    Raises:\n        typer.Exit: If the model could not be pulled\n    \"\"\"\n    # First check if the model exists with a different tag\n    model_exists, existing_model = check_ollama_model_exists(container_name, model)\n    if model_exists:\n        typer.echo(f\"âœ… Model {existing_model} already exists\")\n        return existing_model\n    \n    # Check if model already has a tag\n    has_tag = ':' in model\n    if has_tag:\n        # If the model has a tag, try to pull that exact model\n        typer.echo(f\"ðŸ“¥ Pulling model {model}...\")\n        try:\n            # Run the pull command \n            process = subprocess.Popen(\n                [\"docker\", \"exec\", container_name, \"ollama\", \"pull\", model],\n                stdout=None,  # Use None to show output in real-time\n                stderr=None,  # Use None to show errors in real-time\n                text=True\n            )\n            # Wait for the process to complete\n            process.wait()\n            \n            if process.returncode != 0:\n                typer.echo(f\"âŒ Failed to pull model {model}\", err=True)\n                raise typer.Exit(code=1)\n                \n            typer.echo(f\"âœ… Model {model} pulled successfully\")\n            # Return the model name with tag\n            return model\n        except subprocess.CalledProcessError as e:\n            typer.echo(f\"âŒ Failed to pull model {model}: {e}\", err=True)\n            raise typer.Exit(code=1)\n    else:\n        # If the model doesn't have a tag, try to pull with :latest tag first\n        model_with_tag = f\"{model}:latest\"\n        typer.echo(f\"ðŸ“¥ Pulling model {model_with_tag}...\")\n        try:\n            # Run the pull command \n            process = subprocess.Popen(\n                [\"docker\", \"exec\", container_name, \"ollama\", \"pull\", model_with_tag],\n                stdout=None,  # Use None to show output in real-time\n                stderr=None,  # Use None to show errors in real-time\n                text=True\n            )\n            # Wait for the process to complete\n            process.wait()\n            \n            if process.returncode != 0:\n                typer.echo(f\"âŒ Failed to pull model {model_with_tag}\", err=True)\n                # Try without the tag as a fallback\n                typer.echo(f\"Trying to pull model {model} without tag...\")\n                process = subprocess.Popen(\n                    [\"docker\", \"exec\", container_name, \"ollama\", \"pull\", model],\n                    stdout=None,  # Use None to show output in real-time\n                    stderr=None,  # Use None to show errors in real-time\n                    text=True\n                )\n                # Wait for the process to complete\n                process.wait()\n                \n                if process.returncode != 0:\n                    typer.echo(f\"âŒ Failed to pull model {model}\", err=True)\n                    raise typer.Exit(code=1)\n                    \n                typer.echo(f\"âœ… Model {model} pulled successfully\")\n                return model\n            else:\n                typer.echo(f\"âœ… Model {model_with_tag} pulled successfully\")\n                # Return the model name with tag\n                return model_with_tag\n        except subprocess.CalledProcessError as e:\n            typer.echo(f\"âŒ Failed to pull model {model_with_tag}: {e}\", err=True)\n            raise typer.Exit(code=1)\n\ndef pull_model_from_huggingface(container_name: str, model: str) -> str:\n    \"\"\"\n    Pull a model from HuggingFace to Ollama.\n    Returns the Ollama model name after pulling.\n    \"\"\"\n    from solo_server.utils.hf_utils import get_available_models\n    \n    # Format the model string for Ollama's pull command\n    if model.startswith(\"hf://\"):\n        model = model.replace(\"hf://\", \"\")\n    elif model.startswith(\"hf.co/\"):\n        model = model.replace(\"hf.co/\", \"\")\n    \n    # Get HuggingFace token from environment variable or config file\n    hf_token = os.getenv('HUGGING_FACE_TOKEN', '')\n    if not hf_token:  # If not in env, try config file\n        if os.path.exists(CONFIG_PATH):\n            with open(CONFIG_PATH, 'r') as f:\n                config = json.load(f)\n                hf_token = config.get('hugging_face', {}).get('token', '')\n    \n    # Check if a specific model file is specified or just the repo\n    if model.count('/') >= 2:  \n        # Specific model file is provided (username/repo/filename.gguf)\n        parts = model.split('/')\n        repo_id = '/'.join(parts[:-1])  # username/repo\n        model_file = parts[-1]  # filename.gguf\n        \n        # Extract quantization format from filename (e.g., Q4_K_M)\n        quant_format = None\n        if \".gguf\" in model_file.lower():\n            # Try to extract quantization format like Q4_K_M\n            parts = model_file.lower().split('.')\n            if len(parts) > 1:\n                # Look for Q4_K_M or similar pattern in the filename\n                for part in parts:\n                    if part.startswith('q') and '_' in part:\n                        quant_format = part.upper()\n                        break\n        \n        # Format for Ollama: hf.co/username/repo:QUANT\n        if quant_format:\n            hf_model = f\"hf.co/{repo_id}:{quant_format}\"\n        else:\n            # If no quantization format found, use the repo ID only\n            hf_model = f\"hf.co/{repo_id}\"\n        \n        # Use repo name as model name\n        model_name = repo_id.split('/')[-1]\n\n    else:  # Format: username/repo\n        # Only repo is provided, need to select best model file\n        repo_id = model\n        \n        # Get available GGUF models from the repo\n        model_files = get_available_models(repo_id, suffix=\".gguf\")\n\n        if not model_files:\n            typer.echo(f\"âŒ No GGUF models found in repository {repo_id}\", err=True)\n            raise typer.Exit(code=1)\n        \n        # Select the best model based on quantization\n        best_model = select_best_model_file(model_files)\n        typer.echo(f\"Selected model: {best_model}\")\n        \n        # Extract quantization format from filename (e.g., Q4_K_M)\n        quant_format = None\n        if \".gguf\" in best_model.lower():\n            # Try to extract quantization format like Q4_K_M\n            parts = best_model.lower().split('.')\n            if len(parts) > 1:\n                # Look for Q4_K_M or similar pattern in the filename\n                for part in parts:\n                    if part.startswith('q') and '_' in part:\n                        quant_format = part.upper()\n                        break\n        \n        # Format for Ollama: hf.co/username/repo:QUANT\n        if quant_format:\n            hf_model = f\"hf.co/{repo_id}:{quant_format}\"\n        else:\n            # If no quantization format found, use the repo ID only\n            hf_model = f\"hf.co/{repo_id}\"\n        \n        # Use repo name as model name\n        model_name = repo_id.split('/')[-1]\n    \n    typer.echo(f\"ðŸ“¥ Pulling model {hf_model} from HuggingFace...\")\n    \n    try:\n        subprocess.run(\n            [\"docker\", \"exec\", container_name, \"ollama\", \"pull\", hf_model],\n            check=True\n        )\n        typer.echo(f\"âœ… Successfully pulled model from HuggingFace\")\n        return model_name\n    except subprocess.CalledProcessError as e:\n        typer.echo(f\"âŒ Failed to pull model from HuggingFace: {e}\", err=True)\n        raise e\n\n\n"
  }
}